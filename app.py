import io
import re
import datetime as dt
import pandas as pd
import streamlit as st
import sqlalchemy as sa
from sqlalchemy import inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import altair as alt
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from uuid import uuid4

# ======================== åŸºæœ¬è¨­å®š ========================
st.set_page_config(page_title="Slot Manager", layout="wide")
mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿", "ğŸ“Š å¯è¦–åŒ–"), key="mode_radio")
st.title("ğŸ° Slot Data Manager & Visualizer")

SA_INFO = st.secrets["gcp_service_account"]
PG_CFG  = st.secrets["connections"]["slot_db"]

# ======================== è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« ========================
with open("setting.json", encoding="utf-8") as f:
    setting_map = json.load(f)

# ======================== æ¥ç¶š ========================
def make_drive():
    try:
        creds = Credentials.from_service_account_info(
            SA_INFO, scopes=["https://www.googleapis.com/auth/drive.readonly"]
        )
        return build("drive", "v3", credentials=creds)
    except Exception as e:
        st.error(f"Driveèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_resource
def gdrive():
    return make_drive()

drive = gdrive()

@st.cache_resource
def engine():
    try:
        url = (
            f"postgresql+psycopg2://{PG_CFG.username}:{PG_CFG.password}"
            f"@{PG_CFG.host}:{PG_CFG.port}/{PG_CFG.database}?sslmode=require"
        )
        return sa.create_engine(url, pool_pre_ping=True)
    except Exception as e:
        st.error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

eng = engine()
if eng is None:
    st.stop()

# ======================== ã‚«ãƒ©ãƒ å®šç¾©ãƒãƒƒãƒ”ãƒ³ã‚° ========================
COLUMN_MAP = {
    "ãƒ¡ãƒƒã‚»æ­¦è”µå¢ƒ": {
        "å°ç•ªå·":"å°ç•ªå·","ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":"ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°","ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":"ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°":"BBå›æ•°","RBå›æ•°":"RBå›æ•°","ARTå›æ•°":"ARTå›æ•°",
        "æœ€å¤§æŒã¡ç‰":"æœ€å¤§æŒç‰","æœ€å¤§æŒç‰":"æœ€å¤§æŒç‰",
        "BBç¢ºç‡":"BBç¢ºç‡","RBç¢ºç‡":"RBç¢ºç‡","ARTç¢ºç‡":"ARTç¢ºç‡","åˆæˆç¢ºç‡":"åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ":"å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
    },
    "ã‚¸ãƒ£ãƒ³ã‚¸ãƒ£ãƒ³ãƒãƒ¼ãƒ«ã‚´ãƒƒãƒˆåˆ†å€æ²³åŸ": {
        "å°ç•ªå·":"å°ç•ªå·","ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":"ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ","BBå›æ•°":"BBå›æ•°","RBå›æ•°":"RBå›æ•°",
        "æœ€å¤§æŒã¡ç‰":"æœ€å¤§æŒç‰","æœ€å¤§æŒç‰":"æœ€å¤§æŒç‰",
        "BBç¢ºç‡":"BBç¢ºç‡","RBç¢ºç‡":"RBç¢ºç‡","åˆæˆç¢ºç‡":"åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ":"å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ","ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":"ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
    "ãƒ—ãƒ¬ã‚´ç«‹å·": {
        "å°ç•ªå·":"å°ç•ªå·","ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":"ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ","BBå›æ•°":"BBå›æ•°","RBå›æ•°":"RBå›æ•°",
        "æœ€å¤§å·®ç‰":"æœ€å¤§å·®ç‰","BBç¢ºç‡":"BBç¢ºç‡","RBç¢ºç‡":"RBç¢ºç‡","åˆæˆç¢ºç‡":"åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ":"å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ","ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":"ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
}

# ======================== Drive: å†å¸° + ãƒšãƒ¼ã‚¸ãƒ³ã‚° ========================
@st.cache_data
def list_csv_recursive(folder_id: str):
    if drive is None:
        raise RuntimeError("Driveæœªæ¥ç¶šã§ã™")
    all_files, queue = [], [(folder_id, "")]
    while queue:
        fid, cur = queue.pop()
        page_token = None
        while True:
            res = drive.files().list(
                q=f"'{fid}' in parents and trashed=false",
                fields="nextPageToken, files(id,name,mimeType,md5Checksum,modifiedTime,size)",
                pageSize=1000, pageToken=page_token
            ).execute()
            for f in res.get("files", []):
                if f["mimeType"] == "application/vnd.google-apps.folder":
                    queue.append((f["id"], f"{cur}/{f['name']}"))
                elif f["name"].lower().endswith(".csv"):
                    all_files.append({**f, "path": f"{cur}/{f['name']}"})
            page_token = res.get("nextPageToken")
            if not page_token:
                break
    return all_files

# ======================== ãƒ¡ã‚¿æƒ…å ±è§£æ ========================
DATE_RE = re.compile(r"\d{4}-\d{2}-\d{2}")

def parse_meta(path: str):
    parts = path.strip("/").split("/")
    if len(parts) < 3:
        raise ValueError(f"ãƒ‘ã‚¹ãŒçŸ­ã™ãã¾ã™: {path}")
    store, machine = parts[-3], parts[-2]
    m = DATE_RE.search(parts[-1])
    if not m:
        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜(YYYY-MM-DD)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {parts[-1]}")
    date = dt.date.fromisoformat(m.group(0))
    return store, machine, date

# ======================== æ­£è¦åŒ– ========================
def normalize(df_raw: pd.DataFrame, store: str) -> pd.DataFrame:
    df = df_raw.rename(columns=COLUMN_MAP[store])

    prob_cols = ["BBç¢ºç‡", "RBç¢ºç‡", "ARTç¢ºç‡", "åˆæˆç¢ºç‡"]
    for col in prob_cols:
        if col not in df.columns:
            continue
        ser = df[col].astype(str)
        mask_div = ser.str.contains("/", na=False)

        if mask_div.any():
            denom = pd.to_numeric(
                ser[mask_div].str.split("/", expand=True)[1],
                errors="coerce"
            )
            val = 1.0 / denom
            val[(denom <= 0) | (~denom.notna())] = 0
            df.loc[mask_div, col] = val

        num = pd.to_numeric(ser[~mask_div_]()_
