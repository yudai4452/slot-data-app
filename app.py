import io
import re
import datetime as dt
import pandas as pd
import streamlit as st
import sqlalchemy as sa
from sqlalchemy import inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import altair as alt
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from uuid import uuid4

# ======================== è¨­å®šèª­ã¿è¾¼ã¿ ========================
@st.cache_data
def load_settings() -> dict:
    with open("setting.json", "r", encoding="utf-8") as f:
        return json.load(f)

SETTINGS = load_settings()
PG_CFG = SETTINGS["connections"]["slot_db"]
SA_INFO = SETTINGS["gcp_service_account"]

# ======================== Drive / DB æ¥ç¶š ========================
def make_drive():
    try:
        creds = Credentials.from_service_account_info(
            SA_INFO,
            scopes=["https://www.googleapis.com/auth/drive.readonly"],
        )
        return build("drive", "v3", credentials=creds)
    except Exception as e:
        st.error(f"Driveèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_resource
def gdrive():
    return make_drive()

drive = gdrive()

@st.cache_resource
def engine():
    try:
        url = (
            f"postgresql+psycopg2://{PG_CFG['username']}:{PG_CFG['password']}"
            f"@{PG_CFG['host']}:{PG_CFG['port']}/{PG_CFG['database']}?sslmode={PG_CFG.get('sslmode', 'require')}"
        )
        return sa.create_engine(url, pool_pre_ping=True)
    except Exception as e:
        st.error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

eng = engine()
if eng is None:
    st.stop()

# ======================== Google Drive æ¤œç´¢ ========================
def list_csv_recursive(folder_id: str) -> list[dict]:
    """æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ä»¥ä¸‹ã® csv ã‚’å…¨éƒ¨åˆ—æŒ™ã™ã‚‹ï¼ˆä»®æƒ³ path ä»˜ãï¼‰"""
    if drive is None:
        st.error("Drive ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã§ãã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    stack = [(folder_id, "")]
    files: list[dict] = []

    while stack:
        fid, base = stack.pop()
        q = f"'{fid}' in parents and trashed=false"
        page_token = None
        while True:
            resp = drive.files().list(
                q=q,
                pageSize=1000,
                fields="nextPageToken, files(id, name, mimeType, md5Checksum)",
                pageToken=page_token,
            ).execute()
            for f in resp.get("files", []):
                name = f["name"]
                mime = f["mimeType"]
                if mime == "application/vnd.google-apps.folder":
                    stack.append((f["id"], base + name + "/"))
                    continue
                if not name.lower().endswith(".csv"):
                    continue
                files.append(
                    {
                        "id": f["id"],
                        "name": name,
                        "path": base + name,
                        "mimeType": mime,
                        "md5Checksum": f.get("md5Checksum") or "",
                    }
                )
            page_token = resp.get("nextPageToken")
            if not page_token:
                break
    return files

# ======================== ãƒ¡ã‚¿æƒ…å ±æŠ½å‡º ========================
def parse_meta(path: str) -> tuple[str, str, dt.date]:
    """
    path ã‹ã‚‰ åº—èˆ—å, æ©Ÿç¨®å, æ—¥ä»˜ ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ä¾‹: "ãƒ—ãƒ¬ã‚´ç«‹å·/ãƒã‚¤ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼â…¤/slot_machine_data_2025-01-01.csv"
    """
    parts = path.split("/")
    if len(parts) < 3:
        raise ValueError(f"ãƒ‘ã‚¹å½¢å¼ãŒæƒ³å®šå¤–ã§ã™: {path}")

    store = parts[0]
    machine = parts[1]

    m = re.search(r"(\d{4}-\d{2}-\d{2})", parts[-1])
    if not m:
        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜(YYYY-MM-DD)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {parts[-1]}")
    date = dt.datetime.strptime(m.group(1), "%Y-%m-%d").date()

    return store, machine, date

# ======================== ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚° ========================
COLUMN_MAP = {
    "ãƒ¡ãƒƒã‚»æ­¦è”µå¢ƒ": {
        "å°ç•ªå·": "å°ç•ªå·",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°": "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°": "BBå›æ•°",
        "RBå›æ•°": "RBå›æ•°",
        "ARTå›æ•°": "ARTå›æ•°",
        "æœ€å¤§æŒã¡ç‰": "æœ€å¤§æŒç‰",
    },
    "ãƒ—ãƒ¬ã‚´ç«‹å·": {
        "å°ç•ªå·": "å°ç•ªå·",
        "ç·ã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°": "BBå›æ•°",
        "RBå›æ•°": "RBå›æ•°",
        "AT/ARTå›æ•°": "ARTå›æ•°",
        "æœ€å¤§æŒç‰": "æœ€å¤§æŒç‰",
    },
    "ã‚¸ãƒ£ãƒ³ã‚¸ãƒ£ãƒ³ãƒãƒ¼ãƒ«ã‚´ãƒƒãƒˆåˆ†å€æ²³åŸ": {
        "å°ç•ªå·": "å°ç•ªå·",
        "ã‚¹ã‚¿ãƒ¼ãƒˆ": "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°": "BBå›æ•°",
        "RBå›æ•°": "RBå›æ•°",
        "ARTå›æ•°": "ARTå›æ•°",
        "æœ€å¤§æŒç‰": "æœ€å¤§æŒç‰",
    },
}

def normalize(df_raw: pd.DataFrame, store: str) -> pd.DataFrame:
    if store not in COLUMN_MAP:
        raise ValueError(f"æœªå¯¾å¿œåº—èˆ—: {store}")

    mapping = COLUMN_MAP[store]

    df = df_raw.rename(columns=mapping).copy()

    required = ["å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "BBå›æ•°", "RBå›æ•°", "ARTå›æ•°", "æœ€å¤§æŒç‰"]
    for col in required:
        if col not in df.columns:
            df[col] = 0

    def safe_int(x):
        try:
            if pd.isna(x):
                return 0
            s = str(x).replace(",", "").replace(" ", "").strip()
            if s == "":
                return 0
            return int(float(s))
        except Exception:
            return 0

    for col in required:
        df[col] = df[col].map(safe_int)

    return df[required]

def load_and_normalize(raw_bytes: bytes, store: str) -> pd.DataFrame:
    """
    ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ shift_jis å›ºå®šã§èª­ã¿ã€ã‚«ãƒ©ãƒ ã‚’æ­£è¦åŒ–ã€‚
    """
    header = pd.read_csv(io.BytesIO(raw_bytes), encoding="shift_jis", nrows=0).columns.tolist()
    mapping_keys = list(dict.fromkeys(COLUMN_MAP[store].keys()))
    usecols = [col for col in mapping_keys if col in header]
    df_raw = pd.read_csv(
        io.BytesIO(raw_bytes),
        encoding="shift_jis",
        usecols=usecols,
        on_bad_lines="skip",
        engine="python",
    )
    return normalize(df_raw, store)

# ======================== import_logï¼ˆå·®åˆ†å–ã‚Šè¾¼ã¿ï¼‰ ========================
def ensure_import_log_table():
    meta = sa.MetaData()
    insp = inspect(eng)
    if not insp.has_table("import_log"):
        t = sa.Table(
            "import_log",
            meta,
            sa.Column("file_id", sa.String(128), primary_key=True),
            sa.Column("md5", sa.String(64), nullable=False),
            sa.Column("path", sa.String(512), nullable=False),
            sa.Column("store", sa.String(128), nullable=False),
            sa.Column("machine", sa.String(128), nullable=False),
            sa.Column("date", sa.Date, nullable=False),
            sa.Column("rows", sa.Integer, nullable=False),
            sa.Column("imported_at", sa.DateTime(timezone=True), nullable=False),
        )
        meta.create_all(eng)
        return t
    else:
        t = sa.Table("import_log", meta, autoload_with=eng)
        return t

def get_imported_md5_map():
    log = ensure_import_log_table()
    with eng.connect() as conn:
        rows = conn.execute(sa.select(log.c.file_id, log.c.md5)).fetchall()
    return {r[0]: r[1] for r in rows}

def upsert_import_log(entries: list[dict]):
    if not entries:
        return
    log = ensure_import_log_table()
    stmt = pg_insert(log).values(entries)
    stmt = stmt.on_conflict_do_update(
        index_elements=[log.c.file_id],
        set_={
            "md5": stmt.excluded.md5,
            "path": stmt.excluded.path,
            "store": stmt.excluded.store,
            "machine": stmt.excluded.machine,
            "date": stmt.excluded.date,
            "rows": stmt.excluded.rows,
            "imported_at": stmt.excluded.imported_at,
        },
    )
    with eng.begin() as conn:
        conn.execute(stmt)

# ======================== STORE ãƒ†ãƒ¼ãƒ–ãƒ« ========================
def ensure_store_table(store: str) -> sa.Table:
    """
    åº—èˆ—å˜ä½ã§ 1 ãƒ†ãƒ¼ãƒ–ãƒ«ï¼š
    slot_ãƒ—ãƒ¬ã‚´ç«‹å·
    """
    table_name = "slot_" + store.replace(" ", "_")
    meta = sa.MetaData()
    insp = inspect(eng)
    if insp.has_table(table_name):
        return sa.Table(table_name, meta, autoload_with=eng)

    tbl = sa.Table(
        table_name,
        meta,
        sa.Column("date", sa.Date, nullable=False),
        sa.Column("æ©Ÿç¨®", sa.String(128), nullable=False),
        sa.Column("å°ç•ªå·", sa.Integer, nullable=False),
        sa.Column("ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", sa.Integer, nullable=False),
        sa.Column("BBå›æ•°", sa.Integer, nullable=False),
        sa.Column("RBå›æ•°", sa.Integer, nullable=False),
        sa.Column("ARTå›æ•°", sa.Integer, nullable=False),
        sa.Column("æœ€å¤§æŒç‰", sa.Integer, nullable=False),
        sa.PrimaryKeyConstraint("date", "æ©Ÿç¨®", "å°ç•ªå·", name=f"pk_{table_name}"),
    )
    meta.create_all(eng)
    return tbl

# ======================== é€šå¸¸ UPSERT ========================
def upsert_dataframe(conn, table: sa.Table, df: pd.DataFrame):
    if df.empty:
        return

    ins = pg_insert(table)
    update_cols = [c.name for c in table.columns if c.name not in ("date", "æ©Ÿç¨®", "å°ç•ªå·")]
    stmt = ins.on_conflict_do_update(
        index_elements=["date", "æ©Ÿç¨®", "å°ç•ªå·"],
        set_={c: ins.excluded[c] for c in update_cols},
    )
    conn.execute(stmt, df.to_dict(orient="records"))

# ======================== COPY é«˜é€ŸåŒ– ========================
def q(name: str) -> str:
    return '"' + name.replace('"', '""') + '"'

def bulk_upsert_copy_merge(engine: sa.Engine, table: sa.Table, df: pd.DataFrame, pk=("date", "æ©Ÿç¨®", "å°ç•ªå·")):
    if df.empty:
        return

    valid_cols = [c.name for c in table.c]
    cols = [c for c in df.columns if c in valid_cols]

    for p in pk:
        if p not in cols:
            raise ValueError(f"COPYåˆ—ã«ä¸»ã‚­ãƒ¼ {p} ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

    df_use = df[cols].copy()

    csv_buf = io.StringIO()
    df_use.to_csv(csv_buf, index=False, na_rep="")
    csv_text = csv_buf.getvalue()

    tmp_name = f"tmp_{table.name}_{uuid4().hex[:8]}"
    cols_q = ", ".join(q(c) for c in cols)
    pk_q = ", ".join(q(p) for p in pk)
    upd_cols = [c for c in cols if c not in pk]
    set_clause = ", ".join(f"{q(c)}=EXCLUDED.{q(c)}" for c in upd_cols) if upd_cols else ""

    create_tmp_sql = f'CREATE TEMP TABLE {q(tmp_name)} (LIKE {q(table.name)} INCLUDING ALL);'
    copy_sql = f'COPY {q(tmp_name)} ({cols_q}) FROM STDIN WITH (FORMAT csv, HEADER true);'
    insert_sql = (
        f'INSERT INTO {q(table.name)} ({cols_q}) SELECT {cols_q} FROM {q(tmp_name)} '
        f'ON CONFLICT ({pk_q}) DO ' + ('NOTHING;' if not set_clause else f'UPDATE SET {set_clause};')
    )
    drop_tmp_sql = f'DROP TABLE IF EXISTS {q(tmp_name)};'

    with engine.begin() as conn:
        driver_conn = getattr(conn.connection, "driver_connection", None)
        if driver_conn is None:
            driver_conn = conn.connection.connection  # fallback psycopg2 connection

        with driver_conn.cursor() as cur:
            cur.execute(create_tmp_sql)
            cur.copy_expert(copy_sql, io.StringIO(csv_text))
            cur.execute(insert_sql)
            cur.execute(drop_tmp_sql)

# ======================== ä¸¦åˆ—å‡¦ç†ï¼š1ãƒ•ã‚¡ã‚¤ãƒ« ========================
def process_one_file(file_meta: dict) -> dict | None:
    try:
        store, machine, date = parse_meta(file_meta["path"])
        if store not in COLUMN_MAP:
            # æœªå¯¾å¿œåº—èˆ—ã¯ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦è¨˜éŒ²ã—ã¦ã‚¹ã‚­ãƒƒãƒ—
            return {"error": f"{file_meta.get('path','(unknown)')} æœªå¯¾å¿œåº—èˆ—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {store}"}

        if drive is None:
            # èµ·å‹•æ™‚ã«æ—¢ã«ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã—ã¦æ­¢ã‚ã¦ã„ã‚‹æƒ³å®šã ãŒã€å¿µã®ãŸã‚ã‚»ãƒ¼ãƒ•ã‚¬ãƒ¼ãƒ‰
            raise RuntimeError("Drive ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¦ã„ã¾ã™ã€‚")

        raw = drive.files().get_media(fileId=file_meta["id"]).execute()
        df = load_and_normalize(raw, store)
        if df.empty:
            return None

        df["æ©Ÿç¨®"] = machine
        df["date"] = date
        table_name = "slot_" + store.replace(" ", "_")
        return {
            "table_name": table_name,
            "df": df,
            "store": store,
            "machine": machine,
            "date": date,
            "file_id": file_meta["id"],
            "md5": file_meta.get("md5Checksum") or "",
            "path": file_meta["path"],
        }
    except Exception as e:
        return {"error": f"{file_meta.get('path','(unknown)')} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"}

# ======================== è‡ªå‹•ãƒãƒƒãƒå®Ÿè¡Œãƒ˜ãƒ«ãƒ‘ãƒ¼ ========================
def run_import_for_targets(targets: list[dict], workers: int, use_copy: bool):
    detail_status = st.empty()
    created_tables: dict[str, sa.Table] = {}
    import_log_entries = []
    errors = []
    bucket: dict[str, list[dict]] = defaultdict(list)

    with ThreadPoolExecutor(max_workers=workers) as ex:
        futures = {ex.submit(process_one_file, f): f for f in targets}
        for fut in as_completed(futures):
            res = fut.result()
            if res is None:
                continue
            if "error" in res:
                errors.append(res["error"])
                continue
            bucket[res["table_name"]].append(res)
            detail_status.text(f"å‡¦ç†å®Œäº†: {res['path']}")

    for table_name, items in bucket.items():
        if table_name not in created_tables:
            tbl = ensure_store_table(items[0]["store"])
            created_tables[table_name] = tbl
        else:
            tbl = created_tables[table_name]

        valid_cols = [c.name for c in tbl.c]

        if use_copy:
            try:
                dfs = []
                for res in items:
                    df = res["df"]
                    for c in valid_cols:
                        if c not in df.columns:
                            df[c] = pd.NA
                    dfs.append(df[[c for c in df.columns if c in valid_cols]])
                df_all = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame(columns=valid_cols)
                bulk_upsert_copy_merge(eng, tbl, df_all)
            except Exception as e:
                errors.append(f"{table_name} COPYé«˜é€ŸåŒ–å¤±æ•—ã®ãŸã‚é€šå¸¸UPSERTã§å†è©¦è¡Œ: {e}")
                with eng.begin() as conn:
                    for res in items:
                        df_one = res["df"][[c for c in res["df"].columns if c in valid_cols]]
                        try:
                            upsert_dataframe(conn, tbl, df_one)
                        except Exception as ie:
                            errors.append(f"{res['path']} é€šå¸¸UPSERTã§ã‚‚å¤±æ•—: {ie}")
        else:
            with eng.begin() as conn:
                for res in items:
                    df_one = res["df"][[c for c in res["df"].columns if c in valid_cols]]
                    upsert_dataframe(conn, tbl, df_one)

        for res in items:
            import_log_entries.append(
                {
                    "file_id": res["file_id"],
                    "md5": res["md5"],
                    "path": res["path"],
                    "store": res["store"],
                    "machine": res["machine"],
                    "date": res["date"],
                    "rows": int(len(res["df"])),
                    "imported_at": dt.datetime.now(dt.timezone.utc),
                }
            )

    processed_files = sum(len(v) for v in bucket.values())
    return import_log_entries, errors, processed_files

# ========================= Streamlit UI =========================
st.set_page_config(page_title="Slot Data Importer", layout="wide")

mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ["ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿", "ğŸ“Š å¯è¦–åŒ–"])

# ========================= å–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ =========================
if mode == "ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿":
    st.header("Google Drive â†’ Postgres ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")

    folder_options = {
        "ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨": SETTINGS["import_folders"]["test"],
        "ğŸš€ æœ¬ç•ªç”¨": SETTINGS["import_folders"]["prod"],
    }
    options = list(folder_options.keys())
    default_idx = options.index("ğŸš€ æœ¬ç•ªç”¨") if "ğŸš€ æœ¬ç•ªç”¨" in options else 0
    sel_label = st.selectbox("å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€", options, index=default_idx, key="folder_type")
    folder_id = folder_options[sel_label]

    col1, col2 = st.columns(2)
    with col1:
        imp_start = st.date_input("ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡é–‹å§‹æ—¥", dt.date.today() - dt.timedelta(days=7))
    with col2:
        imp_end = st.date_input("ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡çµ‚äº†æ—¥", dt.date.today())

    if imp_start > imp_end:
        st.error("é–‹å§‹æ—¥ã¯çµ‚äº†æ—¥ä»¥å‰ã«ã—ã¦ãã ã•ã„")
        st.stop()

    c1, c2, c3 = st.columns(3)
    workers = c1.slider("ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°", 1, 16, 4)
    use_copy = c2.checkbox(
        "DBæ›¸ãè¾¼ã¿ã‚’COPYã§é«˜é€ŸåŒ–ï¼ˆæ¨å¥¨ï¼‰",
        value=True,
        help="ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«COPYâ†’ã¾ã¨ã‚ã¦UPSERTã€‚å¤±æ•—æ™‚ã¯é€šå¸¸UPSERTã«è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚",
        key="use_copy",
    )
    max_files = c3.slider(
        "æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆ1å›ã®å®Ÿè¡Œä¸Šé™ï¼‰",
        10,
        2000,
        300,
        step=10,
        help="å¤§é‡ãƒ•ã‚©ãƒ«ãƒ€ã¯åˆ†å‰²ã—ã¦å–ã‚Šè¾¼ã¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰",
        key="max_files",
    )

    auto_batch = st.checkbox("æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã”ã¨ã«è‡ªå‹•ã§ç¶šãã®ãƒãƒƒãƒã‚‚å®Ÿè¡Œã™ã‚‹", value=False, key="auto_batch")
    max_batches = st.number_input(
        "æœ€å¤§ãƒãƒƒãƒå›æ•°ï¼ˆ0ã§åˆ¶é™ãªã—ï¼‰",
        min_value=0,
        max_value=100,
        value=3,
        help="å®Ÿè¡Œæ™‚é–“ãŒé•·ããªã‚Šã™ãã‚‹ã®ã‚’é˜²ããŸã‚ã®ä¸Šé™ã€‚0ãªã‚‰å…¨ãƒãƒƒãƒå®Ÿè¡Œã€‚",
        key="max_batches",
    )

    if st.button("ğŸš€ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ", disabled=not folder_id, key="import_run"):
        try:
            files_all = list_csv_recursive(folder_id)
            files: list[dict] = []
            skipped: list[str] = []
            for f in files_all:
                try:
                    _, _, file_date = parse_meta(f["path"])
                except ValueError as e:
                    skipped.append(f"{f['path']}: {e}")
                    continue
                if imp_start <= file_date <= imp_end:
                    files.append(f)
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        if skipped:
            st.info(f"ãƒ‘ã‚¹å½¢å¼ãŒæƒ³å®šå¤–ã§ã‚¹ã‚­ãƒƒãƒ—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒ {len(skipped)} ä»¶ã‚ã‚Šã¾ã™ã€‚")

        imported_md5 = get_imported_md5_map()
        all_targets = [
            f for f in files if imported_md5.get(f["id"], "") != (f.get("md5Checksum") or "")
        ]
        if not all_targets:
            st.success("å·®åˆ†ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆã™ã¹ã¦æœ€æ–°ï¼‰")
            st.stop()

        all_targets.sort(key=lambda f: parse_meta(f["path"])[2])

        batches = [all_targets[i : i + max_files] for i in range(0, len(all_targets), max_files)]

        # å®Ÿéš›ã«å‡¦ç†ã™ã‚‹ãƒãƒƒãƒä¸€è¦§
        if not auto_batch:
            # è‡ªå‹•ãƒãƒƒãƒ OFF â†’ 1ãƒãƒƒãƒã ã‘
            use_batches = batches[:1]
        else:
            # è‡ªå‹•ãƒãƒƒãƒ ON â†’ max_batches ãŒ 0 ãªã‚‰å…¨ãƒãƒƒãƒã€ãã‚Œä»¥å¤–ã¯æŒ‡å®šæ•°ã¾ã§
            if max_batches == 0:
                use_batches = batches
            else:
                use_batches = batches[: int(max_batches)]

        total_files = sum(len(b) for b in use_batches)
        done_files = 0
        bar = st.progress(0.0)
        batch_status = st.empty()
        all_errors: list[str] = []

        for bi, batch in enumerate(use_batches, start=1):
            batch_status.text(f"ãƒãƒƒãƒ {bi}/{len(use_batches)}ï¼ˆ{len(batch)} ä»¶ï¼‰ã‚’å‡¦ç†ä¸­â€¦")
            entries, errors, processed_files = run_import_for_targets(batch, workers, use_copy)
            upsert_import_log(entries)
            all_errors.extend(errors)

            done_files += processed_files
            bar.progress(min(1.0, done_files / max(1, total_files)))

        batch_status.text("")

        # ã¾ã æ®‹ã‚ŠãŒã‚ã‚‹å ´åˆã ã‘æ¡ˆå†…ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if auto_batch and max_batches > 0 and len(batches) > len(use_batches):
            remaining = sum(len(b) for b in batches[len(use_batches) :])
            st.info(
                f"æœ€å¤§ãƒãƒƒãƒå›æ•°ã«é”ã—ã¾ã—ãŸã€‚æ®‹ã‚Š {remaining} ä»¶ã¯ã€å†åº¦ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ç¶šãã‹ã‚‰å‡¦ç†ã—ã¾ã™ã€‚"
            )

        if all_errors:
            st.warning("ä¸€éƒ¨ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ï¼š")
            for msg in all_errors[:50]:
                st.write("- " + msg)
            if len(all_errors) > 50:
                st.write(f"... ã»ã‹ {len(all_errors)-50} ä»¶")

        st.success(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼ˆå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«: {done_files} ä»¶ï¼‰ï¼")

# ========================= å¯è¦–åŒ–ãƒ¢ãƒ¼ãƒ‰ =========================
if mode == "ğŸ“Š å¯è¦–åŒ–":
    st.header("DB å¯è¦–åŒ–")

    # 1) ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
    try:
        with eng.connect() as conn:
            tables = [
                r[0]
                for r in conn.execute(
                    sa.text("SELECT tablename FROM pg_tables WHERE tablename LIKE 'slot_%'")
                )
            ]
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if not tables:
        st.info("ã¾ãšå–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
        st.stop()

    table_name = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠ", tables)
    if not table_name:
        st.error("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
        st.stop()

    TBL_Q = '"' + table_name.replace('"', '""') + '"'

    # 2) æœ€å°/æœ€å¤§æ—¥ä»˜ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    @st.cache_data(ttl=600)
    def get_date_range(table_name: str):
        TBL_Q = '"' + table_name.replace('"', '""') + '"'
        with eng.connect() as conn:
            row = conn.execute(sa.text(f"SELECT MIN(date), MAX(date) FROM {TBL_Q}")).first()
        return (row[0], row[1]) if row else (None, None)

    min_date, max_date = get_date_range(table_name)
    if not (min_date and max_date):
        st.info("ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšå–ã‚Šè¾¼ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    c1, c2 = st.columns(2)
    vis_start = c1.date_input(
        "é–‹å§‹æ—¥",
        value=min_date,
        min_value=min_date,
        max_value=max_date,
        key=f"visual_start_{table_name}",
    )
    vis_end = c2.date_input(
        "çµ‚äº†æ—¥",
        value=max_date,
        min_value=min_date,
        max_value=max_date,
        key=f"visual_end_{table_name}",
    )

    if vis_start > vis_end:
        st.error("é–‹å§‹æ—¥ã¯çµ‚äº†æ—¥ä»¥å‰ã«ã—ã¦ãã ã•ã„")
        st.stop()

    # 3) ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆä»»æ„ï¼‰
    idx_ok = st.checkbox(
        "èª­ã¿è¾¼ã¿é«˜é€ŸåŒ–ã®ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆï¼ˆæ¨å¥¨ãƒ»ä¸€åº¦ã ã‘ï¼‰", value=True, key="create_index"
    )
    if idx_ok:
        try:
            with eng.begin() as conn:
                conn.execute(
                    sa.text(
                        f'CREATE INDEX IF NOT EXISTS {table_name}_ix_machine_date ON {TBL_Q} ("æ©Ÿç¨®","date");'
                    )
                )
                conn.execute(
                    sa.text(
                        f'CREATE INDEX IF NOT EXISTS {table_name}_ix_machine_slot_date ON {TBL_Q} ("æ©Ÿç¨®","å°ç•ªå·","date");'
                    )
                )
        except Exception as e:
            st.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆå‡¦ç†è‡ªä½“ã¯ç¶šè¡Œã—ã¾ã™ï¼‰: {e}")

    # 4) ãƒ‡ãƒ¼ã‚¿å–å¾—
    try:
        with eng.connect() as conn:
            df = pd.read_sql(
                sa.text(
                    f"SELECT * FROM {TBL_Q} WHERE date BETWEEN :start AND :end ORDER BY date, å°ç•ªå·"
                ),
                conn,
                params={"start": vis_start, "end": vis_end},
            )
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if df.empty:
        st.info("è©²å½“æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    with st.expander("ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã‚‹"):
        st.dataframe(df)

    # ===== å¯è¦–åŒ–ï¼šæ©Ÿç¨®ãƒ»å°ç•ªå·ã”ã¨ã®æˆç¸¾ =====
    st.subheader("æ—¥åˆ¥é›†è¨ˆï¼ˆBB/RB/ART/ã‚¹ã‚¿ãƒ¼ãƒˆï¼‰")

    agg = (
        df.groupby("date", as_index=False)[["ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "BBå›æ•°", "RBå›æ•°", "ARTå›æ•°", "æœ€å¤§æŒç‰"]]
        .sum()
        .sort_values("date")
    )
    c1, c2 = st.columns(2)
    with c1:
        st.dataframe(agg)

    with c2:
        chart = (
            alt.Chart(agg)
            .mark_line(point=True)
            .encode(
                x="date:T",
                y=alt.Y("ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ:Q", title="ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ"),
                tooltip=[
                    "date:T",
                    "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ:Q",
                    "BBå›æ•°:Q",
                    "RBå›æ•°:Q",
                    "ARTå›æ•°:Q",
                    "æœ€å¤§æŒç‰:Q",
                ],
            )
            .properties(width="container", height=280)
        )
        st.altair_chart(chart, use_container_width=True)

    # ===== ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼šå°ç•ªå· Ã— æ—¥ä»˜ï¼ˆæœ€å¤§æŒç‰ï¼‰ =====
    st.subheader("å°ç•ªå· Ã— æ—¥ä»˜ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆæœ€å¤§æŒç‰ï¼‰")
    heat_df = df[["date", "å°ç•ªå·", "æœ€å¤§æŒç‰"]].copy()

    heat = (
        alt.Chart(heat_df)
        .mark_rect()
        .encode(
            x=alt.X("date:T", title="æ—¥ä»˜"),
            y=alt.Y("å°ç•ªå·:O", title="å°ç•ªå·"),
            color=alt.Color("æœ€å¤§æŒç‰:Q", title="æœ€å¤§æŒç‰"),
            tooltip=["date:T", "å°ç•ªå·:O", "æœ€å¤§æŒç‰:Q"],
        )
        .properties(width="container", height=400)
    )
    st.altair_chart(heat, use_container_width=True)

    # ===== å˜ä¸€å°ã®è©³ç´°æ¨ç§» =====
    st.subheader("å˜ä¸€å°ã®è©³ç´°æ¨ç§»")
    tai_list = sorted(df["å°ç•ªå·"].unique())
    tai_sel = st.selectbox("å°ç•ªå·ã‚’é¸æŠ", ["ï¼ˆé¸æŠãªã—ï¼‰"] + [str(t) for t in tai_list])

    if tai_sel != "ï¼ˆé¸æŠãªã—ï¼‰":
        tai_num = int(tai_sel)
        df_one = df[df["å°ç•ªå·"] == tai_num].sort_values("date")

        st.write(f"å°ç•ªå· {tai_num} ã®æ¨ç§»")

        base = alt.Chart(df_one).encode(x="date:T")

        line_start = (
            base.mark_line(point=True)
            .encode(
                y=alt.Y("ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ:Q", title="ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ"),
                tooltip=[
                    "date:T",
                    "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ:Q",
                    "BBå›æ•°:Q",
                    "RBå›æ•°:Q",
                    "ARTå›æ•°:Q",
                    "æœ€å¤§æŒç‰:Q",
                ],
            )
            .properties(width="container", height=240)
        )
        st.altair_chart(line_start, use_container_width=True)

        line_max = (
            base.mark_line(point=True)
            .encode(
                y=alt.Y("æœ€å¤§æŒç‰:Q", title="æœ€å¤§æŒç‰"),
                tooltip=["date:T", "æœ€å¤§æŒç‰:Q"],
            )
            .properties(width="container", height=240)
        )
        st.altair_chart(line_max, use_container_width=True)

    st.caption("â€» å¯è¦–åŒ–ã¯å¿…è¦ã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦OKã§ã™ã€‚")
