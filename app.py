# app.py
import io
import re
import json
import datetime as dt
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from uuid import uuid4

import altair as alt
import numpy as np
import pandas as pd
import sqlalchemy as sa
import streamlit as st
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from sqlalchemy import inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert

# ============================================================
# Streamlit åŸºæœ¬è¨­å®š
# ============================================================
st.set_page_config(page_title="Slot Manager", layout="wide")

MODE_IMPORT = "ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿"
MODE_VIZ = "ğŸ“Š å¯è¦–åŒ–"
MODE_ML = "ğŸ§  MLãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆäºˆæ¸¬UIä»˜ãï¼‰"

mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", (MODE_IMPORT, MODE_VIZ, MODE_ML), key="mode_radio")
st.title("ğŸ° Slot Data Manager & Visualizer")

# ============================================================
# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ / è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
# ============================================================
SA_INFO = st.secrets["gcp_service_account"]
PG_CFG = st.secrets["connections"]["slot_db"]

with open("setting.json", encoding="utf-8") as f:
    setting_map = json.load(f)

# ============================================================
# DB & Google Drive æ¥ç¶š
# ============================================================
def make_drive():
    """éƒ½åº¦ Credentials ã‹ã‚‰ Drive ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã®ãŸã‚ã«æ¯å›ä½œã‚‹ç”¨ï¼‰"""
    try:
        creds = Credentials.from_service_account_info(
            SA_INFO,
            scopes=["https://www.googleapis.com/auth/drive.readonly"],
        )
        return build("drive", "v3", credentials=creds)
    except Exception as e:
        st.error(f"Driveèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None


@st.cache_resource
def gdrive():
    """ã‚·ãƒ³ãƒ—ãƒ«ãª Drive ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½¿ã‚ãªã„å‡¦ç†å‘ã‘ï¼‰"""
    return make_drive()


drive = gdrive()


@st.cache_resource
def engine():
    """Postgres ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆï¼ˆæ¥ç¶šã¯å¿…è¦æ™‚ã«æ¯å› open/closeï¼‰"""
    try:
        url = (
            f"postgresql+psycopg2://{PG_CFG.username}:{PG_CFG.password}"
            f"@{PG_CFG.host}:{PG_CFG.port}/{PG_CFG.database}?sslmode=require"
        )
        return sa.create_engine(url, pool_pre_ping=True)
    except Exception as e:
        st.error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None


eng = engine()
if eng is None:
    st.stop()

# ============================================================
# å…±é€š: Postgres è­˜åˆ¥å­ã‚¯ã‚ªãƒ¼ãƒˆ / ãƒ•ã‚¡ã‚¤ãƒ«åå®‰å…¨åŒ–
# ============================================================
def q(name: str) -> str:
    return '"' + name.replace('"', '""') + '"'


def safe_index_name(table_name: str, suffix: str) -> str:
    base = re.sub(r"[^0-9a-zA-Z_]+", "_", table_name)
    base = re.sub(r"_+", "_", base).strip("_") or "slot"
    return f"{base}_{suffix}"


def safe_filename(s: str) -> str:
    return re.sub(r'[\\/:*?"<>|]+', "_", s)


# ============================================================
# ã‚«ãƒ©ãƒ æ­£è¦åŒ–ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°
# ============================================================
COLUMN_MAP = {
    "ãƒ¡ãƒƒã‚»æ­¦è”µå¢ƒ": {
        "å°ç•ªå·": "å°ç•ªå·",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°": "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°": "BBå›æ•°",
        "RBå›æ•°": "RBå›æ•°",
        "ARTå›æ•°": "ARTå›æ•°",
        "æœ€å¤§æŒã¡ç‰": "æœ€å¤§æŒç‰",
        "æœ€å¤§æŒç‰": "æœ€å¤§æŒç‰",
        "BBç¢ºç‡": "BBç¢ºç‡",
        "RBç¢ºç‡": "RBç¢ºç‡",
        "ARTç¢ºç‡": "ARTç¢ºç‡",
        "åˆæˆç¢ºç‡": "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
    },
    "ã‚¸ãƒ£ãƒ³ã‚¸ãƒ£ãƒ³ãƒãƒ¼ãƒ«ã‚´ãƒƒãƒˆåˆ†å€æ²³åŸ": {
        "å°ç•ªå·": "å°ç•ªå·",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°": "BBå›æ•°",
        "RBå›æ•°": "RBå›æ•°",
        "æœ€å¤§æŒã¡ç‰": "æœ€å¤§æŒç‰",
        "æœ€å¤§æŒç‰": "æœ€å¤§æŒç‰",
        "BBç¢ºç‡": "BBç¢ºç‡",
        "RBç¢ºç‡": "RBç¢ºç‡",
        "åˆæˆç¢ºç‡": "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°": "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
    "ãƒ—ãƒ¬ã‚´ç«‹å·": {
        "å°ç•ªå·": "å°ç•ªå·",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°": "BBå›æ•°",
        "RBå›æ•°": "RBå›æ•°",
        "æœ€å¤§å·®ç‰": "æœ€å¤§å·®ç‰",
        "BBç¢ºç‡": "BBç¢ºç‡",
        "RBç¢ºç‡": "RBç¢ºç‡",
        "åˆæˆç¢ºç‡": "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°": "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
}

# 1/x è¡¨è¨˜ã—ãŸã„ã€Œç¢ºç‡ç³»ã€ã‚«ãƒ©ãƒ 
PROB_PLOT_COLUMNS = ["åˆæˆç¢ºç‡", "BBç¢ºç‡", "RBç¢ºç‡", "ARTç¢ºç‡"]

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é¸æŠã—ãŸã„ã€Œå‡ºç‰ç³»ã€ã‚«ãƒ©ãƒ ï¼ˆä¸Šã‹ã‚‰é †ã«å„ªå…ˆï¼‰
DEFAULT_PAYOUT_COLUMNS = ["æœ€å¤§å·®ç‰", "å·®æš", "å·®ç‰", "æœ€å¤§æŒç‰"]

# ============================================================
# MLç”¨: å·®æšç›¸å½“ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆåº—ã”ã¨ã®é•ã„å¸åï¼‰
# ============================================================
PAYOUT_TARGET_PRIORITY = ["å·®æš", "å·®ç‰", "æœ€å¤§å·®ç‰", "æœ€å¤§æŒç‰"]
PAYOUT_ALIASES = {
    "å·®æš": ["å·®æš", "å·®æšæ•°", "å·®æš(æš)"],
    "å·®ç‰": ["å·®ç‰", "å·®ç‰æ•°"],
    "æœ€å¤§å·®ç‰": ["æœ€å¤§å·®ç‰", "æœ€å¤§å·®æš", "æœ€å¤§å·®æšæ•°"],
    "æœ€å¤§æŒç‰": ["æœ€å¤§æŒç‰", "æœ€å¤§æŒã¡ç‰"],
}


def build_payout_candidates(numeric_candidates: list[str]) -> list[dict]:
    out = []
    seen_source = set()
    for canon in PAYOUT_TARGET_PRIORITY:
        for src in PAYOUT_ALIASES.get(canon, [canon]):
            if src in numeric_candidates and src not in seen_source:
                seen_source.add(src)
                out.append(
                    {
                        "canonical": canon,
                        "source": src,
                        "label": f"{canon}ç›¸å½“ï¼š{src}",
                    }
                )
    return out


# ============================================================
# Google Drive: ãƒ•ã‚©ãƒ«ãƒ€ä»¥ä¸‹ã® CSV ã‚’å†å¸°çš„ã«å–å¾—
# ============================================================
@st.cache_data
def list_csv_recursive(folder_id: str):
    if drive is None:
        raise RuntimeError("Driveæœªæ¥ç¶šã§ã™")

    all_files = []
    queue = [(folder_id, "")]  # (folder_id, path_prefix)

    while queue:
        fid, cur = queue.pop()
        page_token = None

        while True:
            res = (
                drive.files()
                .list(
                    q=f"'{fid}' in parents and trashed=false",
                    fields="nextPageToken, files(id,name,mimeType,md5Checksum,modifiedTime,size)",
                    pageSize=1000,
                    pageToken=page_token,
                )
                .execute()
            )

            for f in res.get("files", []):
                if f["mimeType"] == "application/vnd.google-apps.folder":
                    queue.append((f["id"], f"{cur}/{f['name']}"))
                elif f["name"].lower().endswith(".csv"):
                    all_files.append({**f, "path": f"{cur}/{f['name']}"})

            page_token = res.get("nextPageToken")
            if not page_token:
                break

    return all_files


# ============================================================
# ãƒ‘ã‚¹ã‹ã‚‰ åº—èˆ— / æ©Ÿç¨® / æ—¥ä»˜ ã‚’æŠ½å‡º
# ============================================================
DATE_RE = re.compile(r"\d{4}-\d{2}-\d{2}")


def parse_meta(path: str):
    parts = path.strip("/").split("/")
    if len(parts) < 3:
        raise ValueError(f"ãƒ‘ã‚¹ãŒçŸ­ã™ãã¾ã™: {path}")

    store, machine = parts[-3], parts[-2]
    m = DATE_RE.search(parts[-1])
    if not m:
        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜(YYYY-MM-DD)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {parts[-1]}")
    date = dt.date.fromisoformat(m.group(0))
    return store, machine, date


# ============================================================
# CSV â†’ DataFrame æ­£è¦åŒ–
# ============================================================
def normalize(df_raw: pd.DataFrame, store: str) -> pd.DataFrame:
    df = df_raw.rename(columns=COLUMN_MAP[store])

    # ç¢ºç‡ç³»ã‚’ 0ã€œ1 ã«çµ±ä¸€
    prob_cols = ["BBç¢ºç‡", "RBç¢ºç‡", "ARTç¢ºç‡", "åˆæˆç¢ºç‡"]
    for col in prob_cols:
        if col not in df.columns:
            continue

        ser = df[col].astype(str)
        mask_div = ser.str.contains("/", na=False)

        # "1/113"
        if mask_div.any():
            denom = pd.to_numeric(ser[mask_div].str.split("/", expand=True)[1], errors="coerce")
            val = 1.0 / denom
            val[(denom <= 0) | (~denom.notna())] = 0
            df.loc[mask_div, col] = val

        # "113" â†’ 1/113
        num = pd.to_numeric(ser[~mask_div], errors="coerce")
        conv = num.copy()
        conv[num > 1] = 1.0 / num[num > 1]
        conv = conv.fillna(0)
        df.loc[~mask_div, col] = conv

        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0.0).astype(float)

    # æ•´æ•°ç³»
    int_cols = [
        "å°ç•ªå·",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
        "BBå›æ•°",
        "RBå›æ•°",
        "ARTå›æ•°",
        "æœ€å¤§æŒç‰",
        "æœ€å¤§å·®ç‰",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
    ]
    for col in int_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")

    return df


def load_and_normalize(raw_bytes: bytes, store: str) -> pd.DataFrame:
    header = pd.read_csv(io.BytesIO(raw_bytes), encoding="shift_jis", nrows=0).columns.tolist()
    mapping_keys = list(dict.fromkeys(COLUMN_MAP[store].keys()))
    usecols = [col for col in mapping_keys if col in header]

    df_raw = pd.read_csv(
        io.BytesIO(raw_bytes),
        encoding="shift_jis",
        usecols=usecols,
        on_bad_lines="skip",
        engine="python",
    )
    return normalize(df_raw, store)


# ============================================================
# import_log ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆå·®åˆ†å–ã‚Šè¾¼ã¿ç®¡ç†ï¼‰
# ============================================================
def ensure_import_log_table():
    meta = sa.MetaData()
    insp = inspect(eng)

    if not insp.has_table("import_log"):
        t = sa.Table(
            "import_log",
            meta,
            sa.Column("file_id", sa.Text, primary_key=True),
            sa.Column("md5", sa.Text, nullable=False),
            sa.Column("path", sa.Text, nullable=False),
            sa.Column("store", sa.Text, nullable=False),
            sa.Column("machine", sa.Text, nullable=False),
            sa.Column("date", sa.Date, nullable=False),
            sa.Column("rows", sa.Integer, nullable=False),
            sa.Column("imported_at", sa.DateTime, nullable=False, server_default=sa.func.now()),
        )
        meta.create_all(eng)
    else:
        t = sa.Table("import_log", meta, autoload_with=eng)

    return t


def get_imported_md5_map():
    log = ensure_import_log_table()
    with eng.connect() as conn:
        rows = conn.execute(sa.select(log.c.file_id, log.c.md5)).fetchall()
    return {r[0]: r[1] for r in rows}


def upsert_import_log(entries: list[dict]):
    if not entries:
        return
    log = ensure_import_log_table()
    stmt = pg_insert(log).values(entries)
    stmt = stmt.on_conflict_do_update(
        index_elements=[log.c.file_id],
        set_={
            "md5": stmt.excluded.md5,
            "path": stmt.excluded.path,
            "store": stmt.excluded.store,
            "machine": stmt.excluded.machine,
            "date": stmt.excluded.date,
            "rows": stmt.excluded.rows,
            "imported_at": sa.func.now(),
        },
    )
    with eng.begin() as conn:
        conn.execute(stmt)


# ============================================================
# åº—èˆ—ã”ã¨ã® slot_* ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
# ============================================================
def ensure_store_table(store: str) -> sa.Table:
    safe_name = "slot_" + store.replace(" ", "_")
    insp = inspect(eng)
    meta = sa.MetaData()

    if not insp.has_table(safe_name):
        cols = [
            sa.Column("date", sa.Date, nullable=False),
            sa.Column("æ©Ÿç¨®", sa.Text, nullable=False),
            sa.Column("å°ç•ªå·", sa.Integer, nullable=False),
        ]

        unique_cols = list(dict.fromkeys(COLUMN_MAP[store].values()))
        numeric_int = {
            "å°ç•ªå·",
            "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
            "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
            "BBå›æ•°",
            "RBå›æ•°",
            "ARTå›æ•°",
            "æœ€å¤§æŒç‰",
            "æœ€å¤§å·®ç‰",
            "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
        }

        for col_name in unique_cols:
            if col_name in {"date", "æ©Ÿç¨®", "å°ç•ªå·"}:
                continue
            if col_name in numeric_int:
                cols.append(sa.Column(col_name, sa.Integer))
            else:
                cols.append(sa.Column(col_name, sa.Float))

        t = sa.Table(
            safe_name,
            meta,
            *cols,
            sa.PrimaryKeyConstraint("date", "æ©Ÿç¨®", "å°ç•ªå·"),
        )
        meta.create_all(eng)
        return t

    return sa.Table(safe_name, meta, autoload_with=eng)


# ============================================================
# é€šå¸¸ UPSERTï¼ˆè¡Œã”ã¨ï¼‰
# ============================================================
def upsert_dataframe(conn, table: sa.Table, df: pd.DataFrame, pk=("date", "æ©Ÿç¨®", "å°ç•ªå·")):
    rows = df.to_dict(orient="records")
    if not rows:
        return
    stmt = pg_insert(table).values(rows)
    update_cols = {c.name: stmt.excluded[c.name] for c in table.c if c.name not in pk}
    stmt = stmt.on_conflict_do_update(index_elements=list(pk), set_=update_cols)
    conn.execute(stmt)


# ============================================================
# COPY â†’ MERGE ã§é«˜é€Ÿã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
# ============================================================
def bulk_upsert_copy_merge(table: sa.Table, df: pd.DataFrame, pk=("date", "æ©Ÿç¨®", "å°ç•ªå·")):
    if df.empty:
        return

    valid_cols = [c.name for c in table.c]
    cols = [c for c in df.columns if c in valid_cols]

    for p in pk:
        if p not in cols:
            raise ValueError(f"COPYåˆ—ã«ä¸»ã‚­ãƒ¼ {p} ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

    df_use = df[cols].copy()

    csv_buf = io.StringIO()
    df_use.to_csv(csv_buf, index=False, na_rep="")
    csv_text = csv_buf.getvalue()

    tmp_name = f"tmp_{table.name}_{uuid4().hex[:8]}"
    cols_q = ", ".join(q(c) for c in cols)
    pk_q = ", ".join(q(p) for p in pk)
    upd_cols = [c for c in cols if c not in pk]
    set_clause = ", ".join(f"{q(c)}=EXCLUDED.{q(c)}" for c in upd_cols) if upd_cols else ""

    create_tmp_sql = f"CREATE TEMP TABLE {q(tmp_name)} (LIKE {q(table.name)} INCLUDING ALL);"
    copy_sql = f"COPY {q(tmp_name)} ({cols_q}) FROM STDIN WITH (FORMAT csv, HEADER true);"
    insert_sql = (
        f"INSERT INTO {q(table.name)} ({cols_q}) "
        f"SELECT {cols_q} FROM {q(tmp_name)} "
        f"ON CONFLICT ({pk_q}) DO "
        + ("NOTHING;" if not set_clause else f"UPDATE SET {set_clause};")
    )
    drop_tmp_sql = f"DROP TABLE IF EXISTS {q(tmp_name)};"

    with eng.begin() as conn:
        driver_conn = getattr(conn.connection, "driver_connection", None)
        if driver_conn is None:
            driver_conn = conn.connection.connection  # psycopg2 fallback
        with driver_conn.cursor() as cur:
            cur.execute(create_tmp_sql)
            cur.copy_expert(copy_sql, io.StringIO(csv_text))
            cur.execute(insert_sql)
            cur.execute(drop_tmp_sql)


# ============================================================
# ä¸¦åˆ—å‡¦ç†: CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & æ­£è¦åŒ–
# ============================================================
def process_one_file(file_meta: dict) -> dict | None:
    try:
        store, machine, date = parse_meta(file_meta["path"])
        if store not in COLUMN_MAP:
            return None

        drv = make_drive()
        raw = drv.files().get_media(fileId=file_meta["id"]).execute()
        df = load_and_normalize(raw, store)
        if df.empty:
            return None

        df["æ©Ÿç¨®"] = machine
        df["date"] = date
        table_name = "slot_" + store.replace(" ", "_")

        return {
            "table_name": table_name,
            "df": df,
            "store": store,
            "machine": machine,
            "date": date,
            "file_id": file_meta["id"],
            "md5": file_meta.get("md5Checksum") or "",
            "path": file_meta["path"],
        }
    except Exception as e:
        return {"error": f"{file_meta.get('path', '(unknown)')} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"}


def run_import_for_targets(targets: list[dict], workers: int, use_copy: bool):
    status = st.empty()
    created_tables: dict[str, sa.Table] = {}
    import_log_entries = []
    errors = []
    bucket: dict[str, list[dict]] = defaultdict(list)

    # 1) ä¸¦åˆ—ã§CSVå–å¾—
    with ThreadPoolExecutor(max_workers=workers) as ex:
        futures = {ex.submit(process_one_file, f): f for f in targets}
        for fut in as_completed(futures):
            res = fut.result()
            if res is None:
                continue
            if "error" in res:
                errors.append(res["error"])
                continue
            bucket[res["table_name"]].append(res)
            status.text(f"å‡¦ç†å®Œäº†: {res['path']}")

    # 2) ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã«DBæ›¸ãè¾¼ã¿
    for table_name, items in bucket.items():
        if table_name not in created_tables:
            tbl = ensure_store_table(items[0]["store"])
            created_tables[table_name] = tbl
        else:
            tbl = created_tables[table_name]

        valid_cols = [c.name for c in tbl.c]

        if use_copy:
            try:
                dfs = []
                for res in items:
                    df = res["df"]
                    for c in valid_cols:
                        if c not in df.columns:
                            df[c] = pd.NA
                    dfs.append(df[[c for c in df.columns if c in valid_cols]])

                df_all = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame(columns=valid_cols)
                bulk_upsert_copy_merge(tbl, df_all)

            except Exception as e:
                errors.append(f"{table_name} COPYé«˜é€ŸåŒ–å¤±æ•—ã®ãŸã‚é€šå¸¸UPSERTã§å†è©¦è¡Œ: {e}")
                with eng.begin() as conn:
                    for res in items:
                        df_one = res["df"][[c for c in res["df"].columns if c in valid_cols]]
                        try:
                            upsert_dataframe(conn, tbl, df_one)
                        except Exception as ie:
                            errors.append(f"{res['path']} é€šå¸¸UPSERTã§ã‚‚å¤±æ•—: {ie}")
        else:
            with eng.begin() as conn:
                for res in items:
                    df_one = res["df"][[c for c in res["df"].columns if c in valid_cols]]
                    upsert_dataframe(conn, tbl, df_one)

        for res in items:
            import_log_entries.append(
                {
                    "file_id": res["file_id"],
                    "md5": res["md5"],
                    "path": res["path"],
                    "store": res["store"],
                    "machine": res["machine"],
                    "date": res["date"],
                    "rows": int(len(res["df"])),
                }
            )

    processed_files = sum(len(v) for v in bucket.values())
    return import_log_entries, errors, processed_files


# ============================================================
# æ™‚ç³»åˆ—åŸºç›¤ãƒ¢ãƒ‡ãƒ«ï¼ˆUIå®Ÿè¡Œç”¨ï¼‰
# ============================================================
@st.cache_resource(show_spinner=False)
def get_chronos2_pipeline(device_map: str = "cpu"):
    from chronos import Chronos2Pipeline
    return Chronos2Pipeline.from_pretrained("amazon/chronos-2", device_map=device_map)


@st.cache_resource(show_spinner=False)
def get_timesfm_model():
    import torch
    import timesfm

    torch.set_float32_matmul_precision("high")
    model = timesfm.TimesFM_2p5_200M_torch.from_pretrained("google/timesfm-2.5-200m-pytorch")
    try:
        model.compile(
            timesfm.ForecastConfig(
                max_context=1024,
                max_horizon=256,
                normalize_inputs=True,
            )
        )
    except Exception:
        pass
    return model


def forecast_with_chronos2(df_long: pd.DataFrame, horizon: int, device_map: str = "cpu") -> pd.DataFrame:
    pipe = get_chronos2_pipeline(device_map=device_map)

    pred = pipe.predict_df(
        df_long,
        prediction_length=horizon,
        quantile_levels=[0.1, 0.5, 0.9],
        id_column="id",
        timestamp_column="timestamp",
        target="target",
    )

    if "0.5" in pred.columns:
        pred = pred.rename(columns={"0.5": "yhat"})
    elif "predictions" in pred.columns:
        pred = pred.rename(columns={"predictions": "yhat"})
    else:
        num_cols = [
            c
            for c in pred.columns
            if c not in {"id", "timestamp"} and pd.api.types.is_numeric_dtype(pred[c])
        ]
        if not num_cols:
            raise RuntimeError(f"Chronos-2ã®å‡ºåŠ›åˆ—ãŒæƒ³å®šã¨é•ã„ã¾ã™: {pred.columns.tolist()}")
        pred = pred.rename(columns={num_cols[0]: "yhat"})

    keep = [c for c in ["id", "timestamp", "yhat", "0.1", "0.9"] if c in pred.columns]
    return pred[keep].copy()


def forecast_with_timesfm(df_long: pd.DataFrame, horizon: int, freq: str = "D") -> pd.DataFrame:
    model = get_timesfm_model()

    df_long = df_long.sort_values(["id", "timestamp"]).copy()
    ids = df_long["id"].unique().tolist()

    series_list = []
    last_ts = {}

    for _id in ids:
        g = df_long[df_long["id"] == _id].sort_values("timestamp")
        y = g["target"].astype(float)
        y = y.interpolate(limit_direction="both").fillna(0.0)
        series_list.append(y.to_numpy())
        last_ts[_id] = pd.to_datetime(g["timestamp"].max())

    point_fcst, _ = model.forecast(horizon=horizon, inputs=series_list)

    rows = []
    for i, _id in enumerate(ids):
        start = last_ts[_id]
        future_index = pd.date_range(start=start, periods=horizon + 1, freq=freq)[1:]
        for t, ts in enumerate(future_index):
            rows.append({"id": _id, "timestamp": ts, "yhat": float(point_fcst[i, t])})

    return pd.DataFrame(rows)


def prob_to_denom(p: float) -> float:
    if p is None or (not np.isfinite(p)) or p <= 0:
        return float("inf")
    return 1.0 / float(p)


def score_setting_by_denom(pred_prob: float, thresholds: dict) -> str | None:
    if not thresholds:
        return None

    d = prob_to_denom(pred_prob)
    best = None
    best_dist = float("inf")

    for k, v in thresholds.items():
        try:
            vv = float(v)
        except Exception:
            continue
        dv = prob_to_denom(vv)
        dist = abs(d - dv)
        if dist < best_dist:
            best_dist = dist
            best = k

    return best


# ============================================================
# ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰
# ============================================================
if mode == MODE_IMPORT:
    st.header("Google Drive â†’ Postgres ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")

    folder_options = {
        "ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨": "1MRQFPBahlSwdwhrqqBzudXL18y8-qOb8",
        "ğŸš€ æœ¬ç•ªç”¨": "1hX8GQRuDm_E1A1Cu_fXorvwxv-XF7Ynl",
    }
    options = list(folder_options.keys())
    default_idx = options.index("ğŸš€ æœ¬ç•ªç”¨") if "ğŸš€ æœ¬ç•ªç”¨" in options else 0

    sel_label = st.selectbox("ãƒ•ã‚©ãƒ«ãƒ€ã‚¿ã‚¤ãƒ—", options, index=default_idx, key="folder_type")
    folder_id = st.text_input("Google Drive ãƒ•ã‚©ãƒ«ãƒ€ ID", value=folder_options[sel_label], key="folder_id")

    c1, c2 = st.columns(2)
    imp_start = c1.date_input("é–‹å§‹æ—¥", dt.date(2024, 1, 1), key="import_start_date")
    imp_end = c2.date_input("çµ‚äº†æ—¥", dt.date.today(), key="import_end_date")

    c3, c4 = st.columns(2)
    max_files = c3.slider(
        "æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆ1å›ã®å®Ÿè¡Œä¸Šé™ï¼‰",
        10,
        2000,
        300,
        step=10,
        help="å¤§é‡ãƒ•ã‚©ãƒ«ãƒ€ã¯åˆ†å‰²ã—ã¦å–ã‚Šè¾¼ã¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰",
        key="max_files",
    )
    workers = c4.slider(
        "ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°",
        1,
        8,
        4,
        help="ä¸¦åˆ—æ•°ãŒå¤šã™ãã‚‹ã¨APIåˆ¶é™ã«å½“ãŸã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
        key="workers",
    )

    use_copy = st.checkbox(
        "DBæ›¸ãè¾¼ã¿ã‚’COPYã§é«˜é€ŸåŒ–ï¼ˆæ¨å¥¨ï¼‰",
        value=True,
        help="ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«COPYâ†’ã¾ã¨ã‚ã¦UPSERTã€‚å¤±æ•—æ™‚ã¯è‡ªå‹•ã§é€šå¸¸UPSERTã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚",
        key="use_copy",
    )
    auto_batch = st.checkbox("æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã”ã¨ã«è‡ªå‹•ã§ç¶šãã®ãƒãƒƒãƒã‚‚å®Ÿè¡Œã™ã‚‹", value=False, key="auto_batch")
    max_batches = st.number_input(
        "æœ€å¤§ãƒãƒƒãƒå›æ•°",
        min_value=1,
        max_value=100,
        value=3,
        help="å®Ÿè¡Œæ™‚é–“ãŒé•·ããªã‚Šã™ãã‚‹ã®ã‚’é˜²ããŸã‚ã®ä¸Šé™",
        key="max_batches",
    )

    if st.button("ğŸš€ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ", disabled=not folder_id, key="import_run"):
        try:
            files_all = list_csv_recursive(folder_id)
            files = [f for f in files_all if imp_start <= parse_meta(f["path"])[2] <= imp_end]
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        imported_md5 = get_imported_md5_map()
        all_targets = [f for f in files if imported_md5.get(f["id"], "") != (f.get("md5Checksum") or "")]
        if not all_targets:
            st.success("å·®åˆ†ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆã™ã¹ã¦æœ€æ–°ï¼‰")
            st.stop()

        all_targets.sort(key=lambda f: parse_meta(f["path"])[2])
        batches = [all_targets[i : i + max_files] for i in range(0, len(all_targets), max_files)]
        if not auto_batch:
            batches = batches[:1]

        total_files = sum(len(b) for b in batches[: int(max_batches)])
        done_files = 0
        bar = st.progress(0.0)
        status = st.empty()
        all_errors = []

        for bi, batch in enumerate(batches[: int(max_batches)], start=1):
            status.text(f"ãƒãƒƒãƒ {bi}/{len(batches)}ï¼ˆ{len(batch)} ä»¶ï¼‰ã‚’å‡¦ç†ä¸­â€¦")
            entries, errors, processed_files = run_import_for_targets(batch, workers, use_copy)
            upsert_import_log(entries)
            all_errors.extend(errors)

            done_files += processed_files
            bar.progress(min(1.0, done_files / max(1, total_files)))

        status.text("")

        if len(batches) > max_batches and auto_batch:
            remaining = sum(len(b) for b in batches[int(max_batches) :])
            st.info(f"æœ€å¤§ãƒãƒƒãƒå›æ•°ã«é”ã—ã¾ã—ãŸã€‚æ®‹ã‚Š {remaining} ä»¶ã¯ã€å†åº¦ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ç¶šãã‹ã‚‰å‡¦ç†ã—ã¾ã™ã€‚")

        if all_errors:
            st.warning("ä¸€éƒ¨ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ï¼š")
            for msg in all_errors[:50]:
                st.write("- " + msg)
            if len(all_errors) > 50:
                st.write(f"... ã»ã‹ {len(all_errors) - 50} ä»¶")

        st.success(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼ˆå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«: {done_files} ä»¶ï¼‰ï¼")


# ============================================================
# ğŸ“Š å¯è¦–åŒ–ãƒ¢ãƒ¼ãƒ‰
# ============================================================
if mode == MODE_VIZ:
    st.header("DB å¯è¦–åŒ–")

    try:
        with eng.connect() as conn:
            tables = [r[0] for r in conn.execute(sa.text("SELECT tablename FROM pg_tables WHERE tablename LIKE 'slot_%'"))]
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if not tables:
        st.info("ã¾ãšå–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
        st.stop()

    default_table = "slot_ãƒ—ãƒ¬ã‚´ç«‹å·"
    default_index = next((i for i, t in enumerate(tables) if t == default_table), 0)

    table_name = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ", tables, index=default_index, key="table_select")
    if not table_name:
        st.error("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
        st.stop()

    TBL_Q = q(table_name)

    @st.cache_data(ttl=600)
    def get_date_range(table_name: str):
        TBL_Q_inner = q(table_name)
        with eng.connect() as conn:
            row = conn.execute(sa.text(f"SELECT MIN(date), MAX(date) FROM {TBL_Q_inner}")).first()
        return (row[0], row[1]) if row else (None, None)

    min_date, max_date = get_date_range(table_name)
    if not (min_date and max_date):
        st.info("ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšå–ã‚Šè¾¼ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    c1, c2 = st.columns(2)
    vis_start = c1.date_input("é–‹å§‹æ—¥", value=min_date, min_value=min_date, max_value=max_date, key=f"visual_start_{table_name}")
    vis_end = c2.date_input("çµ‚äº†æ—¥", value=max_date, min_value=min_date, max_value=max_date, key=f"visual_end_{table_name}")

    idx_ok = st.checkbox("èª­ã¿è¾¼ã¿é«˜é€ŸåŒ–ã®ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆï¼ˆæ¨å¥¨ãƒ»ä¸€åº¦ã ã‘ï¼‰", value=True, key="create_index")
    if idx_ok:
        try:
            ix1 = safe_index_name(table_name, "ix_machine_date")
            ix2 = safe_index_name(table_name, "ix_machine_slot_date")
            with eng.begin() as conn:
                conn.execute(sa.text(f'CREATE INDEX IF NOT EXISTS {q(ix1)} ON {TBL_Q} ("æ©Ÿç¨®","date");'))
                conn.execute(sa.text(f'CREATE INDEX IF NOT EXISTS {q(ix2)} ON {TBL_Q} ("æ©Ÿç¨®","å°ç•ªå·","date");'))
        except Exception as e:
            st.info(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

    @st.cache_data(ttl=600)
    def get_machines_fast(table_name: str, start: dt.date, end: dt.date):
        TBL_Q_inner = q(table_name)
        sql = sa.text(
            f'SELECT DISTINCT "æ©Ÿç¨®" FROM {TBL_Q_inner} '
            f"WHERE date BETWEEN :s AND :e ORDER BY \"æ©Ÿç¨®\""
        )
        with eng.connect() as conn:
            return [r[0] for r in conn.execute(sql, {"s": start, "e": end})]

    machines = get_machines_fast(table_name, vis_start, vis_end)
    if not machines:
        st.warning("æŒ‡å®šæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()

    machine_sel = st.selectbox("æ©Ÿç¨®é¸æŠ", machines, key="machine_select")
    show_avg = st.checkbox("å…¨å°å¹³å‡ã‚’è¡¨ç¤º", value=False, key="show_avg")

    insp = inspect(eng)
    cols_info = insp.get_columns(table_name)

    numeric_candidates: list[str] = []
    for c in cols_info:
        name = c["name"]
        if name in {"date", "æ©Ÿç¨®", "å°ç•ªå·"}:
            continue
        col_type = str(c["type"]).upper()
        if any(t in col_type for t in ("INT", "NUMERIC", "REAL", "DOUBLE", "FLOAT")):
            numeric_candidates.append(name)

    if not numeric_candidates:
        st.error("ãƒ—ãƒ­ãƒƒãƒˆå¯èƒ½ãªæ•°å€¤ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    numeric_candidates = sorted(numeric_candidates, key=lambda n: (0 if n in PROB_PLOT_COLUMNS else 1, n))

    payout_candidates = [c for c in DEFAULT_PAYOUT_COLUMNS if c in numeric_candidates]
    if payout_candidates:
        default_metric = payout_candidates[0]
    elif "åˆæˆç¢ºç‡" in numeric_candidates:
        default_metric = "åˆæˆç¢ºç‡"
    else:
        default_metric = numeric_candidates[0]

    metric_col = st.selectbox("è¡¨ç¤ºã™ã‚‹é …ç›®", numeric_candidates, index=numeric_candidates.index(default_metric), key="metric_select")
    is_prob_metric = metric_col in PROB_PLOT_COLUMNS

    @st.cache_data(ttl=600)
    def get_slots_fast(table_name: str, machine: str, start: dt.date, end: dt.date):
        TBL_Q_inner = q(table_name)
        sql = sa.text(
            f"""
            SELECT DISTINCT "å°ç•ªå·"
            FROM {TBL_Q_inner}
            WHERE "æ©Ÿç¨®" = :m
              AND date BETWEEN :s AND :e
              AND "å°ç•ªå·" IS NOT NULL
            ORDER BY "å°ç•ªå·"
            """
        )
        with eng.connect() as conn:
            vals = [r[0] for r in conn.execute(sql, {"m": machine, "s": start, "e": end})]
        return [int(v) for v in vals if v is not None]

    @st.cache_data(ttl=300)
    def fetch_plot_avg(table_name: str, machine: str, metric: str, start: dt.date, end: dt.date) -> pd.DataFrame:
        TBL_Q_inner = q(table_name)
        COL_Q = q(metric)
        sql = sa.text(
            f"""
            SELECT date, AVG({COL_Q}) AS plot_val
            FROM {TBL_Q_inner}
            WHERE "æ©Ÿç¨®" = :m
              AND date BETWEEN :s AND :e
            GROUP BY date
            ORDER BY date
            """
        )
        with eng.connect() as conn:
            return pd.read_sql(sql, conn, params={"m": machine, "s": start, "e": end})

    @st.cache_data(ttl=300)
    def fetch_plot_slot(table_name: str, machine: str, metric: str, slot: int, start: dt.date, end: dt.date) -> pd.DataFrame:
        TBL_Q_inner = q(table_name)
        COL_Q = q(metric)
        sql = sa.text(
            f"""
            SELECT date, {COL_Q} AS plot_val
            FROM {TBL_Q_inner}
            WHERE "æ©Ÿç¨®" = :m
              AND "å°ç•ªå·" = :n
              AND date BETWEEN :s AND :e
            ORDER BY date
            """
        )
        with eng.connect() as conn:
            return pd.read_sql(sql, conn, params={"m": machine, "n": int(slot), "s": start, "e": end})

    if show_avg:
        df_plot = fetch_plot_avg(table_name, machine_sel, metric_col, vis_start, vis_end)
        title = f"ğŸ“ˆ å…¨å°å¹³å‡ {metric_col} | {machine_sel}"
    else:
        slots = get_slots_fast(table_name, machine_sel, vis_start, vis_end)
        if not slots:
            st.warning("å°ç•ªå·ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.stop()
        slot_sel = st.selectbox("å°ç•ªå·", slots, key="slot_select")
        df_plot = fetch_plot_slot(table_name, machine_sel, metric_col, slot_sel, vis_start, vis_end)
        title = f"ğŸ“ˆ {metric_col} | {machine_sel} | å° {slot_sel}"

    if df_plot is None or df_plot.empty:
        st.info("ã“ã®æ¡ä»¶ã§ã¯è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æœŸé–“ã‚„æ©Ÿç¨®ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    df_plot = df_plot.copy()
    df_plot["date"] = pd.to_datetime(df_plot["date"])
    xdomain_start = df_plot["date"].min()
    xdomain_end = df_plot["date"].max()
    if pd.isna(xdomain_start) or pd.isna(xdomain_end):
        st.info("è¡¨ç¤ºå¯¾è±¡ã®æœŸé–“ã«æ—¥ä»˜ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()
    if xdomain_start == xdomain_end:
        xdomain_end = xdomain_end + pd.Timedelta(days=1)

    def prob_to_label(v):
        if v is None or pd.isna(v) or v <= 0:
            return "0"
        try:
            return "1/" + str(int(round(1.0 / float(v))))
        except Exception:
            return "0"

    if is_prob_metric:
        df_plot["inv_label"] = df_plot["plot_val"].apply(prob_to_label)
    else:
        df_plot["inv_label"] = df_plot["plot_val"].apply(lambda v: "" if v is None or pd.isna(v) else f"{v:,.0f}")

    if is_prob_metric:
        thresholds = setting_map.get(machine_sel, {})
        if thresholds:
            df_rules = pd.DataFrame([{"setting": k, "value": float(v)} for k, v in thresholds.items()])
        else:
            df_rules = pd.DataFrame(columns=["setting", "value"])
    else:
        df_rules = pd.DataFrame(columns=["setting", "value"])

    legend_sel = alt.selection_point(fields=["setting"], bind="legend")

    if is_prob_metric:
        y_axis = alt.Axis(
            title=metric_col,
            format=".4f",
            labelExpr=(
                "isValid(datum.value) && isFinite(datum.value) "
                "? (datum.value <= 0 ? '0' : '1/' + format(1/datum.value, '.0f')) "
                ": ''"
            ),
        )
    else:
        y_axis = alt.Axis(title=metric_col, format=",.0f")

    x_axis_days = alt.Axis(title="æ—¥ä»˜", format="%m/%d", labelAngle=0)
    x_scale = alt.Scale(domain=[xdomain_start, xdomain_end])
    x_field = alt.X("date:T", axis=x_axis_days, scale=x_scale)

    tooltip_fields = [alt.Tooltip("date:T", title="æ—¥ä»˜", format="%Y-%m-%d")]
    if is_prob_metric:
        tooltip_fields.append(alt.Tooltip("inv_label:N", title="è¦‹ã‹ã‘ã®ç¢ºç‡"))
        tooltip_fields.append(alt.Tooltip("plot_val:Q", title="ç¢ºç‡(0ã€œ1)", format=".4f"))
    else:
        tooltip_fields.append(alt.Tooltip("plot_val:Q", title=metric_col, format=",.0f"))

    base = (
        alt.Chart(df_plot)
        .mark_line(point=True)
        .encode(x=x_field, y=alt.Y("plot_val:Q", axis=y_axis), tooltip=tooltip_fields)
        .properties(height=400, width="container")
    )

    if not df_rules.empty:
        rules = (
            alt.Chart(df_rules)
            .mark_rule(strokeDash=[4, 2])
            .encode(
                y="value:Q",
                color=alt.Color("setting:N", legend=alt.Legend(title="è¨­å®šãƒ©ã‚¤ãƒ³")),
                opacity=alt.condition(legend_sel, alt.value(1), alt.value(0.15)),
            )
        )
        final_chart = (base + rules).add_params(legend_sel)
    else:
        final_chart = base

    st.subheader(title)
    st.altair_chart(final_chart, use_container_width=True)


# ============================================================
# ğŸ§  MLãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ¢ãƒ¼ãƒ‰ï¼ˆäºˆæ¸¬UIä»˜ãï¼‰
# ============================================================
if mode == MODE_ML:
    st.header("ğŸ§  æ©Ÿæ¢°å­¦ç¿’ / æ™‚ç³»åˆ—åŸºç›¤ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆï¼‹ äºˆæ¸¬UIï¼‰")

    # --- ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ ---
    try:
        with eng.connect() as conn:
            tables = [r[0] for r in conn.execute(sa.text("SELECT tablename FROM pg_tables WHERE tablename LIKE 'slot_%'"))]
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if not tables:
        st.info("ã¾ãšå–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
        st.stop()

    default_table = "slot_ãƒ—ãƒ¬ã‚´ç«‹å·"
    default_index = next((i for i, t in enumerate(tables) if t == default_table), 0)

    table_name = st.selectbox("åº—èˆ—ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆslot_â—¯â—¯ï¼‰", tables, index=default_index, key="ml_table")
    TBL_Q = q(table_name)

    # --- æ—¥ä»˜ç¯„å›² ---
    @st.cache_data(ttl=600)
    def get_date_range_ml(table_name: str):
        TBL_Q_inner = q(table_name)
        with eng.connect() as conn:
            row = conn.execute(sa.text(f"SELECT MIN(date), MAX(date) FROM {TBL_Q_inner}")).first()
        return (row[0], row[1]) if row else (None, None)

    min_date, max_date = get_date_range_ml(table_name)
    if not (min_date and max_date):
        st.warning("ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    c1, c2 = st.columns(2)
    ml_start = c1.date_input("é–‹å§‹æ—¥", value=min_date, min_value=min_date, max_value=max_date, key="ml_start")
    ml_end = c2.date_input("çµ‚äº†æ—¥", value=max_date, min_value=min_date, max_value=max_date, key="ml_end")

    # --- æ©Ÿç¨®ä¸€è¦§ ---
    @st.cache_data(ttl=600)
    def get_machines_ml(table_name: str, start: dt.date, end: dt.date):
        TBL_Q_inner = q(table_name)
        sql = sa.text(
            f'SELECT DISTINCT "æ©Ÿç¨®" FROM {TBL_Q_inner} '
            f"WHERE date BETWEEN :s AND :e ORDER BY \"æ©Ÿç¨®\""
        )
        with eng.connect() as conn:
            return [r[0] for r in conn.execute(sql, {"s": start, "e": end})]

    machines = get_machines_ml(table_name, ml_start, ml_end)
    if not machines:
        st.warning("æŒ‡å®šæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    machine_sel = st.selectbox("æ©Ÿç¨®", machines, key="ml_machine")

    # --- æ•°å€¤ã‚«ãƒ©ãƒ å€™è£œï¼ˆDBå®šç¾©ã‹ã‚‰ï¼‰ ---
    insp = inspect(eng)
    cols_info = insp.get_columns(table_name)

    numeric_candidates: list[str] = []
    for c in cols_info:
        name = c["name"]
        if name in {"date", "æ©Ÿç¨®", "å°ç•ªå·"}:
            continue
        col_type = str(c["type"]).upper()
        if any(t in col_type for t in ("INT", "NUMERIC", "REAL", "DOUBLE", "FLOAT")):
            numeric_candidates.append(name)

    if not numeric_candidates:
        st.error("æ•°å€¤ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    prob_cols = [c for c in ["åˆæˆç¢ºç‡", "BBç¢ºç‡", "RBç¢ºç‡", "ARTç¢ºç‡"] if c in numeric_candidates]
    other_cols = [c for c in numeric_candidates if c not in prob_cols]
    numeric_candidates = prob_cols + sorted(other_cols)

    # --- ç²’åº¦ ---
    gran = st.radio("ç²’åº¦", ["å°åˆ¥ï¼ˆå°ç•ªå·ã”ã¨ï¼‰", "å…¨å°å¹³å‡ï¼ˆdateã§é›†ç´„ï¼‰"], horizontal=True, key="ml_gran")

    # --- äºˆæ¸¬ã‚¿ã‚¹ã‚¯ï¼ˆ2æŠï¼‰ ---
    TASK_SETTING = "â‘  è¨­å®šæ¨å®šï¼ˆåˆæˆç¢ºç‡â†’setting.jsonï¼‰"
    TASK_PAYOUT = "â‘¡ å·®æšç³»äºˆæ¸¬ï¼ˆå·®æš/å·®ç‰/æœ€å¤§å·®ç‰/æœ€å¤§æŒç‰ï¼‰"

    default_task = TASK_SETTING if (machine_sel in setting_map and setting_map.get(machine_sel)) else TASK_PAYOUT
    task = st.radio(
        "äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³",
        [TASK_SETTING, TASK_PAYOUT],
        index=[TASK_SETTING, TASK_PAYOUT].index(default_task),
        horizontal=True,
        key="ml_task",
    )

    # --- target æ±ºå®š ---
    if task == TASK_SETTING:
        if "åˆæˆç¢ºç‡" not in numeric_candidates:
            st.error("ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯ã€åˆæˆç¢ºç‡ã€ãŒç„¡ã„ã®ã§è¨­å®šæ¨å®šã¯ã§ãã¾ã›ã‚“ã€‚å·®æšç³»äºˆæ¸¬ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
            st.stop()
        target_col = "åˆæˆç¢ºç‡"
        st.caption("åˆæˆç¢ºç‡(0ã€œ1)ã‚’äºˆæ¸¬ â†’ äºˆæ¸¬å€¤ã‚’ setting.json ã®è¨­å®šãƒ©ã‚¤ãƒ³ã«æœ€ã‚‚è¿‘ã„è¨­å®šã¸å‰²ã‚Šå½“ã¦ã¾ã™ã€‚")
        if not setting_map.get(machine_sel, {}):
            st.warning("setting.json ã«ã“ã®æ©Ÿç¨®ã®è¨­å®šãƒ©ã‚¤ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆè¨­å®šæ¨å®šã®ãƒ©ãƒ™ãƒ«ä»˜ã‘ãŒã§ãã¾ã›ã‚“ï¼‰ã€‚")
    else:
        payout_cands = build_payout_candidates(numeric_candidates)
        if not payout_cands:
            st.warning("å·®æš/å·®ç‰/æœ€å¤§å·®ç‰/æœ€å¤§æŒç‰ ç³»ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ•°å€¤ã‚«ãƒ©ãƒ å…ˆé ­ã‚’targetã«ã—ã¾ã™ã€‚")
            target_col = numeric_candidates[0]
        else:
            labels = [c["label"] for c in payout_cands]
            picked = st.selectbox("targetï¼ˆå·®æšç›¸å½“ï¼‰ã«ä½¿ã†åˆ—", options=labels, index=0, key="ml_payout_target_pick")
            picked_obj = payout_cands[labels.index(picked)]
            target_col = picked_obj["source"]
        st.caption(f"ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã¯ **{target_col}** ã‚’ã€Œå·®æšç›¸å½“ã€ã¨ã—ã¦äºˆæ¸¬ã—ã¾ã™ï¼ˆåº—ã”ã¨ã«åˆ—ãŒé•ã†ãŸã‚ï¼‰ã€‚")

    st.write("âœ… ä»Šå›äºˆæ¸¬ã™ã‚‹ã‚‚ã®ï¼ˆtargetï¼‰:", target_col)

    # --- ç‰¹å¾´é‡ï¼ˆå…±å¤‰é‡ï¼‰ä»»æ„ ---
    default_feats = [c for c in ["ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°", "RBå›æ•°", "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰"] if c in numeric_candidates]
    feats = st.multiselect("ç‰¹å¾´é‡ï¼ˆå…±å¤‰é‡ï¼‰ã¨ã—ã¦ä»˜ã‘ãŸã„ã‚«ãƒ©ãƒ ï¼ˆä»»æ„ï¼‰", numeric_candidates, default=default_feats, key="ml_feats")

    # --- å‡ºåŠ›å½¢å¼ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã ã‘ã«å½±éŸ¿ã€‚äºˆæ¸¬UIã¯å¸¸ã«é•·å½¢å¼ã‚’å†…éƒ¨ä½¿ç”¨ï¼‰ ---
    out_fmt = st.selectbox(
        "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼",
        ["é•·å½¢å¼ï¼ˆChronos-2 / TimesFMå‘ã‘ï¼‰", "åºƒå½¢å¼ï¼ˆtimestamp index, series columnsï¼‰"],
        key="ml_outfmt",
    )

    # --- å°ç•ªå·ï¼ˆå°åˆ¥ã®ã¨ãã ã‘ï¼‰ ---
    slots_sel: list[int] | None = None
    if gran == "å°åˆ¥ï¼ˆå°ç•ªå·ã”ã¨ï¼‰":
        @st.cache_data(ttl=600)
        def get_slots_ml(table_name: str, machine: str, start: dt.date, end: dt.date):
            TBL_Q_inner = q(table_name)
            sql = sa.text(
                f"""
                SELECT DISTINCT "å°ç•ªå·"
                FROM {TBL_Q_inner}
                WHERE "æ©Ÿç¨®" = :m
                  AND date BETWEEN :s AND :e
                  AND "å°ç•ªå·" IS NOT NULL
                ORDER BY "å°ç•ªå·"
                """
            )
            with eng.connect() as conn:
                vals = [r[0] for r in conn.execute(sql, {"m": machine, "s": start, "e": end})]
            return [int(v) for v in vals if v is not None]

        slots = get_slots_ml(table_name, machine_sel, ml_start, ml_end)
        if not slots:
            st.warning("å°ç•ªå·ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        slots_sel = st.multiselect("å¯¾è±¡å°ç•ªå·ï¼ˆæœªé¸æŠãªã‚‰å…¨å°ï¼‰", slots, default=[], key="ml_slots_multi")

    # --- DBã‹ã‚‰å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ ---
    @st.cache_data(ttl=300)
    def fetch_ml_df(
        table_name: str,
        machine: str,
        start: dt.date,
        end: dt.date,
        cols: list[str],
        slots: list[int] | None,
        avg: bool,
    ) -> pd.DataFrame:
        TBL_Q_inner = q(table_name)

        slots_clause = ""
        bindparams = []
        params = {"m": machine, "s": start, "e": end}

        if slots is not None and len(slots) > 0:
            slots_clause = ' AND "å°ç•ªå·" IN :slots'
            bindparams.append(sa.bindparam("slots", expanding=True))
            params["slots"] = slots

        if avg:
            agg_cols = ", ".join([f"AVG({q(c)}) AS {q(c)}" for c in cols])
            sql = sa.text(
                f"""
                SELECT
                    date,
                    :m AS "æ©Ÿç¨®",
                    NULL::int AS "å°ç•ªå·",
                    {agg_cols}
                FROM {TBL_Q_inner}
                WHERE "æ©Ÿç¨®" = :m
                  AND date BETWEEN :s AND :e
                  {slots_clause}
                GROUP BY date
                ORDER BY date
                """
            )
        else:
            select_cols = ["date", '"æ©Ÿç¨®"', '"å°ç•ªå·"'] + [q(c) for c in cols]
            sql = sa.text(
                f"""
                SELECT {", ".join(select_cols)}
                FROM {TBL_Q_inner}
                WHERE "æ©Ÿç¨®" = :m
                  AND date BETWEEN :s AND :e
                  {slots_clause}
                ORDER BY date, "å°ç•ªå·"
                """
            )

        if bindparams:
            sql = sql.bindparams(*bindparams)

        with eng.connect() as conn:
            return pd.read_sql(sql, conn, params=params)

    cols_out = list(dict.fromkeys([target_col] + feats))
    avg = (gran == "å…¨å°å¹³å‡ï¼ˆdateã§é›†ç´„ï¼‰")
    df = fetch_ml_df(table_name, machine_sel, ml_start, ml_end, cols_out, slots_sel, avg)

    if df.empty:
        st.warning("ã“ã®æ¡ä»¶ã§ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    # --- series idï¼ˆç³»åˆ—IDï¼‰ ---
    def make_id(row):
        slot = row["å°ç•ªå·"]
        if pd.isna(slot):
            return f"{table_name}|{row['æ©Ÿç¨®']}|AVG"
        return f"{table_name}|{row['æ©Ÿç¨®']}|{int(slot)}"

    df = df.copy()
    df["id"] = df.apply(make_id, axis=1)
    df["timestamp"] = pd.to_datetime(df["date"])

    # --- é•·å½¢å¼ï¼ˆäºˆæ¸¬UIç”¨ï¼‰ ---
    out_long = df.rename(columns={target_col: "target"}).copy()
    keep_cols = ["id", "timestamp", "target"] + [c for c in feats if c in out_long.columns]
    out_long = out_long[keep_cols].sort_values(["id", "timestamp"])

    # --- åºƒå½¢å¼ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰ ---
    out_wide = df.pivot_table(index="timestamp", columns="id", values=target_col, aggfunc="mean").sort_index()

    # --- ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ & ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---
    st.subheader("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
    if out_fmt.startswith("é•·å½¢å¼"):
        st.dataframe(out_long.head(50), use_container_width=True)
        st.download_button(
            "â¬‡ï¸ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆé•·å½¢å¼ï¼‰",
            data=out_long.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
            file_name=safe_filename(f"ml_long_{table_name}_{machine_sel}_{ml_start}_{ml_end}.csv"),
            mime="text/csv",
        )
        st.caption("Chronos-2 / TimesFMå‘ã‘ï¼ˆid,timestamp,targetï¼‰")
    else:
        st.dataframe(out_wide.head(50), use_container_width=True)
        st.download_button(
            "â¬‡ï¸ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåºƒå½¢å¼ï¼‰",
            data=out_wide.to_csv(index=True, encoding="utf-8-sig").encode("utf-8-sig"),
            file_name=safe_filename(f"ml_wide_{table_name}_{machine_sel}_{ml_start}_{ml_end}.csv"),
            mime="text/csv",
        )
        st.caption("wideå½¢å¼ï¼ˆtimestamp index, series columnsï¼‰")

    # ============================================================
    # ğŸ”® äºˆæ¸¬ã‚’UIã§å®Ÿè¡Œï¼ˆCLIä¸è¦ï¼‰
    # ============================================================
    st.divider()
    st.subheader("ğŸ”® æ™‚ç³»åˆ—åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ï¼ˆUIå®Ÿè¡Œï¼‰")

    uniq_ids = out_long["id"].unique().tolist()
    st.caption(f"ç³»åˆ—æ•°: {len(uniq_ids)}ï¼ˆå¤šã„ã¨é‡ã„ã®ã§ã€ã¾ãšã¯å°‘æ•°ã§è©¦ã™ã®ãŒãŠã™ã™ã‚ï¼‰")

    max_n = min(200, len(uniq_ids))
    n_pick = st.slider(
        "å€™è£œã«å‡ºã™ç³»åˆ—æ•°ï¼ˆå…ˆé ­ã‹ã‚‰ï¼‰",
        1,
        max_n if max_n >= 1 else 1,
        min(20, max_n) if max_n >= 1 else 1,
        key="fcst_topn",
    )
    cand_ids = uniq_ids[:n_pick]

    pick_ids = st.multiselect("äºˆæ¸¬ã™ã‚‹ç³»åˆ—ï¼ˆidï¼‰", options=cand_ids, default=cand_ids[:1], key="fcst_ids")
    if not pick_ids:
        st.warning("å°‘ãªãã¨ã‚‚1ã¤é¸ã‚“ã§ãã ã•ã„ã€‚")
        st.stop()

    df_long_use = out_long[out_long["id"].isin(pick_ids)].copy()

    c1, c2, c3, c4 = st.columns(4)
    model_name = c1.selectbox("ãƒ¢ãƒ‡ãƒ«", ["chronos2", "timesfm"], index=0, key="fcst_model")
    horizon = c2.slider("äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆæ—¥æ•°ï¼‰", 1, 60, 14, key="fcst_h")
    device_map = c3.selectbox("ãƒ‡ãƒã‚¤ã‚¹ï¼ˆChronos-2ï¼‰", ["cpu", "cuda"], index=0, key="fcst_dev")
    freq = c4.selectbox("freqï¼ˆTimesFMï¼‰", ["D", "W", "M"], index=0, key="fcst_freq")

    if st.button("ğŸš€ äºˆæ¸¬ã‚’å®Ÿè¡Œ", key="run_forecast"):
        try:
            with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ã—ã¦äºˆæ¸¬ä¸­â€¦ï¼ˆåˆå›ã¯é‡ã„ã§ã™ï¼‰"):
                if model_name == "chronos2":
                    pred = forecast_with_chronos2(
                        df_long_use[["id", "timestamp", "target"]],
                        horizon=horizon,
                        device_map=device_map,
                    )
                else:
                    pred = forecast_with_timesfm(
                        df_long_use[["id", "timestamp", "target"]],
                        horizon=horizon,
                        freq=freq,
                    )

            # è¨­å®šæ¨å®šï¼ˆåˆæˆç¢ºç‡ã®å ´åˆã ã‘ï¼‰
            if task == TASK_SETTING:
                thresholds = setting_map.get(machine_sel, {})
                if thresholds:
                    pred = pred.copy()
                    pred["pred_setting"] = pred["yhat"].apply(lambda p: score_setting_by_denom(p, thresholds))
                    pred["pred_1_over"] = pred["yhat"].apply(
                        lambda p: 0 if (p is None or (not np.isfinite(p)) or p <= 0) else int(round(1.0 / p))
                    )
                else:
                    pred = pred.copy()
                    pred["pred_setting"] = None
                    pred["pred_1_over"] = None

            # ============================================================
            # âœ… äºˆæ¸¬çµæœã‚’ã€Œã‚ã‹ã‚Šã‚„ã™ãè¡¨ç¤ºã€
            # ============================================================
            pred_view = pred.copy()
            pred_view["timestamp"] = pd.to_datetime(pred_view["timestamp"])

            if task == TASK_SETTING:
                pred_view["yhat_denom"] = pred_view["yhat"].apply(
                    lambda p: np.nan if (p is None or (not np.isfinite(p)) or p <= 0) else round(1.0 / float(p))
                )
                pred_view["yhat_disp"] = pred_view["yhat_denom"].apply(lambda d: "â€”" if pd.isna(d) else f"1/{int(d)}")
            else:
                pred_view["yhat_disp"] = pred_view["yhat"].apply(
                    lambda v: "â€”" if (v is None or pd.isna(v)) else f"{int(round(float(v))):,}"
                )

            hist = df_long_use[["id", "timestamp", "target"]].copy()
            hist["timestamp"] = pd.to_datetime(hist["timestamp"])
            hist = hist.sort_values(["id", "timestamp"])

            st.success("äºˆæ¸¬å®Œäº†ï¼")
            st.subheader("ğŸ“Œ äºˆæ¸¬çµæœï¼ˆè¦‹ã‚„ã™ã„è¡¨ç¤ºï¼‰")

            vmode = st.radio("è¡¨ç¤º", ["ã‚°ãƒ©ãƒ•ä¸­å¿ƒ", "è¡¨ä¸­å¿ƒ", "ä¸¡æ–¹"], horizontal=True, index=2, key="pred_view_mode")
            show_band = st.checkbox("ä¸ç¢ºå®Ÿæ€§ã®å¸¯ã‚’è¡¨ç¤ºï¼ˆChronos-2ã®0.1/0.9ãŒã‚ã‚‹å ´åˆï¼‰", value=True, key="pred_show_band")
            hist_days = st.slider("å®Ÿç¸¾ã‚’ä½•æ—¥åˆ†é‡ã­ã¦è¡¨ç¤ºã™ã‚‹ï¼Ÿ", 7, 90, 30, step=1, key="pred_hist_days")

            view_ids = pred_view["id"].unique().tolist()
            tabs = st.tabs([f"ğŸ§© {i}" for i in view_ids])

            for ti, _id in enumerate(view_ids):
                with tabs[ti]:
                    p1 = pred_view[pred_view["id"] == _id].sort_values("timestamp").copy()
                    h1 = hist[hist["id"] == _id].sort_values("timestamp").copy()

                    if not h1.empty:
                        last_ts = h1["timestamp"].max()
                        h1 = h1[h1["timestamp"] >= (last_ts - pd.Timedelta(days=hist_days))].copy()

                    # ---- ã‚µãƒãƒªãƒ¼ ----
                    cA, cB, cC, cD = st.columns(4)
                    next_row = p1.iloc[0] if len(p1) > 0 else None

                    if task == TASK_SETTING:
                        next_disp = next_row["yhat_disp"] if next_row is not None else "â€”"
                        next_set = next_row.get("pred_setting", "â€”") if next_row is not None else "â€”"
                        cA.metric("æ¬¡ã®æ—¥ã®äºˆæ¸¬ï¼ˆåˆæˆï¼‰", next_disp)
                        cB.metric("æ¬¡ã®æ—¥ã®äºˆæ¸¬è¨­å®š", str(next_set))
                    else:
                        next_disp = next_row["yhat_disp"] if next_row is not None else "â€”"
                        cA.metric(f"æ¬¡ã®æ—¥ã®äºˆæ¸¬ï¼ˆ{target_col}ï¼‰", next_disp)
                        cB.metric("ï¼ˆç©ºï¼‰", "")

                    if not p1.empty:
                        avg_val = float(p1["yhat"].mean())
                        if task == TASK_SETTING:
                            avg_disp = "â€”" if avg_val <= 0 else f"1/{int(round(1/avg_val))}"
                        else:
                            avg_disp = f"{int(round(avg_val)):,}"
                    else:
                        avg_disp = "â€”"
                    cC.metric("äºˆæ¸¬æœŸé–“ã®å¹³å‡", avg_disp)

                    if len(p1) >= 2:
                        slope = float(p1["yhat"].iloc[-1] - p1["yhat"].iloc[0])
                        if task == TASK_SETTING:
                            d0 = p1["yhat_denom"].iloc[0] if "yhat_denom" in p1.columns else np.nan
                            d1 = p1["yhat_denom"].iloc[-1] if "yhat_denom" in p1.columns else np.nan
                            slope_disp = "â€”" if (pd.isna(d0) or pd.isna(d1)) else f"{int(d1 - d0):+d} (åˆ†æ¯å·®)"
                        else:
                            slope_disp = f"{int(round(slope)):+,}"
                    else:
                        slope_disp = "â€”"
                    cD.metric("æœŸé–“ã®å¤‰åŒ–é‡ï¼ˆã–ã£ãã‚Šï¼‰", slope_disp)

                    # ---- ã‚°ãƒ©ãƒ•ï¼ˆå®Ÿç¸¾ï¼‹äºˆæ¸¬ï¼‰----
                    if vmode in ("ã‚°ãƒ©ãƒ•ä¸­å¿ƒ", "ä¸¡æ–¹"):
                        chart_hist = (
                            alt.Chart(h1)
                            .mark_line(point=True)
                            .encode(
                                x=alt.X("timestamp:T", title="æ—¥ä»˜"),
                                y=alt.Y("target:Q", title=f"å®Ÿç¸¾ï¼ˆ{target_col}ï¼‰"),
                                tooltip=[
                                    alt.Tooltip("timestamp:T", title="æ—¥ä»˜", format="%Y-%m-%d"),
                                    alt.Tooltip("target:Q", title="å®Ÿç¸¾", format=".6f" if task == TASK_SETTING else ",.0f"),
                                ],
                            )
                        )

                        chart_pred = (
                            alt.Chart(p1)
                            .mark_line(point=True, strokeDash=[4, 2])
                            .encode(
                                x=alt.X("timestamp:T", title="æ—¥ä»˜"),
                                y=alt.Y("yhat:Q", title=f"äºˆæ¸¬ï¼ˆ{target_col}ï¼‰"),
                                tooltip=[
                                    alt.Tooltip("timestamp:T", title="æ—¥ä»˜", format="%Y-%m-%d"),
                                    alt.Tooltip("yhat_disp:N", title="äºˆæ¸¬(è¡¨ç¤ºç”¨)"),
                                    alt.Tooltip("yhat:Q", title="äºˆæ¸¬(æ•°å€¤)", format=".6f" if task == TASK_SETTING else ",.0f"),
                                ],
                            )
                        )

                        band = None
                        if show_band and ("0.1" in p1.columns) and ("0.9" in p1.columns):
                            band = (
                                alt.Chart(p1)
                                .mark_area(opacity=0.2)
                                .encode(
                                    x="timestamp:T",
                                    y=alt.Y("0.1:Q", title=""),
                                    y2="0.9:Q",
                                    tooltip=[
                                        alt.Tooltip("timestamp:T", title="æ—¥ä»˜", format="%Y-%m-%d"),
                                        alt.Tooltip("0.1:Q", title="ä¸‹æŒ¯ã‚Œ(0.1)", format=".6f"),
                                        alt.Tooltip("0.9:Q", title="ä¸ŠæŒ¯ã‚Œ(0.9)", format=".6f"),
                                    ],
                                )
                            )

                        final = (chart_hist + band + chart_pred) if band is not None else (chart_hist + chart_pred)
                        st.altair_chart(final.properties(height=320), use_container_width=True)

                    # ---- è¡¨ï¼ˆèª­ã¿ã‚„ã™ãï¼‰----
                    if vmode in ("è¡¨ä¸­å¿ƒ", "ä¸¡æ–¹"):
                        show_cols = ["timestamp", "yhat_disp"]
                        rename_map = {"timestamp": "æ—¥ä»˜", "yhat_disp": "äºˆæ¸¬å€¤"}

                        if task == TASK_SETTING:
                            if "pred_setting" in p1.columns:
                                show_cols += ["pred_setting"]
                                rename_map["pred_setting"] = "äºˆæ¸¬è¨­å®š"
                            show_cols += ["yhat"]
                            rename_map["yhat"] = "äºˆæ¸¬(ç¢ºç‡0-1)"
                        else:
                            show_cols += ["yhat"]
                            rename_map["yhat"] = f"äºˆæ¸¬({target_col})"

                        tdf = p1[show_cols].copy().rename(columns=rename_map)
                        st.dataframe(tdf, use_container_width=True, height=260)

                        light = p1[["timestamp", "yhat_disp"]].copy()
                        light = light.rename(columns={"timestamp": "date", "yhat_disp": "prediction"})
                        st.download_button(
                            "â¬‡ï¸ ã“ã®å°ã ã‘ã®è»½é‡CSVï¼ˆdate,predictionï¼‰",
                            data=light.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                            file_name=safe_filename(f"pred_light_{model_name}_{_id}.csv"),
                            mime="text/csv",
                            key=f"dl_light_{_id}",
                        )

            # å…¨ä½“CSV
            fname = safe_filename(
                f"pred_{model_name}_{'setting' if task==TASK_SETTING else 'payout'}_{table_name}_{machine_sel}_{ml_start}_{ml_end}.csv"
            )
            st.download_button(
                "â¬‡ï¸ äºˆæ¸¬çµæœCSVï¼ˆå…¨ä½“ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=pred.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                file_name=fname,
                mime="text/csv",
                key="dl_pred_all",
            )

        except ModuleNotFoundError as e:
            st.error(
                "å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå…¥ã£ã¦ã„ã¾ã›ã‚“ã€‚\n"
                "requirements.txt ã« torch / transformers / accelerate / chronos-forecasting / timesfm ã‚’è¿½åŠ ã—ã¦å†ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦ãã ã•ã„ã€‚\n"
                f"è©³ç´°: {e}"
            )
        except Exception as e:
            st.error(f"äºˆæ¸¬å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼: {e}")
