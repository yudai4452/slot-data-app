# app.py
import io
import re
import datetime as dt
import pandas as pd
import streamlit as st
import sqlalchemy as sa
from sqlalchemy import inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import altair as alt
import json

# ======================== åŸºæœ¬è¨­å®š ========================
st.set_page_config(page_title="Slot Manager", layout="wide")
mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿", "ğŸ“Š å¯è¦–åŒ–"))
st.title("ğŸ° Slot Data Manager & Visualizer")

SA_INFO = st.secrets["gcp_service_account"]
PG_CFG  = st.secrets["connections"]["slot_db"]

# ======================== è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« ========================
with open("setting.json", encoding="utf-8") as f:
    setting_map = json.load(f)

# ======================== æ¥ç¶š ========================
@st.cache_resource
def gdrive():
    try:
        creds = Credentials.from_service_account_info(
            SA_INFO, scopes=["https://www.googleapis.com/auth/drive.readonly"]
        )
        return build("drive", "v3", credentials=creds)
    except Exception as e:
        st.error(f"Driveèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

drive = gdrive()

@st.cache_resource
def engine():
    try:
        url = (
            f"postgresql+psycopg2://{PG_CFG.username}:{PG_CFG.password}"
            f"@{PG_CFG.host}:{PG_CFG.port}/{PG_CFG.database}?sslmode=require"
        )
        return sa.create_engine(url, pool_pre_ping=True)
    except Exception as e:
        st.error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

eng = engine()
if eng is None:
    st.stop()

# ======================== ã‚«ãƒ©ãƒ å®šç¾©ãƒãƒƒãƒ”ãƒ³ã‚° ========================
COLUMN_MAP = {
    "ãƒ¡ãƒƒã‚»æ­¦è”µå¢ƒ": {
        "å°ç•ªå·":           "å°ç•ªå·",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":     "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":     "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°":          "BBå›æ•°",
        "RBå›æ•°":          "RBå›æ•°",
        "ARTå›æ•°":         "ARTå›æ•°",
        "æœ€å¤§æŒã¡ç‰":       "æœ€å¤§æŒç‰",
        "æœ€å¤§æŒç‰":         "æœ€å¤§æŒç‰",
        "BBç¢ºç‡":          "BBç¢ºç‡",
        "RBç¢ºç‡":          "RBç¢ºç‡",
        "ARTç¢ºç‡":         "ARTç¢ºç‡",
        "åˆæˆç¢ºç‡":        "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
    },
    "ã‚¸ãƒ£ãƒ³ã‚¸ãƒ£ãƒ³ãƒãƒ¼ãƒ«ã‚´ãƒƒãƒˆåˆ†å€æ²³åŸ": {
        "å°ç•ªå·":           "å°ç•ªå·",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":     "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°":          "BBå›æ•°",
        "RBå›æ•°":          "RBå›æ•°",
        "æœ€å¤§æŒã¡ç‰":       "æœ€å¤§æŒç‰",
        "æœ€å¤§æŒç‰":         "æœ€å¤§æŒç‰",
        "BBç¢ºç‡":          "BBç¢ºç‡",
        "RBç¢ºç‡":          "RBç¢ºç‡",
        "åˆæˆç¢ºç‡":        "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":     "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
    "ãƒ—ãƒ¬ã‚´ç«‹å·": {
        "å°ç•ªå·":           "å°ç•ªå·",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":     "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°":          "BBå›æ•°",
        "RBå›æ•°":          "RBå›æ•°",
        "æœ€å¤§å·®ç‰":         "æœ€å¤§å·®ç‰",
        "BBç¢ºç‡":          "BBç¢ºç‡",
        "RBç¢ºç‡":          "RBç¢ºç‡",
        "åˆæˆç¢ºç‡":        "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":     "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
}

# ======================== Drive: å†å¸° + ãƒšãƒ¼ã‚¸ãƒ³ã‚° ========================
@st.cache_data(show_spinner=False)
def list_csv_recursive(folder_id: str):
    if drive is None:
        raise RuntimeError("Driveæœªæ¥ç¶šã§ã™")
    all_files, queue = [], [(folder_id, "")]
    while queue:
        fid, cur = queue.pop()
        page_token = None
        while True:
            res = drive.files().list(
                q=f"'{fid}' in parents and trashed=false",
                fields="nextPageToken, files(id,name,mimeType)",
                pageSize=1000, pageToken=page_token
            ).execute()
            for f in res.get("files", []):
                if f["mimeType"] == "application/vnd.google-apps.folder":
                    queue.append((f["id"], f"{cur}/{f['name']}"))
                elif f["name"].lower().endswith(".csv"):
                    all_files.append({**f, "path": f"{cur}/{f['name']}"})
            page_token = res.get("nextPageToken")
            if not page_token:
                break
    return all_files

# ======================== ãƒ¡ã‚¿æƒ…å ±è§£æï¼ˆæ­£è¦è¡¨ç¾ã§æ—¥ä»˜æŠ½å‡ºï¼‰ ========================
DATE_RE = re.compile(r"\d{4}-\d{2}-\d{2}")

def parse_meta(path: str):
    parts = path.strip("/").split("/")
    if len(parts) < 3:
        raise ValueError(f"ãƒ‘ã‚¹ãŒçŸ­ã™ãã¾ã™: {path}")
    store, machine = parts[-3], parts[-2]
    m = DATE_RE.search(parts[-1])
    if not m:
        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜(YYYY-MM-DD)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {parts[-1]}")
    date = dt.date.fromisoformat(m.group(0))
    return store, machine, date

# ======================== æ­£è¦åŒ– ========================
def normalize(df_raw: pd.DataFrame, store: str) -> pd.DataFrame:
    df = df_raw.rename(columns=COLUMN_MAP[store])

    prob_cols = ["BBç¢ºç‡", "RBç¢ºç‡", "ARTç¢ºç‡", "åˆæˆç¢ºç‡"]
    for col in prob_cols:
        if col not in df.columns:
            continue
        ser = df[col].astype(str)
        mask_div = ser.str.contains("/", na=False)

        # "1/x" å½¢å¼
        if mask_div.any():
            denom = pd.to_numeric(
                ser[mask_div].str.split("/", expand=True)[1],
                errors="coerce"
            )
            val = 1.0 / denom
            val[(denom <= 0) | (~denom.notna())] = 0  # 0ãƒ»è² å€¤ãƒ»æ¬ æã¯0
            df.loc[mask_div, col] = val

        # æ•°å€¤ç›´æ›¸ãï¼ˆ>1 ã¯ 1/å€¤, <=1 ã¯ãã®ã¾ã¾ï¼‰
        num = pd.to_numeric(ser[~mask_div], errors="coerce")
        conv = num.copy()
        conv[num > 1] = 1.0 / num[num > 1]
        df.loc[~mask_div, col] = conv.fillna(0)

        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0.0).astype(float)

    # æ•´æ•°ã‚«ãƒ©ãƒ 
    int_cols = [
        "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°",
        "RBå›æ•°", "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
    ]
    for col in int_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")

    return df

# ======================== èª­ã¿è¾¼ã¿ + æ­£è¦åŒ– ========================
@st.cache_data(show_spinner=False)
def load_and_normalize(raw_bytes: bytes, store: str) -> pd.DataFrame:
    header = pd.read_csv(io.BytesIO(raw_bytes), encoding="shift_jis", nrows=0).columns.tolist()
    mapping_keys = list(dict.fromkeys(COLUMN_MAP[store].keys()))
    usecols = [col for col in mapping_keys if col in header]
    df_raw = pd.read_csv(
        io.BytesIO(raw_bytes),
        encoding="shift_jis",
        usecols=usecols,
        on_bad_lines="skip",
        engine="c",
    )
    return normalize(df_raw, store)

# ======================== ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆPK/Indexï¼‰ ========================
def ensure_store_table(store: str):
    safe = "slot_" + store.replace(" ", "_")
    insp = inspect(eng)
    meta = sa.MetaData()
    if not insp.has_table(safe):
        cols = [
            sa.Column("date", sa.Date, nullable=False),
            sa.Column("æ©Ÿç¨®", sa.Text, nullable=False),
            sa.Column("å°ç•ªå·", sa.Integer, nullable=False),
        ]
        unique_cols = list(dict.fromkeys(COLUMN_MAP[store].values()))
        numeric_int = {
            "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°", "RBå›æ•°",
            "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
        }
        for col_name in unique_cols:
            if col_name in {"date", "æ©Ÿç¨®", "å°ç•ªå·"}:
                continue
            if col_name in numeric_int:
                cols.append(sa.Column(col_name, sa.Integer))
            else:
                cols.append(sa.Column(col_name, sa.Float))
        t = sa.Table(safe, meta, *cols, sa.PrimaryKeyConstraint("date", "æ©Ÿç¨®", "å°ç•ªå·"))
        meta.create_all(eng)
        # ä»£è¡¨çš„ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        with eng.begin() as conn:
            conn.execute(sa.text(f'CREATE INDEX IF NOT EXISTS idx_{safe}_kisyudate ON "{safe}" ("æ©Ÿç¨®", "date");'))
            conn.execute(sa.text(f'CREATE INDEX IF NOT EXISTS idx_{safe}_date ON "{safe}" ("date");'))
            conn.execute(sa.text(f'CREATE INDEX IF NOT EXISTS idx_{safe}_slotdate ON "{safe}" ("æ©Ÿç¨®","å°ç•ªå·","date");'))
        return t
    return sa.Table(safe, meta, autoload_with=eng)

# ======================== ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ ========================
def upsert_dataframe(conn, table, df: pd.DataFrame, pk=("date", "æ©Ÿç¨®", "å°ç•ªå·")):
    rows = df.to_dict(orient="records")
    if not rows:
        return
    stmt = pg_insert(table).values(rows)
    update_cols = {c.name: stmt.excluded[c.name] for c in table.c if c.name not in pk}
    stmt = stmt.on_conflict_do_update(index_elements=list(pk), set_=update_cols)
    conn.execute(stmt)

# ========================= å–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ =========================
if mode == "ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿":
    st.header("Google Drive â†’ Postgres ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
    folder_options = {
        "ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨": "1MRQFPBahlSwdwhrqqBzudXL18y8-qOb8",
        "ğŸš€ æœ¬ç•ªç”¨":   "1hX8GQRuDm_E1A1Cu_fXorvwxv-XF7Ynl",
    }
    sel_label = st.selectbox("ãƒ•ã‚©ãƒ«ãƒ€ã‚¿ã‚¤ãƒ—", list(folder_options.keys()))
    folder_id = st.text_input("Google Drive ãƒ•ã‚©ãƒ«ãƒ€ ID", value=folder_options[sel_label])

    c1, c2 = st.columns(2)
    imp_start = c1.date_input("é–‹å§‹æ—¥", dt.date(2024, 1, 1), key="import_start_date")
    imp_end   = c2.date_input("çµ‚äº†æ—¥", dt.date.today(), key="import_end_date")

    preview_n = st.number_input("å…ˆé ­ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°ï¼ˆæ¤œè¨¼ç”¨ï¼‰", 0, 10, 3, help="èª­ã¿è¾¼ã‚“ã CSVã®å…ˆé ­æ•°è¡Œã‚’è¡¨ç¤ºã—ã¦ãƒãƒƒãƒ”ãƒ³ã‚°ç¢ºèª")

    colx, coly, colz = st.columns(3)
    dry_run   = colx.checkbox("ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆæ¤œè¨¼ã®ã¿ï¼‰", value=False)
    delta_only = coly.checkbox("å·®åˆ†ã®ã¿ï¼ˆæ—¢å­˜MAXæ—¥ä»˜ã‚ˆã‚Šæ–°ã—ã„CSVï¼‰", value=True)
    show_path = colz.checkbox("é€²æ—ã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¡¨ç¤º", value=True)

    if st.button("ğŸš€ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ", disabled=not folder_id):
        try:
            all_files = list_csv_recursive(folder_id)
            files = []
            for f in all_files:
                store, machine, date = parse_meta(f["path"])
                if imp_start <= date <= imp_end:
                    files.append({**f, "store": store, "machine": machine, "date": date})
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        st.write(f"ğŸ” å¯¾è±¡ CSV: **{len(files)} ä»¶**")
        bar = st.progress(0.0)
        current_file = st.empty()

        # å·®åˆ†ç”¨ã«æ—¢å­˜MAX(date)ã‚’å–å¾—
        latest_by_store = {}
        if delta_only and files:
            with eng.connect() as conn:
                for store in set([f["store"] for f in files]):
                    tname = "slot_" + store.replace(" ", "_")
                    try:
                        row = conn.execute(sa.text(f'SELECT MAX(date) FROM "{tname}"')).first()
                        latest_by_store[tname] = row[0] if row else None
                    except Exception:
                        latest_by_store[tname] = None

        created_tables = {}
        for i, f in enumerate(sorted(files, key=lambda x: (x["store"], x["machine"], x["date"])), 1):
            if show_path:
                current_file.text(f"å‡¦ç†ä¸­ãƒ•ã‚¡ã‚¤ãƒ«: {f['path']}")
            try:
                raw = drive.files().get_media(fileId=f["id"]).execute()
                store, machine, date = f["store"], f["machine"], f["date"]
                table_name = "slot_" + store.replace(" ", "_")

                # å·®åˆ†ã‚¹ã‚­ãƒƒãƒ—
                if delta_only and latest_by_store.get(table_name) and date <= latest_by_store[table_name]:
                    bar.progress(i / len(files))
                    continue

                # ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºä¿
                if table_name not in created_tables:
                    tbl = ensure_store_table(store)
                    created_tables[table_name] = tbl
                else:
                    tbl = created_tables[table_name]

                # èª­ã¿ãƒ»æ­£è¦åŒ–
                df = load_and_normalize(raw, store)
                if df.empty:
                    bar.progress(i / len(files)); continue

                # å…ˆé ­ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                if preview_n > 0 and i <= preview_n:
                    st.caption(f"ğŸ“„ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {f['path']}")
                    st.dataframe(df.head(min(5, len(df))))

                df["æ©Ÿç¨®"], df["date"] = machine, date

                # ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿
                valid_cols = [c.name for c in tbl.c]
                df = df[[c for c in df.columns if c in valid_cols]]

                if not dry_run:
                    with eng.begin() as conn:
                        upsert_dataframe(conn, tbl, df)

            except Exception as e:
                st.error(f"{f['path']} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

            bar.progress(i / len(files))

        current_file.text("")
        st.success("ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼")

# ========================= å¯è¦–åŒ–ãƒ¢ãƒ¼ãƒ‰ =========================
if mode == "ğŸ“Š å¯è¦–åŒ–":
    st.header("DB å¯è¦–åŒ–")

    # URLã‚¯ã‚¨ãƒªã‹ã‚‰åˆæœŸå€¤ã‚’å¾©å…ƒ
    qp = st.query_params
    qp_table   = qp.get("table", [""])[0] if isinstance(qp.get("table"), list) else qp.get("table", "")
    qp_machine = qp.get("machine", [""])[0] if isinstance(qp.get("machine"), list) else qp.get("machine", "")
    qp_start   = qp.get("start", [""])[0] if isinstance(qp.get("start"), list) else qp.get("start", "")
    qp_end     = qp.get("end", [""])[0] if isinstance(qp.get("end"), list) else qp.get("end", "")
    qp_avg     = qp.get("avg", ["false"])[0] if isinstance(qp.get("avg"), list) else qp.get("avg", "false")

    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
    try:
        with eng.connect() as conn:
            tables = [r[0] for r in conn.execute(sa.text(
                "SELECT tablename FROM pg_tables WHERE tablename LIKE 'slot_%'"
            ))]
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if not tables:
        st.info("ã¾ãšå–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
        st.stop()

    table_name = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ", tables, index=max(0, tables.index(qp_table)) if qp_table in tables else 0)
    if not table_name:
        st.error("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
        st.stop()

    tbl = sa.Table(table_name, sa.MetaData(), autoload_with=eng)

    # æœ€å°/æœ€å¤§æ—¥ä»˜
    with eng.connect() as conn:
        row = conn.execute(sa.text(f'SELECT MIN(date), MAX(date) FROM "{table_name}"')).first()
        min_date, max_date = (row or (None, None))

    if not (min_date and max_date):
        st.info("ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšå–ã‚Šè¾¼ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ãƒ—ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
    def apply_preset(preset: str):
        today = dt.date.today()
        if preset == "today":
            return today, today
        if preset == "this_week":
            start = today - dt.timedelta(days=today.weekday())
            return start, today
        if preset == "this_month":
            start = today.replace(day=1)
            return start, today
        return None

    c0, c1, c2, c3, c4, c5 = st.columns(6)
    if c0.button("ğŸ“… ä»Šæ—¥"):
        s,e = apply_preset("today"); st.session_state[f"visual_start_{table_name}"]=s; st.session_state[f"visual_end_{table_name}"]=e
    if c1.button("ğŸ“… ä»Šé€±"):
        s,e = apply_preset("this_week"); st.session_state[f"visual_start_{table_name}"]=s; st.session_state[f"visual_end_{table_name}"]=e
    if c2.button("ğŸ“… ä»Šæœˆ"):
        s,e = apply_preset("this_month"); st.session_state[f"visual_start_{table_name}"]=s; st.session_state[f"visual_end_{table_name}"]=e

    c1a, c2a = st.columns(2)
    vis_start = c1a.date_input(
        "é–‹å§‹æ—¥",
        value=dt.date.fromisoformat(qp_start) if qp_start else min_date,
        min_value=min_date, max_value=max_date,
        key=f"visual_start_{table_name}"
    )
    vis_end   = c2a.date_input(
        "çµ‚äº†æ—¥",
        value=dt.date.fromisoformat(qp_end) if qp_end else max_date,
        min_value=min_date, max_value=max_date,
        key=f"visual_end_{table_name}"
    )

    # ã‚«ãƒ©ãƒ ã‚­ãƒ¼å®‰å®šåŒ–
    needed_cols = tuple(c.name for c in tbl.c)

    @st.cache_data(show_spinner=False)
    def get_machines(table_name: str, start: dt.date, end: dt.date, _cols_key: tuple):
        t = sa.Table(table_name, sa.MetaData(), autoload_with=eng)
        q = sa.select(t.c.æ©Ÿç¨®).where(t.c.date.between(start, end)).distinct()
        with eng.connect() as conn:
            return [r[0] for r in conn.execute(q)]

    machines = get_machines(table_name, vis_start, vis_end, needed_cols)
    if not machines:
        st.warning("æŒ‡å®šæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()

    default_machine_index = max(0, machines.index(qp_machine)) if qp_machine in machines else 0
    machine_sel = st.selectbox(f"æ©Ÿç¨®é¸æŠï¼ˆ{len(machines)}ï¼‰", machines, index=default_machine_index)

    c_filter1, c_filter2, c_filter3 = st.columns(3)
    only_5       = c_filter1.checkbox("5ã®ã¤ãæ—¥ã ã‘")
    only_7       = c_filter2.checkbox("7ã®ã¤ãæ—¥ã ã‘")
    only_weekend = c_filter3.checkbox("åœŸæ—¥ã ã‘")

    show_avg = st.checkbox("å…¨å°å¹³å‡ã‚’è¡¨ç¤º", value=(qp_avg == "true"))

    @st.cache_data(show_spinner=False)
    def get_data(table_name: str, machine: str, start: dt.date, end: dt.date, _cols_key: tuple):
        t = sa.Table(table_name, sa.MetaData(), autoload_with=eng)
        q = sa.select(t).where(
            t.c.æ©Ÿç¨® == machine,
            t.c.date.between(start, end)
        ).order_by(t.c.date)
        df = pd.read_sql(q, eng)
        # å‹èª¿æ•´
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"]).dt.date
        if "å°ç•ªå·" in df.columns:
            # æ•´æ•°è¡¨ç¤ºï¼ˆæ¬ æã¯NAã®ã¾ã¾ï¼‰
            try:
                df["å°ç•ªå·"] = pd.to_numeric(df["å°ç•ªå·"], errors="coerce").astype("Int64")
            except Exception:
                pass
        return df

    df = get_data(table_name, machine_sel, vis_start, vis_end, needed_cols)
    if df.empty:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()

    # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ5/7/åœŸæ—¥ï¼‰
    srs_date = pd.to_datetime(df["date"])
    if only_5:
        df = df[(srs_date.dt.day % 10) == 5]
    if only_7:
        df = df[(srs_date.dt.day % 10) == 7]
    if only_weekend:
        df = df[srs_date.dt.weekday.isin([5, 6])]
    if df.empty:
        st.warning("ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã§ãƒ‡ãƒ¼ã‚¿ãŒãªããªã‚Šã¾ã—ãŸ")
        st.stop()

    # è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆ1/â—¯ or å®Ÿæ•°ï¼‰
    display_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰", ("1/â—¯ï¼ˆç›´æ„Ÿï¼‰", "å®Ÿæ•°ï¼ˆ0ã€œ1ï¼‰"), horizontal=True)

    # ã‚°ãƒ©ãƒ•ç”¨ãƒ‡ãƒ¼ã‚¿
    if show_avg:
        df_plot = (
            df.groupby("date", as_index=False)["åˆæˆç¢ºç‡"]
              .mean()
              .rename(columns={"åˆæˆç¢ºç‡": "plot_val"})
        )
        title = f"ğŸ“ˆ å…¨å°å¹³å‡ åˆæˆç¢ºç‡ | {machine_sel}"
        # å°ç•ªå·é¸æŠUIã¯éè¡¨ç¤º
        slot_sel = None
    else:
        @st.cache_data(show_spinner=False)
        def get_slots(table_name: str, machine: str, start: dt.date, end: dt.date, _cols_key: tuple):
            t = sa.Table(table_name, sa.MetaData(), autoload_with=eng)
            q = sa.select(t.c.å°ç•ªå·).where(
                t.c.æ©Ÿç¨® == machine, t.c.date.between(start, end)
            ).distinct().order_by(t.c.å°ç•ªå·)
            with eng.connect() as conn:
                vals = [r[0] for r in conn.execute(q) if r[0] is not None]
            return [int(v) for v in vals]

        slots = get_slots(table_name, machine_sel, vis_start, vis_end, needed_cols)
        if not slots:
            st.warning("å°ç•ªå·ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.stop()
        slot_sel = st.selectbox("å°ç•ªå·", slots)
        df_plot = df[df["å°ç•ªå·"] == slot_sel].rename(columns={"åˆæˆç¢ºç‡": "plot_val"})
        title = f"ğŸ“ˆ åˆæˆç¢ºç‡ | {machine_sel} | å° {slot_sel}"

    # ç§»å‹•å¹³å‡
    win = st.slider("ç§»å‹•å¹³å‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆæ—¥ï¼‰", 1, 14, 1, help="1ãªã‚‰ç¾çŠ¶ç¶­æŒã€5ã‚„7ã§æ»‘ã‚‰ã‹ã«")
    df_plot_ma = df_plot.copy()
    if win > 1:
        df_plot_ma["ma"] = df_plot_ma["plot_val"].rolling(win, min_periods=1).mean()
    else:
        df_plot_ma["ma"] = df_plot_ma["plot_val"]

    # è¨­å®šãƒ©ã‚¤ãƒ³
    thresholds = setting_map.get(machine_sel, {})
    df_rules = pd.DataFrame([{"setting": k, "value": v} for k, v in thresholds.items()]) if thresholds else pd.DataFrame(columns=["setting","value"])

    # Yè»¸
    if display_mode == "1/â—¯ï¼ˆç›´æ„Ÿï¼‰":
        y_axis = alt.Axis(
            title="åˆæˆç¢ºç‡",
            format=".4f",
            labelExpr="isValid(datum.value) ? (datum.value==0 ? '0' : '1/'+format(round(1/datum.value),'d')) : ''"
        )
    else:
        y_axis = alt.Axis(title="åˆæˆç¢ºç‡ï¼ˆå®Ÿæ•°ï¼‰", format=".4f")

    base = alt.Chart(df_plot_ma).mark_line().encode(
        x="date:T",
        y=alt.Y("plot_val:Q", axis=y_axis),
        tooltip=[
            alt.Tooltip("date:T", title="æ—¥ä»˜"),
            alt.Tooltip("plot_val:Q", title="å€¤", format=".4f"),
            alt.Tooltip("ma:Q", title="ç§»å‹•å¹³å‡", format=".4f")
        ],
    ).properties(height=400)

    ma_line = alt.Chart(df_plot_ma).mark_line(strokeDash=[4,2]).encode(
        x="date:T", y="ma:Q", color=alt.value("gray")
    )

    chart = base + ma_line

    if not df_rules.empty:
        legend_sel = alt.selection_multi(fields=["setting"], bind="legend")
        rules = alt.Chart(df_rules).mark_rule(strokeDash=[4, 2]).encode(
            y="value:Q",
            color=alt.Color("setting:N", legend=alt.Legend(title="è¨­å®šãƒ©ã‚¤ãƒ³")),
            opacity=alt.condition(legend_sel, alt.value(1), alt.value(0.2))
        ).add_selection(legend_sel)
        chart = chart + rules

    st.subheader(title)
    st.altair_chart(chart, use_container_width=True)

    # è¡¨ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    st.caption("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ï¼ˆæœŸé–“ãƒ»æ©Ÿç¨®ãƒ»å°ã®ç¾åœ¨ãƒ“ãƒ¥ãƒ¼ï¼‰")
    df_show = df.sort_values(["date","å°ç•ªå·"], na_position="last")
    st.dataframe(df_show)
    csv = df_show.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "â¬‡ï¸ è¡¨ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVï¼‰",
        data=csv,
        file_name=f"{table_name}_{machine_sel}_{vis_start}_{vis_end}{'' if slot_sel is None else f'_slot{slot_sel}'}.csv",
        mime="text/csv"
    )

    # PNGä¿å­˜ï¼ˆç’°å¢ƒã«ã‚ˆã‚Šä¸å¯ã®å ´åˆã‚ã‚Šï¼‰
    try:
        from altair_saver import save
        png_path = f"{table_name}_{machine_sel}_{vis_start}_{vis_end}{'' if slot_sel is None else f'_slot{slot_sel}'}.png"
        save(chart, png_path)
        with open(png_path, "rb") as f:
            st.download_button("ğŸ–¼ ã‚°ãƒ©ãƒ•PNGã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=f, file_name=png_path, mime="image/png")
    except Exception:
        st.info("PNGä¿å­˜ã¯ã“ã®ç’°å¢ƒã§ã¯ç„¡åŠ¹ã®ãŸã‚ã€CSVã®ã¿æä¾›ä¸­ã§ã™ã€‚")

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆæ©Ÿç¨®å…¨ä½“ï¼‰
    st.subheader("ğŸ—º å°ç•ªå·Ã—æ—¥ä»˜ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆåˆæˆç¢ºç‡ï¼‰")
    df_heat = df.copy()
    if "å°ç•ªå·" in df_heat.columns:
        df_heat = df_heat.dropna(subset=["å°ç•ªå·", "åˆæˆç¢ºç‡"])
        try:
            df_heat["å°ç•ªå·"] = pd.to_numeric(df_heat["å°ç•ªå·"], errors="coerce").astype("Int64")
        except Exception:
            pass
        df_heat["inv"] = df_heat["åˆæˆç¢ºç‡"].replace(0, pd.NA)
        df_heat["inv"] = df_heat["inv"].apply(lambda x: None if pd.isna(x) else 1.0/x)
        heat = alt.Chart(df_heat).mark_rect().encode(
            x=alt.X("date:T", title="æ—¥ä»˜"),
            y=alt.Y("å°ç•ªå·:O", sort="ascending"),
            color=alt.Color("inv:Q", title="1/åˆæˆç¢ºç‡ï¼ˆå¤§ãã„=ç†±ã„ï¼‰"),
            tooltip=[
                alt.Tooltip("date:T", title="æ—¥ä»˜"),
                alt.Tooltip("å°ç•ªå·:O", title="å°ç•ªå·"),
                alt.Tooltip("åˆæˆç¢ºç‡:Q", format=".4f"),
                alt.Tooltip("inv:Q", title="1/åˆæˆ", format=".0f"),
            ]
        ).properties(height=300)
        st.altair_chart(heat, use_container_width=True)
    else:
        st.info("å°ç•ªå·åˆ—ãŒç„¡ã„ãŸã‚ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯è¡¨ç¤ºã—ã¾ã›ã‚“ã€‚")

    # ãŠæ°—ã«å…¥ã‚Šï¼ˆç°¡æ˜“ï¼‰
    st.divider()
    st.caption("â­ ã‚ˆãä½¿ã†çµ„åˆã›ã‚’ä¿å­˜")
    if "favorites" not in st.session_state:
        st.session_state["favorites"] = []
    fav_name = st.text_input("ãŠæ°—ã«å…¥ã‚Šåï¼ˆä¾‹: æ­¦è”µå¢ƒ_ãƒã‚¤ã‚¸ãƒ£ã‚°V_å°237ï¼‰")
    if st.button("â­ è¿½åŠ ", disabled=not fav_name):
        st.session_state["favorites"].append({
            "name": fav_name,
            "table": table_name,
            "machine": machine_sel,
            "start": vis_start.isoformat(),
            "end": vis_end.isoformat(),
            "avg": str(show_avg).lower()
        })
        st.success("ãŠæ°—ã«å…¥ã‚Šã«è¿½åŠ ã—ã¾ã—ãŸ")

    if st.session_state["favorites"]:
        colf1, colf2 = st.columns([3,1])
        sel_fav = colf1.selectbox("ãŠæ°—ã«å…¥ã‚Šã‚’å‘¼ã³å‡ºã—", [f['name'] for f in st.session_state["favorites"]])
        if colf2.button("å‘¼ã³å‡ºã—"):
            fav = next(f for f in st.session_state["favorites"] if f["name"]==sel_fav)
            st.query_params.update({
                "table": fav["table"],
                "machine": fav["machine"],
                "start": fav["start"],
                "end": fav["end"],
                "avg": fav["avg"]
            })
            st.experimental_rerun()

    # URLã‚¯ã‚¨ãƒªã¸ç¾åœ¨çŠ¶æ…‹ã‚’åæ˜ ï¼ˆå…±æœ‰ç”¨ï¼‰
    st.query_params.update({
        "table": table_name,
        "machine": machine_sel,
        "start": vis_start.isoformat(),
        "end": vis_end.isoformat(),
        "avg": str(show_avg).lower()
    })
