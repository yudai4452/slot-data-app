import io
import re
import datetime as dt
import pandas as pd
import streamlit as st
import sqlalchemy as sa
from sqlalchemy import inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import altair as alt
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from uuid import uuid4

# ======================== åŸºæœ¬è¨­å®š ========================
st.set_page_config(page_title="Slot Manager", layout="wide")
mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿", "ğŸ“Š å¯è¦–åŒ–"), key="mode_radio")
st.title("ğŸ° Slot Data Manager & Visualizer")

SA_INFO = st.secrets["gcp_service_account"]
PG_CFG  = st.secrets["connections"]["slot_db"]

# ======================== è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« ========================
with open("setting.json", encoding="utf-8") as f:
    setting_map = json.load(f)

# ======================== æ¥ç¶š ========================
def make_drive():
    try:
        creds = Credentials.from_service_account_info(
            SA_INFO, scopes=["https://www.googleapis.com/auth/drive.readonly"]
        )
        return build("drive", "v3", credentials=creds)
    except Exception as e:
        st.error(f"Driveèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_resource
def gdrive():
    return make_drive()

drive = gdrive()

@st.cache_resource
def engine():
    try:
        url = (
            f"postgresql+psycopg2://{PG_CFG.username}:{PG_CFG.password}"
            f"@{PG_CFG.host}:{PG_CFG.port}/{PG_CFG.database}?sslmode=require"
        )
        return sa.create_engine(url, pool_pre_ping=True)
    except Exception as e:
        st.error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

eng = engine()
if eng is None:
    st.stop()

# ======================== ã‚«ãƒ©ãƒ å®šç¾©ãƒãƒƒãƒ”ãƒ³ã‚° ========================
COLUMN_MAP = {
    "ãƒ¡ãƒƒã‚»æ­¦è”µå¢ƒ": {
        "å°ç•ªå·":"å°ç•ªå·","ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":"ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°","ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":"ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°":"BBå›æ•°","RBå›æ•°":"RBå›æ•°","ARTå›æ•°":"ARTå›æ•°",
        "æœ€å¤§æŒã¡ç‰":"æœ€å¤§æŒç‰","æœ€å¤§æŒç‰":"æœ€å¤§æŒç‰",
        "BBç¢ºç‡":"BBç¢ºç‡","RBç¢ºç‡":"RBç¢ºç‡","ARTç¢ºç‡":"ARTç¢ºç‡","åˆæˆç¢ºç‡":"åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ":"å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
    },
    "ã‚¸ãƒ£ãƒ³ã‚¸ãƒ£ãƒ³ãƒãƒ¼ãƒ«ã‚´ãƒƒãƒˆåˆ†å€æ²³åŸ": {
        "å°ç•ªå·":"å°ç•ªå·","ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":"ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ","BBå›æ•°":"BBå›æ•°","RBå›æ•°":"RBå›æ•°",
        "æœ€å¤§æŒã¡ç‰":"æœ€å¤§æŒç‰","æœ€å¤§æŒç‰":"æœ€å¤§æŒç‰",
        "BBç¢ºç‡":"BBç¢ºç‡","RBç¢ºç‡":"RBç¢ºç‡","åˆæˆç¢ºç‡":"åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ":"å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ","ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":"ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
    "ãƒ—ãƒ¬ã‚´ç«‹å·": {
        "å°ç•ªå·":"å°ç•ªå·","ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":"ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ","BBå›æ•°":"BBå›æ•°","RBå›æ•°":"RBå›æ•°",
        "æœ€å¤§å·®ç‰":"æœ€å¤§å·®ç‰","BBç¢ºç‡":"BBç¢ºç‡","RBç¢ºç‡":"RBç¢ºç‡","åˆæˆç¢ºç‡":"åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ":"å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ","ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":"ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
}

# ======================== Drive: å†å¸° + ãƒšãƒ¼ã‚¸ãƒ³ã‚° ========================
@st.cache_data
def list_csv_recursive(folder_id: str):
    if drive is None:
        raise RuntimeError("Driveæœªæ¥ç¶šã§ã™")
    all_files, queue = [], [(folder_id, "")]
    while queue:
        fid, cur = queue.pop()
        page_token = None
        while True:
            res = drive.files().list(
                q=f"'{fid}' in parents and trashed=false",
                fields="nextPageToken, files(id,name,mimeType,md5Checksum,modifiedTime,size)",
                pageSize=1000, pageToken=page_token
            ).execute()
            for f in res.get("files", []):
                if f["mimeType"] == "application/vnd.google-apps.folder":
                    queue.append((f["id"], f"{cur}/{f['name']}"))
                elif f["name"].lower().endswith(".csv"):
                    all_files.append({**f, "path": f"{cur}/{f['name']}"})
            page_token = res.get("nextPageToken")
            if not page_token:
                break
    return all_files

# ======================== ãƒ¡ã‚¿æƒ…å ±è§£æ ========================
DATE_RE = re.compile(r"\d{4}-\d{2}-\d{2}")

def parse_meta(path: str):
    parts = path.strip("/").split("/")
    if len(parts) < 3:
        raise ValueError(f"ãƒ‘ã‚¹ãŒçŸ­ã™ãã¾ã™: {path}")
    store, machine = parts[-3], parts[-2]
    m = DATE_RE.search(parts[-1])
    if not m:
        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜(YYYY-MM-DD)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {parts[-1]}")
    date = dt.date.fromisoformat(m.group(0))
    return store, machine, date

# ======================== æ­£è¦åŒ– ========================
def normalize(df_raw: pd.DataFrame, store: str) -> pd.DataFrame:
    df = df_raw.rename(columns=COLUMN_MAP[store])

    prob_cols = ["BBç¢ºç‡", "RBç¢ºç‡", "ARTç¢ºç‡", "åˆæˆç¢ºç‡"]
    for col in prob_cols:
        if col not in df.columns:
            continue
        ser = df[col].astype(str)
        mask_div = ser.str.contains("/", na=False)

        if mask_div.any():
            denom = pd.to_numeric(
                ser[mask_div].str.split("/", expand=True)[1],
                errors="coerce"
            )
            val = 1.0 / denom
            val[(denom <= 0) | (~denom.notna())] = 0
            df.loc[mask_div, col] = val

        num = pd.to_numeric(ser[~mask_div], errors="coerce")
        conv = num.copy()
        conv[num > 1] = 1.0 / num[num > 1]
        conv = conv.fillna(0)
        df.loc[~mask_div, col] = conv

        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0.0).astype(float)

    int_cols = [
        "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°",
        "RBå›æ•°", "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
    ]
    for col in int_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")

    return df

# ======================== èª­ã¿è¾¼ã¿ + æ­£è¦åŒ– ========================
def load_and_normalize(raw_bytes: bytes, store: str) -> pd.DataFrame:
    header = pd.read_csv(io.BytesIO(raw_bytes), encoding="shift_jis", nrows=0).columns.tolist()
    mapping_keys = list(dict.fromkeys(COLUMN_MAP[store].keys()))
    usecols = [col for col in mapping_keys if col in header]
    df_raw = pd.read_csv(
        io.BytesIO(raw_bytes),
        encoding="shift_jis",
        usecols=usecols,
        on_bad_lines="skip",
        engine="python",
    )
    return normalize(df_raw, store)

# ======================== import_logï¼ˆå·®åˆ†å–ã‚Šè¾¼ã¿ï¼‰ ========================
def ensure_import_log_table():
    meta = sa.MetaData()
    insp = inspect(eng)
    if not insp.has_table("import_log"):
        t = sa.Table(
            "import_log", meta,
            sa.Column("file_id", sa.Text, primary_key=True),
            sa.Column("md5", sa.Text, nullable=False),
            sa.Column("path", sa.Text, nullable=False),
            sa.Column("store", sa.Text, nullable=False),
            sa.Column("machine", sa.Text, nullable=False),
            sa.Column("date", sa.Date, nullable=False),
            sa.Column("rows", sa.Integer, nullable=False),
            sa.Column("imported_at", sa.DateTime, nullable=False, server_default=sa.func.now()),
        )
        meta.create_all(eng)
    else:
        t = sa.Table("import_log", meta, autoload_with=eng)
    return t

def get_imported_md5_map():
    log = ensure_import_log_table()
    with eng.connect() as conn:
        rows = conn.execute(sa.select(log.c.file_id, log.c.md5)).fetchall()
    return {r[0]: r[1] for r in rows}

def upsert_import_log(entries: list[dict]):
    if not entries:
        return
    log = ensure_import_log_table()
    stmt = pg_insert(log).values(entries)
    stmt = stmt.on_conflict_do_update(
        index_elements=[log.c.file_id],
        set_={"md5": stmt.excluded.md5,
              "path": stmt.excluded.path,
              "store": stmt.excluded.store,
              "machine": stmt.excluded.machine,
              "date": stmt.excluded.date,
              "rows": stmt.excluded.rows,
              "imported_at": sa.func.now()}
    )
    with eng.begin() as conn:
        conn.execute(stmt)

# ======================== ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ ========================
def ensure_store_table(store: str):
    safe = "slot_" + store.replace(" ", "_")
    insp = inspect(eng)
    meta = sa.MetaData()
    if not insp.has_table(safe):
        cols = [
            sa.Column("date", sa.Date, nullable=False),
            sa.Column("æ©Ÿç¨®", sa.Text, nullable=False),
            sa.Column("å°ç•ªå·", sa.Integer, nullable=False),
        ]
        unique_cols = list(dict.fromkeys(COLUMN_MAP[store].values()))
        numeric_int = {
            "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°", "RBå›æ•°",
            "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
        }
        for col_name in unique_cols:
            if col_name in {"date", "æ©Ÿç¨®", "å°ç•ªå·"}:
                continue
            if col_name in numeric_int:
                cols.append(sa.Column(col_name, sa.Integer))
            else:
                cols.append(sa.Column(col_name, sa.Float))
        t = sa.Table(safe, meta, *cols, sa.PrimaryKeyConstraint("date", "æ©Ÿç¨®", "å°ç•ªå·"))
        meta.create_all(eng)
        return t
    return sa.Table(safe, meta, autoload_with=eng)

# ======================== é€šå¸¸UPSERT ========================
def upsert_dataframe(conn, table, df: pd.DataFrame, pk=("date", "æ©Ÿç¨®", "å°ç•ªå·")):
    rows = df.to_dict(orient="records")
    if not rows:
        return
    stmt = pg_insert(table).values(rows)
    update_cols = {c.name: stmt.excluded[c.name] for c in table.c if c.name not in pk}
    stmt = stmt.on_conflict_do_update(index_elements=list(pk), set_=update_cols)
    conn.execute(stmt)

# ======================== COPYâ†’MERGE é«˜é€Ÿã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ ========================
def q(name: str) -> str:
    return '"' + name.replace('"', '""') + '"'

def bulk_upsert_copy_merge(table: sa.Table, df: pd.DataFrame, pk=("date", "æ©Ÿç¨®", "å°ç•ªå·")):
    if df.empty:
        return

    valid_cols = [c.name for c in table.c]
    cols = [c for c in df.columns if c in valid_cols]

    for p in pk:
        if p not in cols:
            raise ValueError(f"COPYåˆ—ã«ä¸»ã‚­ãƒ¼ {p} ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

    df_use = df[cols].copy()

    csv_buf = io.StringIO()
    df_use.to_csv(csv_buf, index=False, na_rep="")
    csv_text = csv_buf.getvalue()

    tmp_name = f"tmp_{table.name}_{uuid4().hex[:8]}"
    cols_q = ", ".join(q(c) for c in cols)
    pk_q   = ", ".join(q(p) for p in pk)
    upd_cols = [c for c in cols if c not in pk]
    set_clause = ", ".join(f"{q(c)}=EXCLUDED.{q(c)}" for c in upd_cols) if upd_cols else ""

    create_tmp_sql = f'CREATE TEMP TABLE {q(tmp_name)} (LIKE {q(table.name)} INCLUDING ALL);'
    copy_sql = f'COPY {q(tmp_name)} ({cols_q}) FROM STDIN WITH (FORMAT csv, HEADER true);'
    insert_sql = f'INSERT INTO {q(table.name)} ({cols_q}) SELECT {cols_q} FROM {q(tmp_name)} ' \
                 f'ON CONFLICT ({pk_q}) DO ' + ('NOTHING;' if not set_clause else f'UPDATE SET {set_clause};')
    drop_tmp_sql = f'DROP TABLE IF EXISTS {q(tmp_name)};'

    with eng.begin() as conn:
        driver_conn = getattr(conn.connection, "driver_connection", None)
        if driver_conn is None:
            driver_conn = conn.connection.connection  # fallback psycopg2 connection

        with driver_conn.cursor() as cur:
            cur.execute(create_tmp_sql)
            cur.copy_expert(copy_sql, io.StringIO(csv_text))
            cur.execute(insert_sql)
            cur.execute(drop_tmp_sql)

# ======================== ä¸¦åˆ—å‡¦ç†: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & æ­£è¦åŒ– ========================
def process_one_file(file_meta: dict) -> dict | None:
    try:
        store, machine, date = parse_meta(file_meta["path"])
        if store not in COLUMN_MAP:
            return None

        drv = make_drive()
        raw = drv.files().get_media(fileId=file_meta["id"]).execute()
        df = load_and_normalize(raw, store)
        if df.empty:
            return None

        df["æ©Ÿç¨®"] = machine
        df["date"] = date
        table_name = "slot_" + store.replace(" ", "_")
        return {
            "table_name": table_name,
            "df": df,
            "store": store,
            "machine": machine,
            "date": date,
            "file_id": file_meta["id"],
            "md5": file_meta.get("md5Checksum") or "",
            "path": file_meta["path"],
        }
    except Exception as e:
        return {"error": f"{file_meta.get('path','(unknown)')} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"}

# ======================== è‡ªå‹•ãƒãƒƒãƒå®Ÿè¡Œãƒ˜ãƒ«ãƒ‘ãƒ¼ ========================
def run_import_for_targets(targets: list[dict], workers: int, use_copy: bool):
    status = st.empty()
    created_tables: dict[str, sa.Table] = {}
    import_log_entries = []
    errors = []
    bucket: dict[str, list[dict]] = defaultdict(list)

    with ThreadPoolExecutor(max_workers=workers) as ex:
        futures = {ex.submit(process_one_file, f): f for f in targets}
        for fut in as_completed(futures):
            res = fut.result()
            if res is None:
                continue
            if "error" in res:
                errors.append(res["error"])
                continue
            bucket[res["table_name"]].append(res)
            status.text(f"å‡¦ç†å®Œäº†: {res['path']}")

    for table_name, items in bucket.items():
        if table_name not in created_tables:
            tbl = ensure_store_table(items[0]["store"])
            created_tables[table_name] = tbl
        else:
            tbl = created_tables[table_name]

        valid_cols = [c.name for c in tbl.c]

        if use_copy:
            try:
                dfs = []
                for res in items:
                    df = res["df"]
                    for c in valid_cols:
                        if c not in df.columns:
                            df[c] = pd.NA
                    dfs.append(df[[c for c in df.columns if c in valid_cols]])
                df_all = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame(columns=valid_cols)
                bulk_upsert_copy_merge(tbl, df_all)
            except Exception as e:
                errors.append(f"{table_name} COPYé«˜é€ŸåŒ–å¤±æ•—ã®ãŸã‚é€šå¸¸UPSERTã§å†è©¦è¡Œ: {e}")
                with eng.begin() as conn:
                    for res in items:
                        df_one = res["df"][[c for c in res["df"].columns if c in valid_cols]]
                        try:
                            upsert_dataframe(conn, tbl, df_one)
                        except Exception as ie:
                            errors.append(f"{res['path']} é€šå¸¸UPSERTã§ã‚‚å¤±æ•—: {ie}")
        else:
            with eng.begin() as conn:
                for res in items:
                    df_one = res["df"][[c for c in res["df"].columns if c in valid_cols]]
                    upsert_dataframe(conn, tbl, df_one)

        for res in items:
            import_log_entries.append({
                "file_id": res["file_id"],
                "md5": res["md5"],
                "path": res["path"],
                "store": res["store"],
                "machine": res["machine"],
                "date": res["date"],
                "rows": int(len(res["df"])),
            })

    processed_files = sum(len(v) for v in bucket.values())
    return import_log_entries, errors, processed_files

# ========================= ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ =========================
if mode == "ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿":
    st.header("Google Drive â†’ Postgres ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
    folder_options = {
        "ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨": "1MRQFPBahlSwdwhrqqBzudXL18y8-qOb8",
        "ğŸš€ æœ¬ç•ªç”¨":   "1hX8GQRuDm_E1A1Cu_fXorvwxv-XF7Ynl",
    }

    options = list(folder_options.keys())
    default_idx = options.index("ğŸš€ æœ¬ç•ªç”¨") if "ğŸš€ æœ¬ç•ªç”¨" in options else 0
    sel_label = st.selectbox("ãƒ•ã‚©ãƒ«ãƒ€ã‚¿ã‚¤ãƒ—", options, index=default_idx, key="folder_type")
    folder_id = st.text_input("Google Drive ãƒ•ã‚©ãƒ«ãƒ€ ID", value=folder_options[sel_label], key="folder_id")

    c1, c2 = st.columns(2)
    imp_start = c1.date_input("é–‹å§‹æ—¥", dt.date(2024, 1, 1), key="import_start_date")
    imp_end   = c2.date_input("çµ‚äº†æ—¥", dt.date.today(), key="import_end_date")

    c3, c4 = st.columns(2)
    max_files = c3.slider("æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆ1å›ã®å®Ÿè¡Œä¸Šé™ï¼‰", 10, 2000, 300, step=10,
                          help="å¤§é‡ãƒ•ã‚©ãƒ«ãƒ€ã¯åˆ†å‰²ã—ã¦å–ã‚Šè¾¼ã¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰", key="max_files")
    workers = c4.slider("ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°", 1, 8, 4,
                        help="ä¸¦åˆ—æ•°ãŒå¤šã™ãã‚‹ã¨APIåˆ¶é™ã«å½“ãŸã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™", key="workers")

    use_copy = st.checkbox("DBæ›¸ãè¾¼ã¿ã‚’COPYã§é«˜é€ŸåŒ–ï¼ˆæ¨å¥¨ï¼‰", value=True,
                           help="ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«COPYâ†’ã¾ã¨ã‚ã¦UPSERTã€‚å¤±æ•—æ™‚ã¯è‡ªå‹•ã§é€šå¸¸UPSERTã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚", key="use_copy")
    auto_batch = st.checkbox("æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã”ã¨ã«è‡ªå‹•ã§ç¶šãã®ãƒãƒƒãƒã‚‚å®Ÿè¡Œã™ã‚‹", value=False, key="auto_batch")
    max_batches = st.number_input("æœ€å¤§ãƒãƒƒãƒå›æ•°", min_value=1, max_value=100, value=3,
                                  help="å®Ÿè¡Œæ™‚é–“ãŒé•·ããªã‚Šã™ãã‚‹ã®ã‚’é˜²ããŸã‚ã®ä¸Šé™", key="max_batches")

    if st.button("ğŸš€ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ", disabled=not folder_id, key="import_run"):
        try:
            files_all = list_csv_recursive(folder_id)
            files = [f for f in files_all if imp_start <= parse_meta(f['path'])[2] <= imp_end]
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        imported_md5 = get_imported_md5_map()
        all_targets = [f for f in files if imported_md5.get(f["id"], "") != (f.get("md5Checksum") or "")]
        if not all_targets:
            st.success("å·®åˆ†ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆã™ã¹ã¦æœ€æ–°ï¼‰")
            st.stop()

        all_targets.sort(key=lambda f: parse_meta(f["path"])[2])

        batches = [all_targets[i:i+max_files] for i in range(0, len(all_targets), max_files)]
        if not auto_batch:
            batches = batches[:1]

        total_files = sum(len(b) for b in batches[:int(max_batches)])
        done_files = 0
        bar = st.progress(0.0)
        status = st.empty()
        all_errors = []

        for bi, batch in enumerate(batches[:int(max_batches)], start=1):
            status.text(f"ãƒãƒƒãƒ {bi}/{len(batches)}ï¼ˆ{len(batch)} ä»¶ï¼‰ã‚’å‡¦ç†ä¸­â€¦")
            entries, errors, processed_files = run_import_for_targets(batch, workers, use_copy)
            upsert_import_log(entries)
            all_errors.extend(errors)

            done_files += processed_files
            bar.progress(min(1.0, done_files / max(1, total_files)))

        status.text("")
        if len(batches) > max_batches and auto_batch:
            remaining = sum(len(b) for b in batches[int(max_batches):])
            st.info(f"æœ€å¤§ãƒãƒƒãƒå›æ•°ã«é”ã—ã¾ã—ãŸã€‚æ®‹ã‚Š {remaining} ä»¶ã¯ã€å†åº¦ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ç¶šãã‹ã‚‰å‡¦ç†ã—ã¾ã™ã€‚")

        if all_errors:
            st.warning("ä¸€éƒ¨ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ï¼š")
            for msg in all_errors[:50]:
                st.write("- " + msg)
            if len(all_errors) > 50:
                st.write(f"... ã»ã‹ {len(all_errors)-50} ä»¶")

        st.success(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼ˆå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«: {done_files} ä»¶ï¼‰ï¼")

# ========================= å¯è¦–åŒ–ãƒ¢ãƒ¼ãƒ‰ =========================
if mode == "ğŸ“Š å¯è¦–åŒ–":
    st.header("DB å¯è¦–åŒ–")

    # 1) ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
    try:
        with eng.connect() as conn:
            tables = [r[0] for r in conn.execute(sa.text(
                "SELECT tablename FROM pg_tables WHERE tablename LIKE 'slot_%'"
            ))]
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if not tables:
        st.info("ã¾ãšå–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
        st.stop()

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ slot_ãƒ—ãƒ¬ã‚´ç«‹å· ã«
    default_table = "slot_ãƒ—ãƒ¬ã‚´ç«‹å·"
    default_index = next((i for i, t in enumerate(tables) if t == default_table), 0)

    table_name = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ", tables, index=default_index, key="table_select")
    if not table_name:
        st.error("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
        st.stop()

    TBL_Q = '"' + table_name.replace('"', '""') + '"'

    # 2) æœ€å°/æœ€å¤§æ—¥ä»˜ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    @st.cache_data(ttl=600)
    def get_date_range(table_name: str):
        TBL_Q = '"' + table_name.replace('"', '""') + '"'
        with eng.connect() as conn:
            row = conn.execute(sa.text(f"SELECT MIN(date), MAX(date) FROM {TBL_Q}")).first()
        return (row[0], row[1]) if row else (None, None)

    min_date, max_date = get_date_range(table_name)
    if not (min_date and max_date):
        st.info("ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšå–ã‚Šè¾¼ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    c1, c2 = st.columns(2)
    vis_start = c1.date_input("é–‹å§‹æ—¥", value=min_date, min_value=min_date, max_value=max_date, key=f"visual_start_{table_name}")
    vis_end   = c2.date_input("çµ‚äº†æ—¥", value=max_date, min_value=min_date, max_value=max_date, key=f"visual_end_{table_name}")

    # 3) ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆä»»æ„ï¼‰
    idx_ok = st.checkbox("èª­ã¿è¾¼ã¿é«˜é€ŸåŒ–ã®ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆï¼ˆæ¨å¥¨ãƒ»ä¸€åº¦ã ã‘ï¼‰", value=True, key="create_index")
    if idx_ok:
        try:
            with eng.begin() as conn:
                conn.execute(sa.text(f'CREATE INDEX IF NOT EXISTS {table_name}_ix_machine_date ON {TBL_Q} ("æ©Ÿç¨®","date");'))
                conn.execute(sa.text(f'CREATE INDEX IF NOT EXISTS {table_name}_ix_machine_slot_date ON {TBL_Q} ("æ©Ÿç¨®","å°ç•ªå·","date");'))
        except Exception as e:
            st.info(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

    # 4) æ©Ÿç¨®ä¸€è¦§ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    @st.cache_data(ttl=600)
    def get_machines_fast(table_name: str, start: dt.date, end: dt.date):
        TBL_Q = '"' + table_name.replace('"', '""') + '"'
        sql = sa.text(f'SELECT DISTINCT "æ©Ÿç¨®" FROM {TBL_Q} WHERE date BETWEEN :s AND :e ORDER BY "æ©Ÿç¨®"')
        with eng.connect() as conn:
            return [r[0] for r in conn.execute(sql, {"s": start, "e": end})]

    machines = get_machines_fast(table_name, vis_start, vis_end)
    if not machines:
        st.warning("æŒ‡å®šæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        st.stop()

    machine_sel = st.selectbox("æ©Ÿç¨®é¸æŠ", machines, key="machine_select")
    show_avg = st.checkbox("å…¨å°å¹³å‡ã‚’è¡¨ç¤º", value=True, key="show_avg")

    # 5) å°ç•ªå·ä¸€è¦§ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    @st.cache_data(ttl=600)
    def get_slots_fast(table_name: str, machine: str, start: dt.date, end: dt.date):
        TBL_Q = '"' + table_name.replace('"', '""') + '"'
        sql = sa.text(f'SELECT DISTINCT "å°ç•ªå·" FROM {TBL_Q} WHERE "æ©Ÿç¨®"=:m AND date BETWEEN :s AND :e AND "å°ç•ªå·" IS NOT NULL ORDER BY "å°ç•ªå·"')
        with eng.connect() as conn:
            vals = [r[0] for r in conn.execute(sql, {"m": machine, "s": start, "e": end})]
        return [int(v) for v in vals if v is not None]

    # 6) ãƒ—ãƒ­ãƒƒãƒˆç”¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ & å¿…è¦åˆ—ã ã‘ï¼‰
    @st.cache_data(ttl=300)
    def fetch_plot_avg(table_name: str, machine: str, start: dt.date, end: dt.date) -> pd.DataFrame:
        TBL_Q = '"' + table_name.replace('"', '""') + '"'
        sql = sa.text(f'''
            SELECT date, AVG("åˆæˆç¢ºç‡") AS plot_val
            FROM {TBL_Q}
            WHERE "æ©Ÿç¨®" = :m AND date BETWEEN :s AND :e
            GROUP BY date
            ORDER BY date
        ''')
        with eng.connect() as conn:
            df = pd.read_sql(sql, conn, params={"m": machine, "s": start, "e": end})
        return df  # date ã¯ SQL ã‹ã‚‰ datetime64[ns] ã§æ¥ã‚‹
    @st.cache_data(ttl=300)
    def fetch_plot_slot(table_name: str, machine: str, slot: int, start: dt.date, end: dt.date) -> pd.DataFrame:
        TBL_Q = '"' + table_name.replace('"', '""') + '"'
        sql = sa.text(f'''
            SELECT date, "åˆæˆç¢ºç‡" AS plot_val
            FROM {TBL_Q}
            WHERE "æ©Ÿç¨®" = :m AND "å°ç•ªå·" = :n AND date BETWEEN :s AND :e
            ORDER BY date
        ''')
        with eng.connect() as conn:
            df = pd.read_sql(sql, conn, params={"m": machine, "n": int(slot), "s": start, "e": end})
        return df

    if show_avg:
        df_plot = fetch_plot_avg(table_name, machine_sel, vis_start, vis_end)
        title = f"ğŸ“ˆ å…¨å°å¹³å‡ åˆæˆç¢ºç‡ | {machine_sel}"
    else:
        slots = get_slots_fast(table_name, machine_sel, vis_start, vis_end)
        if not slots:
            st.warning("å°ç•ªå·ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.stop()
        slot_sel = st.selectbox("å°ç•ªå·", slots, key="slot_select")
        df_plot = fetch_plot_slot(table_name, machine_sel, slot_sel, vis_start, vis_end)
        title = f"ğŸ“ˆ åˆæˆç¢ºç‡ | {machine_sel} | å° {slot_sel}"

    if df_plot is None or df_plot.empty:
        st.info("ã“ã®æ¡ä»¶ã§ã¯è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æœŸé–“ã‚„æ©Ÿç¨®ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ===== Xè»¸ã‚’å®Ÿãƒ‡ãƒ¼ã‚¿ç¯„å›²ã«å›ºå®šï¼ˆç©ºç™½é™¤å»ï¼‰ =====
    df_plot["date"] = pd.to_datetime(df_plot["date"])
    xdomain_start = df_plot["date"].min()
    xdomain_end   = df_plot["date"].max()
    if pd.isna(xdomain_start) or pd.isna(xdomain_end):
        st.info("è¡¨ç¤ºå¯¾è±¡ã®æœŸé–“ã«æ—¥ä»˜ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()
    if xdomain_start == xdomain_end:
        xdomain_end = xdomain_end + pd.Timedelta(days=1)

    # 7) è¨­å®šãƒ©ã‚¤ãƒ³
    thresholds = setting_map.get(machine_sel, {})
    df_rules = pd.DataFrame([{"setting": k, "value": v} for k, v in thresholds.items()]) \
               if thresholds else pd.DataFrame(columns=["setting","value"])

    legend_sel = alt.selection_point(fields=["setting"], bind="legend")

    # Yè»¸ï¼ˆ1/xè¡¨è¨˜ï¼‰
    y_axis = alt.Axis(
        title="åˆæˆç¢ºç‡",
        format=".4f",
        labelExpr="isValid(datum.value) ? (datum.value==0 ? '0' : '1/'+format(round(1/datum.value),'d')) : ''"
    )

    # ===== ãƒ™ãƒ¼ã‚¹ãƒãƒ£ãƒ¼ãƒˆï¼šæ—¥ä»˜ãƒ©ãƒ™ãƒ«ã¯æœˆåˆã®ã¿ M/Dã€ä»–ã¯ Dã€‚è‡ªå‹•é–“å¼•ãã€‚=====
    x_axis_days = alt.Axis(
        title="æ—¥ä»˜",
        labelExpr="date(datum.value)==1 ? timeFormat(datum.value,'%-m/%-d') : timeFormat(datum.value,'%-d')",
        labelAngle=0,
        labelPadding=6,
        labelOverlap=True,
        labelBound=True,
    )
    x_scale = alt.Scale(domain=[xdomain_start, xdomain_end])
    x_field = alt.X("date:T", axis=x_axis_days, scale=x_scale)

    base = alt.Chart(df_plot).mark_line().encode(
        x=x_field,
        y=alt.Y("plot_val:Q", axis=y_axis),
        tooltip=[
            alt.Tooltip("date:T", title="æ—¥ä»˜", format="%Y-%m-%d"),
            alt.Tooltip("plot_val:Q", title="å€¤", format=".4f")
        ],
    ).properties(height=400, width='container')

    if not df_rules.empty:
        rules = alt.Chart(df_rules).mark_rule(strokeDash=[4, 2]).encode(
            y="value:Q",
            color=alt.Color("setting:N", legend=alt.Legend(title="è¨­å®šãƒ©ã‚¤ãƒ³")),
            opacity=alt.condition(legend_sel, alt.value(1), alt.value(0.15)),
        )
        main_chart = (base + rules).add_params(legend_sel).properties(width='container')
    else:
        main_chart = base.properties(width='container')

    # ===== ã‚¹ãƒˆãƒªãƒƒãƒ—ï¼šæœˆã¨å¹´ã‚’å„1å›ã ã‘ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ç¯„å›²ã«åˆã‚ã›ã‚‹ï¼‰=====
    def month_starts(start: dt.date, end: dt.date) -> pd.DataFrame:
        s = start.replace(day=1)
        rng = pd.date_range(s, end, freq="MS")
        return pd.DataFrame({"date": rng, "label": [f"{d.month}æœˆ" for d in rng]})

    def year_starts(start: dt.date, end: dt.date) -> pd.DataFrame:
        y0 = start.replace(month=1, day=1)
        rng = pd.date_range(y0, end, freq="YS")
        return pd.DataFrame({"date": rng, "label": [f"{d.year}å¹´" for d in rng]})

    df_month = month_starts(xdomain_start.date(), xdomain_end.date())
    df_year  = year_starts(xdomain_start.date(), xdomain_end.date())

    month_text = alt.Chart(df_month).mark_text(baseline="top").encode(
        x=alt.X("date:T", axis=None),
        y=alt.value(22),
        text="label:N"
    ).properties(width='container')

    year_text = alt.Chart(df_year).mark_text(baseline="top").encode(
        x=alt.X("date:T", axis=None),
        y=alt.value(6),
        text="label:N"
    ).properties(width='container')

    strip = (year_text + month_text).properties(height=28, width='container')

    # ===== é€£çµï¼ˆXå…±æœ‰ï¼‰ã€‚ä½™ç™½ã‚’è©°ã‚ã‚‹ =====
    final = alt.vconcat(main_chart, strip).resolve_scale(x="shared").properties(
        padding={"left": 8, "right": 8, "top": 8, "bottom": 8},
        bounds="flush",
    )

    st.subheader(title)
    st.altair_chart(final, use_container_width=True)
