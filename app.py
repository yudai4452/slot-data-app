import io
import re
import time
import random
import datetime as dt
import pandas as pd
import streamlit as st
import sqlalchemy as sa
from sqlalchemy import inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import altair as alt
import json

# ======================== åŸºæœ¬è¨­å®š ========================
st.set_page_config(page_title="Slot Manager", layout="wide")
st.title("ğŸ° Slot Data Manager & Visualizer")
mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿", "ğŸ“Š å¯è¦–åŒ–"))

SA_INFO = st.secrets["gcp_service_account"]
PG_CFG  = st.secrets["connections"]["slot_db"]

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
with st.sidebar:
    if st.button("â™»ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"):
        st.cache_data.clear()
        st.success("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
        st.rerun()

# ======================== è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« ========================
with open("setting.json", encoding="utf-8") as f:
    setting_map = json.load(f)

# ======================== æ¥ç¶š ========================
@st.cache_resource
def gdrive():
    try:
        creds = Credentials.from_service_account_info(
            SA_INFO, scopes=["https://www.googleapis.com/auth/drive.readonly"]
        )
        return build("drive", "v3", credentials=creds)
    except Exception as e:
        st.error(f"Driveèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

drive = gdrive()

@st.cache_resource
def engine():
    try:
        url = (
            f"postgresql+psycopg2://{PG_CFG.username}:{PG_CFG.password}"
            f"@{PG_CFG.host}:{PG_CFG.port}/{PG_CFG.database}?sslmode=require"
        )
        return sa.create_engine(url, pool_pre_ping=True)
    except Exception as e:
        st.error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

eng = engine()
if eng is None:
    st.stop()

# ======================== ã‚«ãƒ©ãƒ å®šç¾©ãƒãƒƒãƒ”ãƒ³ã‚° ========================
# ã€Œæœ€å¤§æŒã¡ç‰ã€ã¨ã€Œæœ€å¤§æŒç‰ã€ã®è¡¨è¨˜ã‚†ã‚Œä¸¡å¯¾å¿œ
COLUMN_MAP = {
    "ãƒ¡ãƒƒã‚»æ­¦è”µå¢ƒ": {
        "å°ç•ªå·":           "å°ç•ªå·",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":     "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":     "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°":          "BBå›æ•°",
        "RBå›æ•°":          "RBå›æ•°",
        "ARTå›æ•°":         "ARTå›æ•°",
        "æœ€å¤§æŒã¡ç‰":       "æœ€å¤§æŒç‰",
        "æœ€å¤§æŒç‰":         "æœ€å¤§æŒç‰",
        "BBç¢ºç‡":          "BBç¢ºç‡",
        "RBç¢ºç‡":          "RBç¢ºç‡",
        "ARTç¢ºç‡":         "ARTç¢ºç‡",
        "åˆæˆç¢ºç‡":        "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
    },
    "ã‚¸ãƒ£ãƒ³ã‚¸ãƒ£ãƒ³ãƒãƒ¼ãƒ«ã‚´ãƒƒãƒˆåˆ†å€æ²³åŸ": {
        "å°ç•ªå·":           "å°ç•ªå·",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":     "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°":          "BBå›æ•°",
        "RBå›æ•°":          "RBå›æ•°",
        "æœ€å¤§æŒã¡ç‰":       "æœ€å¤§æŒç‰",
        "æœ€å¤§æŒç‰":         "æœ€å¤§æŒç‰",
        "BBç¢ºç‡":          "BBç¢ºç‡",
        "RBç¢ºç‡":          "RBç¢ºç‡",
        "åˆæˆç¢ºç‡":        "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":     "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
    "ãƒ—ãƒ¬ã‚´ç«‹å·": {
        "å°ç•ªå·":           "å°ç•ªå·",
        "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ":     "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°":          "BBå›æ•°",
        "RBå›æ•°":          "RBå›æ•°",
        "æœ€å¤§å·®ç‰":         "æœ€å¤§å·®ç‰",
        "BBç¢ºç‡":          "BBç¢ºç‡",
        "RBç¢ºç‡":          "RBç¢ºç‡",
        "åˆæˆç¢ºç‡":        "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
        "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°":     "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
}

# ======================== Google API ãƒªãƒˆãƒ©ã‚¤ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ========================
def gapi_call(req_builder, *a, **kw):
    """Google API å‘¼ã³å‡ºã—ã«æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã‚’é©ç”¨"""
    for i in range(5):
        try:
            return req_builder(*a, **kw).execute()
        except Exception as e:
            if i == 4:
                raise
            time.sleep((2 ** i) + random.random())

# ======================== Drive: å†å¸° + ãƒšãƒ¼ã‚¸ãƒ³ã‚° ========================
@st.cache_data(ttl=300)
def list_csv_recursive(folder_id: str):
    if drive is None:
        raise RuntimeError("Driveæœªæ¥ç¶šã§ã™")
    all_files, queue = [], [(folder_id, "")]
    while queue:
        fid, cur = queue.pop()
        page_token = None
        while True:
            res = gapi_call(
                drive.files().list,
                q=f"'{fid}' in parents and trashed=false",
                fields="nextPageToken, files(id,name,mimeType)",
                pageSize=1000, pageToken=page_token
            )
            for f in res.get("files", []):
                if f["mimeType"] == "application/vnd.google-apps.folder":
                    queue.append((f["id"], f"{cur}/{f['name']}"))
                elif f["name"].lower().endswith(".csv"):
                    all_files.append({**f, "path": f"{cur}/{f['name']}"})
            page_token = res.get("nextPageToken")
            if not page_token:
                break
    return all_files

# ======================== ãƒ¡ã‚¿æƒ…å ±è§£æï¼ˆæ­£è¦è¡¨ç¾ã§æ—¥ä»˜æŠ½å‡ºï¼‰ ========================
DATE_RE = re.compile(r"\d{4}-\d{2}-\d{2}")

def parse_meta(path: str):
    parts = path.strip("/").split("/")
    if len(parts) < 3:
        raise ValueError(f"ãƒ‘ã‚¹ãŒçŸ­ã™ãã¾ã™: {path}")
    store, machine = parts[-3], parts[-2]
    m = DATE_RE.search(parts[-1])
    if not m:
        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜(YYYY-MM-DD)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {parts[-1]}")
    date = dt.date.fromisoformat(m.group(0))
    return store, machine, date

# ======================== æ­£è¦åŒ– ========================
def normalize(df_raw: pd.DataFrame, store: str) -> pd.DataFrame:
    df = df_raw.rename(columns=COLUMN_MAP[store])

    prob_cols = ["BBç¢ºç‡", "RBç¢ºç‡", "ARTç¢ºç‡", "åˆæˆç¢ºç‡"]
    for col in prob_cols:
        if col not in df.columns:
            continue
        ser = df[col].astype(str)
        mask_div = ser.str.contains("/", na=False)

        # "1/x" å½¢å¼
        if mask_div.any():
            denom = pd.to_numeric(
                ser[mask_div].str.split("/", expand=True)[1],
                errors="coerce"
            )
            val = 1.0 / denom
            val[(denom <= 0) | (~denom.notna())] = 0  # 0/è² /æ¬ æã¯0ã«
            df.loc[mask_div, col] = val

        # æ•°å€¤ç›´æ›¸ãï¼ˆ>1 ã¯ 1/å€¤, <=1 ã¯ãã®ã¾ã¾ï¼‰
        num = pd.to_numeric(ser[~mask_div], errors="coerce")
        conv = num.copy()
        conv[num > 1] = 1.0 / num[num > 1]
        df.loc[~mask_div, col] = conv.fillna(0)

        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0.0).astype(float)
        # å€¤åŸŸã‚¯ãƒ©ãƒ³ãƒ—ï¼ˆã‚ãšã‹ãªå¤–ã‚Œã‚‚0ã€œ1ã«åã‚ã‚‹ï¼‰
        df[col] = df[col].clip(lower=0, upper=1)

    # æ•´æ•°ã‚«ãƒ©ãƒ 
    int_cols = [
        "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°",
        "RBå›æ•°", "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
    ]
    for col in int_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")

    return df

# ======================== èª­ã¿è¾¼ã¿ + æ­£è¦åŒ– ========================
@st.cache_data(ttl=300)
def load_and_normalize(raw_bytes: bytes, store: str) -> pd.DataFrame:
    header = pd.read_csv(io.BytesIO(raw_bytes), encoding="shift_jis", nrows=0).columns.tolist()
    mapping_keys = list(dict.fromkeys(COLUMN_MAP[store].keys()))
    usecols = [col for col in mapping_keys if col in header]
    df_raw = pd.read_csv(
        io.BytesIO(raw_bytes),
        encoding="shift_jis",
        usecols=usecols,
        on_bad_lines="skip",
        engine="c",
    )
    return normalize(df_raw, store)

# ======================== ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆå°ç•ªå·ã®è¿½åŠ  & å‹ä¿®æ­£ï¼‰ ========================
def ensure_store_table(store: str):
    safe = "slot_" + store.replace(" ", "_")
    insp = inspect(eng)
    meta = sa.MetaData()
    if not insp.has_table(safe):
        cols = [
            sa.Column("date", sa.Date, nullable=False),
            sa.Column("æ©Ÿç¨®", sa.Text, nullable=False),
            sa.Column("å°ç•ªå·", sa.Integer, nullable=False),
        ]
        unique_cols = list(dict.fromkeys(COLUMN_MAP[store].values()))
        numeric_int = {
            "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°", "RBå›æ•°",
            "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
        }
        for col_name in unique_cols:
            if col_name in {"date", "æ©Ÿç¨®", "å°ç•ªå·"}:
                continue
            if col_name in numeric_int:
                cols.append(sa.Column(col_name, sa.Integer))
            else:
                cols.append(sa.Column(col_name, sa.Float))
        t = sa.Table(safe, meta, *cols, sa.PrimaryKeyConstraint("date", "æ©Ÿç¨®", "å°ç•ªå·"))
        meta.create_all(eng)
        return t
    return sa.Table(safe, meta, autoload_with=eng)

def ensure_indexes(table_name: str, conn):
    conn.execute(sa.text(
        f"CREATE INDEX IF NOT EXISTS ix_{table_name}_date_machine ON {table_name}(date, æ©Ÿç¨®)"
    ))
    conn.execute(sa.text(
        f"CREATE INDEX IF NOT EXISTS ix_{table_name}_machine_slot_date ON {table_name}(æ©Ÿç¨®, å°ç•ªå·, date)"
    ))

# ======================== ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆï¼ˆæ–°è¦/æ›´æ–°ä»¶æ•°ã‚’è¿”ã™ï¼‰ ========================
def upsert_dataframe(conn, table, df: pd.DataFrame, pk=("date", "æ©Ÿç¨®", "å°ç•ªå·")):
    rows = df.to_dict(orient="records")
    if not rows:
        return 0, 0
    stmt = pg_insert(table).values(rows)
    update_cols = {c.name: stmt.excluded[c.name] for c in table.c if c.name not in pk}
    # Postgresã®xmaxã§æ–°è¦(0)/æ›´æ–°(!=0) ã‚’åˆ¤å®š
    stmt = stmt.on_conflict_do_update(
        index_elements=list(pk),
        set_=update_cols
    ).returning(sa.literal_column("xmax"))
    res = conn.execute(stmt).fetchall()
    created = sum(1 for r in res if getattr(r, "xmax", 0) == 0)
    updated = len(res) - created
    return created, updated

# ========================= ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ =========================
if mode == "ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿":
    st.header("Google Drive â†’ Postgres ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
    folder_options = {
        "ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨": "1MRQFPBahlSwdwhrqqBzudXL18y8-qOb8",
        "ğŸš€ æœ¬ç•ªç”¨":   "1hX8GQRuDm_E1A1Cu_fXorvwxv-XF7Ynl",
    }
    sel_label = st.selectbox("ãƒ•ã‚©ãƒ«ãƒ€ã‚¿ã‚¤ãƒ—", list(folder_options.keys()))
    folder_id = st.text_input("Google Drive ãƒ•ã‚©ãƒ«ãƒ€ ID", value=folder_options[sel_label])

    c1, c2 = st.columns(2)
    imp_start = c1.date_input("é–‹å§‹æ—¥", dt.date(2024, 1, 1), key="import_start_date")
    imp_end   = c2.date_input("çµ‚äº†æ—¥", dt.date.today(), key="import_end_date")

    if st.button("ğŸš€ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ", disabled=not folder_id):
        try:
            files = [
                f for f in list_csv_recursive(folder_id)
                if imp_start <= parse_meta(f["path"])[2] <= imp_end
            ]
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        st.write(f"ğŸ” å¯¾è±¡ CSV: **{len(files)} ä»¶**")
        bar = st.progress(0.0)
        current_file = st.empty()
        created_tables = {}
        total_new = total_upd = 0
        errors = []

        for i, f in enumerate(files, 1):
            current_file.text(f"å‡¦ç†ä¸­ãƒ•ã‚¡ã‚¤ãƒ«: {f['path']}")
            try:
                raw = gapi_call(drive.files().get_media, fileId=f["id"])
                store, machine, date = parse_meta(f["path"])
                table_name = "slot_" + store.replace(" ", "_")

                if table_name not in created_tables:
                    tbl = ensure_store_table(store)
                    created_tables[table_name] = tbl
                else:
                    tbl = created_tables[table_name]

                df = load_and_normalize(raw, store)
                if df.empty:
                    bar.progress(i / len(files))
                    continue

                df["æ©Ÿç¨®"], df["date"] = machine, date
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿
                valid_cols = [c.name for c in tbl.c]
                df = df[[c for c in df.columns if c in valid_cols]]

                with eng.begin() as conn:
                    n_new, n_upd = upsert_dataframe(conn, tbl, df)
                    ensure_indexes(tbl.name, conn)
                    total_new += n_new
                    total_upd += n_upd

            except Exception as e:
                errors.append({"file": f["path"], "error": str(e)})

            bar.progress(i / len(files))

        current_file.text("")

        if errors:
            st.error(f"âš ï¸ {len(errors)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å¤±æ•—ã—ã¾ã—ãŸã€‚ä¸‹ã®è¡¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.dataframe(pd.DataFrame(errors))

        st.success(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼ æ–°è¦: {total_new:,} / æ›´æ–°: {total_upd:,}")

# ========================= å¯è¦–åŒ–ãƒ¢ãƒ¼ãƒ‰ =========================
if mode == "ğŸ“Š å¯è¦–åŒ–":
    st.header("DB å¯è¦–åŒ–")

    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
    try:
        with eng.connect() as conn:
            tables = [r[0] for r in conn.execute(sa.text(
                "SELECT tablename FROM pg_tables WHERE tablename LIKE 'slot_%'"
            ))]
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if not tables:
        st.info("ã¾ãšå–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
        st.stop()

    table_name = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ", tables)
    if not table_name:
        st.error("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
        st.stop()

    tbl = sa.Table(table_name, sa.MetaData(), autoload_with=eng)

    # æœ€å°/æœ€å¤§æ—¥ä»˜
    with eng.connect() as conn:
        row = conn.execute(sa.text(f"SELECT MIN(date), MAX(date) FROM {table_name}")).first()
        min_date, max_date = (row or (None, None))

    if not (min_date and max_date):
        st.info("ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšå–ã‚Šè¾¼ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ç›´è¿‘90æ—¥ã‚’åˆæœŸå€¤ï¼ˆç¯„å›²å¤–ãªã‚‰ min/max ã«ä¸¸ã‚ï¼‰
    default_start = max(min_date, max_date - dt.timedelta(days=89))
    default_end = max_date

    c1, c2 = st.columns(2)
    vis_start = c1.date_input(
        "é–‹å§‹æ—¥", value=default_start, min_value=min
