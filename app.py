import io
import re
import unicodedata
import datetime as dt
import pandas as pd
import streamlit as st
import sqlalchemy as sa
from sqlalchemy import inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import altair as alt
import json

# ======================== åŸºæœ¬è¨­å®š ========================
st.set_page_config(page_title="Slot Manager", layout="wide")
mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿", "ğŸ“Š å¯è¦–åŒ–"))
st.title("ğŸ° Slot Data Manager & Visualizer")

SA_INFO = st.secrets["gcp_service_account"]
PG_CFG  = st.secrets["connections"]["slot_db"]

# ======================== è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« ========================
with open("setting.json", encoding="utf-8") as f:
    setting_map = json.load(f)

# ======================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆã‚­ãƒ¼/è¡¨è¨˜ã‚†ã‚Œæ­£è¦åŒ–ï¼‰ ========================
def norm_key(s: str) -> str:
    s = unicodedata.normalize("NFKC", s or "")
    s = s.replace("ãƒ¼", "-").replace("ã€€", " ").strip()
    return s

# åº—èˆ—åã®æ­£è¦åŒ–ç‰ˆãƒãƒƒãƒ—ï¼ˆã‚­ãƒ¼ã‚’æ­£è¦åŒ–ï¼‰
COLUMN_MAP_RAW = {
    "ãƒ¡ãƒƒã‚»æ­¦è”µå¢ƒ": {
        "å°ç•ªå·": "å°ç•ªå·", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°": "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ",
        "BBå›æ•°": "BBå›æ•°", "RBå›æ•°": "RBå›æ•°", "ARTå›æ•°": "ARTå›æ•°",
        "æœ€å¤§æŒã¡ç‰": "æœ€å¤§æŒç‰", "æœ€å¤§æŒç‰": "æœ€å¤§æŒç‰",
        "BBç¢ºç‡": "BBç¢ºç‡", "RBç¢ºç‡": "RBç¢ºç‡", "ARTç¢ºç‡": "ARTç¢ºç‡",
        "åˆæˆç¢ºç‡": "åˆæˆç¢ºç‡", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ",
    },
    "ã‚¸ãƒ£ãƒ³ã‚¸ãƒ£ãƒ³ãƒãƒ¼ãƒ«ã‚´ãƒƒãƒˆåˆ†å€æ²³åŸ": {
        "å°ç•ªå·": "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "BBå›æ•°": "BBå›æ•°", "RBå›æ•°": "RBå›æ•°",
        "æœ€å¤§æŒã¡ç‰": "æœ€å¤§æŒç‰", "æœ€å¤§æŒç‰": "æœ€å¤§æŒç‰",
        "BBç¢ºç‡": "BBç¢ºç‡", "RBç¢ºç‡": "RBç¢ºç‡", "åˆæˆç¢ºç‡": "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°": "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
    "ãƒ—ãƒ¬ã‚´ç«‹å·": {
        "å°ç•ªå·": "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ": "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "BBå›æ•°": "BBå›æ•°", "RBå›æ•°": "RBå›æ•°",
        "æœ€å¤§å·®ç‰": "æœ€å¤§å·®ç‰",
        "BBç¢ºç‡": "BBç¢ºç‡", "RBç¢ºç‡": "RBç¢ºç‡", "åˆæˆç¢ºç‡": "åˆæˆç¢ºç‡",
        "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ": "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°": "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°",
    },
}
COLUMN_MAP = {norm_key(k): v for k, v in COLUMN_MAP_RAW.items()}

# ä¸€éƒ¨ã‚«ãƒ©ãƒ ã®è¡¨è¨˜ã‚†ã‚Œåˆ¥åï¼ˆã‚ã‚Œã°æ¡ç”¨ï¼‰
FALLBACK_ALIASES = {
    "æœ€å¤§æŒç‰": ["æœ€å¤§æŒã¡ç‰"],
    "æœ€å¤§å·®ç‰": ["æœ€å¤§å·®æš"],
}

# ======================== æ¥ç¶š ========================
@st.cache_resource
def gdrive():
    try:
        creds = Credentials.from_service_account_info(
            SA_INFO, scopes=["https://www.googleapis.com/auth/drive.readonly"]
        )
        return build("drive", "v3", credentials=creds)
    except Exception as e:
        st.error(f"Driveèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

drive = gdrive()

@st.cache_resource
def engine():
    try:
        url = (
            f"postgresql+psycopg2://{PG_CFG.username}:{PG_CFG.password}"
            f"@{PG_CFG.host}:{PG_CFG.port}/{PG_CFG.database}?sslmode=require"
        )
        return sa.create_engine(url, pool_pre_ping=True, echo=False)
    except Exception as e:
        st.error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

eng = engine()
if eng is None:
    st.stop()

# ======================== Drive: å†å¸° + ãƒšãƒ¼ã‚¸ãƒ³ã‚° ========================
@st.cache_data
def list_csv_recursive(folder_id: str):
    if drive is None:
        raise RuntimeError("Driveæœªæ¥ç¶šã§ã™")
    all_files, queue = [], [(folder_id, "")]
    while queue:
        fid, cur = queue.pop()
        page_token = None
        while True:
            res = drive.files().list(
                q=f"'{fid}' in parents and trashed=false",
                fields="nextPageToken, files(id,name,mimeType)",
                pageSize=1000, pageToken=page_token
            ).execute()
            for f in res.get("files", []):
                if f["mimeType"] == "application/vnd.google-apps.folder":
                    queue.append((f["id"], f"{cur}/{f['name']}"))
                elif f["name"].lower().endswith(".csv"):
                    all_files.append({**f, "path": f"{cur}/{f['name']}"})
            page_token = res.get("nextPageToken")
            if not page_token:
                break
    # å‡¦ç†é †ã®å®‰å®šåŒ–
    all_files.sort(key=lambda x: x["path"])
    return all_files

# ======================== ãƒ¡ã‚¿æƒ…å ±è§£æï¼ˆæ­£è¦è¡¨ç¾ã§æ—¥ä»˜æŠ½å‡ºï¼‰ ========================
DATE_RE = re.compile(r"\d{4}-\d{2}-\d{2}")

def parse_meta(path: str):
    parts = path.strip("/").split("/")
    if len(parts) < 3:
        raise ValueError(f"ãƒ‘ã‚¹ãŒçŸ­ã™ãã¾ã™: {path}")
    store, machine = parts[-3], parts[-2]
    m = DATE_RE.search(parts[-1])
    if not m:
        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜(YYYY-MM-DD)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {parts[-1]}")
    date = dt.date.fromisoformat(m.group(0))
    return store, machine, date

# ======================== æ­£è¦åŒ– ========================
def _build_usecols(header: list[str], store_norm: str) -> list[str]:
    mapping = COLUMN_MAP[store_norm].copy()
    # ã‚¨ã‚¤ãƒªã‚¢ã‚¹é©ç”¨ï¼šãƒ˜ãƒƒãƒ€ã«å­˜åœ¨ã™ã‚‹åˆ¥åã‚’æ­£è¦åã¸å¸å
    for canon, aliases in FALLBACK_ALIASES.items():
        for a in aliases:
            if a in header and canon not in header and a in mapping:
                mapping[canon] = mapping[a]
    # mapping ã‚­ãƒ¼ã®ã†ã¡ãƒ˜ãƒƒãƒ€ã«ã‚ã‚‹ã‚‚ã®ã ã‘æ¡ç”¨
    keys = list(dict.fromkeys(k for k in mapping.keys() if k in header))
    return keys

def normalize(df_raw: pd.DataFrame, store_norm: str) -> pd.DataFrame:
    df = df_raw.rename(columns=COLUMN_MAP[store_norm])

    # ç¢ºç‡åˆ—ã‚’å®Ÿæ•°(0ã€œ1)ã¸æƒãˆã‚‹
    prob_cols = ["BBç¢ºç‡", "RBç¢ºç‡", "ARTç¢ºç‡", "åˆæˆç¢ºç‡"]
    for col in prob_cols:
        if col not in df.columns:
            continue
        ser = df[col].astype(str)
        mask_div = ser.str.contains("/", na=False)

        # "1/x" å½¢å¼
        if mask_div.any():
            denom = pd.to_numeric(ser[mask_div].str.split("/", expand=True)[1], errors="coerce")
            val = 1.0 / denom
            val[(denom <= 0) | (~denom.notna())] = 0
            df.loc[mask_div, col] = val

        # æ•°å€¤ç›´æ›¸ãï¼ˆ>1 ã¯ 1/å€¤, <=1 ã¯ãã®ã¾ã¾ï¼‰
        num = pd.to_numeric(ser[~mask_div], errors="coerce")
        conv = num.copy()
        conv[num > 1] = 1.0 / num[num > 1]
        conv = conv.fillna(0)
        df.loc[~mask_div, col] = conv

        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0.0).astype(float)

    # æ•´æ•°ã‚«ãƒ©ãƒ 
    int_cols = [
        "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°",
        "RBå›æ•°", "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
    ]
    for col in int_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")

    return df

# ======================== èª­ã¿è¾¼ã¿ + æ­£è¦åŒ– ========================
@st.cache_data
def load_and_normalize(raw_bytes: bytes, store_raw: str) -> pd.DataFrame:
    store_norm = norm_key(store_raw)
    if store_norm not in COLUMN_MAP:
        raise ValueError(f"æœªå¯¾å¿œã®åº—èˆ—åã§ã™: {store_raw}")
    # ãƒ˜ãƒƒãƒ€ç¢ºèª
    header = pd.read_csv(io.BytesIO(raw_bytes), encoding="shift_jis", nrows=0).columns.tolist()
    usecols = _build_usecols(header, store_norm)
    df_raw = pd.read_csv(
        io.BytesIO(raw_bytes),
        encoding="shift_jis",
        usecols=usecols,
        on_bad_lines="skip",
        engine="c",
    )
    return normalize(df_raw, store_norm)

# ======================== ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ ========================
def ensure_store_table(store_raw: str):
    safe = "slot_" + norm_key(store_raw).replace(" ", "_")
    insp = inspect(eng)
    meta = sa.MetaData()
    if not insp.has_table(safe):
        cols = [
            sa.Column("date", sa.Date, nullable=False),
            sa.Column("æ©Ÿç¨®", sa.Text, nullable=False),
            sa.Column("å°ç•ªå·", sa.Integer, nullable=False),
        ]
        unique_cols = list(dict.fromkeys(COLUMN_MAP[norm_key(store_raw)].values()))
        numeric_int = {
            "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "ã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°", "BBå›æ•°", "RBå›æ•°",
            "ARTå›æ•°", "æœ€å¤§æŒç‰", "æœ€å¤§å·®ç‰", "å‰æ—¥æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
        }
        for col_name in unique_cols:
            if col_name in {"date", "æ©Ÿç¨®", "å°ç•ªå·"}:
                continue
            if col_name in numeric_int:
                cols.append(sa.Column(col_name, sa.Integer))
            else:
                cols.append(sa.Column(col_name, sa.Float))
        t = sa.Table(safe, meta, *cols, sa.PrimaryKeyConstraint("date", "æ©Ÿç¨®", "å°ç•ªå·"))
        meta.create_all(eng)
        # æ¨å¥¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        with eng.begin() as conn:
            conn.execute(sa.text(f"CREATE INDEX IF NOT EXISTS idx_{safe}_kisyudate ON {safe}(æ©Ÿç¨®, date)"))
            conn.execute(sa.text(f"CREATE INDEX IF NOT EXISTS idx_{safe}_date ON {safe}(date)"))
        return t
    return sa.Table(safe, meta, autoload_with=eng)

# ======================== ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ ========================
def upsert_dataframe(conn, table, df: pd.DataFrame, pk=("date", "æ©Ÿç¨®", "å°ç•ªå·")):
    rows = df.to_dict(orient="records")
    if not rows:
        return
    stmt = pg_insert(table).values(rows)
    update_cols = {c.name: stmt.excluded[c.name] for c in table.c if c.name not in pk}
    stmt = stmt.on_conflict_do_update(index_elements=list(pk), set_=update_cols)
    conn.execute(stmt)

# ========================= ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ =========================
if mode == "ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿":
    st.header("Google Drive â†’ Postgres ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
    folder_options = {
        "ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨": "1MRQFPBahlSwdwhrqqBzudXL18y8-qOb8",
        "ğŸš€ æœ¬ç•ªç”¨":   "1hX8GQRuDm_E1A1Cu_fXorvwxv-XF7Ynl",
    }
    sel_label = st.selectbox("ãƒ•ã‚©ãƒ«ãƒ€ã‚¿ã‚¤ãƒ—", list(folder_options.keys()))
    with st.expander("é«˜åº¦ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆé€šå¸¸ã¯ä¸è¦ï¼‰", expanded=False):
        folder_id = st.text_input("Google Drive ãƒ•ã‚©ãƒ«ãƒ€ ID ã‚’æ‰‹å…¥åŠ›", value=folder_options[sel_label])
        dry_run = st.checkbox("ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆDBã«ã¯æ›¸ãè¾¼ã¾ãªã„ï¼‰", value=False)
        exclude_kw = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã¦ã„ãŸã‚‰é™¤å¤–ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰", value="ã‚µãƒ³ãƒ—ãƒ«,ãƒ†ã‚¹ãƒˆ").strip()
    if not folder_id:
        folder_id = folder_options[sel_label]

    c1, c2 = st.columns(2)
    imp_start = c1.date_input("é–‹å§‹æ—¥", dt.date(2024, 1, 1), key="import_start_date")
    imp_end   = c2.date_input("çµ‚äº†æ—¥", dt.date.today(), key="import_end_date")

    if st.button("ğŸš€ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ", disabled=not folder_id):
        try:
            files = [
                f for f in list_csv_recursive(folder_id)
                if imp_start <= parse_meta(f['path'])[2] <= imp_end
            ]
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        st.write(f"ğŸ” å¯¾è±¡ CSV: **{len(files)} ä»¶**")
        bar = st.progress(0.0)
        current_file = st.empty()
        created_tables = {}
        exclude_list = [x.strip() for x in exclude_kw.split(",") if x.strip()]

        for i, f in enumerate(files, 1):
            # é™¤å¤–ãƒ«ãƒ¼ãƒ«
            if any(x in f["name"] for x in exclude_list):
                bar.progress(i / len(files)); continue

            current_file.text(f"å‡¦ç†ä¸­ãƒ•ã‚¡ã‚¤ãƒ«: {f['path']}")
            try:
                raw = drive.files().get_media(fileId=f["id"]).execute()
                store, machine, date = parse_meta(f["path"])
                table_name = "slot_" + norm_key(store).replace(" ", "_")
                if table_name not in created_tables:
                    tbl = ensure_store_table(store)
                    created_tables[table_name] = tbl
                else:
                    tbl = created_tables[table_name]

                df = load_and_normalize(raw, store)
                if df.empty:
                    bar.progress(i / len(files)); continue

                df["æ©Ÿç¨®"], df["date"] = machine, date
                valid_cols = [c.name for c in tbl.c]
                df = df[[c for c in df.columns if c in valid_cols]]

                if not dry_run:
                    with eng.begin() as conn:
                        upsert_dataframe(conn, tbl, df)

            except Exception as e:
                st.error(f"{f['path']} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

            bar.progress(i / len(files))

        current_file.text("")
        st.success("ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼" + ("ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼‰" if dry_run else ""))

# ========================= å¯è¦–åŒ–ãƒ¢ãƒ¼ãƒ‰ =========================
if mode == "ğŸ“Š å¯è¦–åŒ–":
    st.header("DB å¯è¦–åŒ–")

    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
    try:
        with eng.connect() as conn:
            tables = [r[0] for r in conn.execute(sa.text(
                "SELECT tablename FROM pg_tables WHERE tablename LIKE 'slot_%'"
            ))]
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if not tables:
        st.info("ã¾ãšå–ã‚Šè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
        st.stop()

    # URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰åˆæœŸåŒ–
    qparams = st.query_params
    default_table = qparams.get("table") if "table" in qparams else None
    if default_table in tables:
        table_name = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ", tables, index=tables.index(default_table))
    else:
        table_name = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ", tables)

    tbl = sa.Table(table_name, sa.MetaData(), autoload_with=eng)

    # æœ€å°/æœ€å¤§æ—¥ä»˜
    with eng.connect() as conn:
        row = conn.execute(sa.text(f"SELECT MIN(date), MAX(date) FROM {table_name}")).first()
        min_date, max_date = (row or (None, None))

    if not (min_date and max_date):
        st.info("ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšå–ã‚Šè¾¼ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # æœŸé–“ãƒ—ãƒªã‚»ãƒƒãƒˆ
    preset = st.radio(
        "æœŸé–“ãƒ—ãƒªã‚»ãƒƒãƒˆ",
        ["éå»7æ—¥", "éå»30æ—¥", "å…¨æœŸé–“", "ã‚«ã‚¹ã‚¿ãƒ "],
        horizontal=True,
        index=1,
        help="ã‚ˆãä½¿ã†æœŸé–“ã‚’ãƒ¯ãƒ³ã‚¿ãƒƒãƒ—ã§åˆ‡æ›¿"
    )

    if preset == "éå»7æ—¥":
        vis_start, vis_end = max_date - dt.timedelta(days=6), max_date
    elif preset == "éå»30æ—¥":
        vis_start, vis_end = max_date - dt.timedelta(days=29), max_date
    elif preset == "å…¨æœŸé–“":
        vis_start, vis_end = min_date, max_date
    else:
        c1, c2 = st.columns(2)
        vis_start = c1.date_input(
            "é–‹å§‹æ—¥", value=min_date, min_value=min_date, max_value=max_date, key=f"visual_start_{table_name}"
        )
        vis_end   = c2.date_input(
            "çµ‚äº†æ—¥", value=max_date, min_value=min_date, max_value=max_date, key=f"visual_end_{table_name}"
        )

    needed_cols = tuple(c.name for c in tbl.c)

    # æ©Ÿç¨®ï¼ˆäººæ°—é †ï¼‰
    @st.cache_data
    def get_machines_with_freq(table_name: str, start: dt.date, end: dt.date, _cols_key: tuple):
        t = sa.Table(table_name, sa.MetaData(), autoload_with=eng)
        q = sa.select(t.c.æ©Ÿç¨®, sa.func.count().label("n")).where(
            t.c.date.between(start, end)
        ).group_by(t.c.æ©Ÿç¨®).order_by(sa.desc("n"), t.c.æ©Ÿç¨®)
        with eng.connect() as conn:
            return [r[0] for r in conn.execute(q)]

    machines = get_machines_with_freq(table_name, vis_start, vis_end, needed_cols)
    if not machines:
        st.warning("æŒ‡å®šæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"); st.stop()

    # æ©Ÿç¨®æ¤œç´¢ï¼ˆå‰æ–¹ä¸€è‡´â†’éƒ¨åˆ†ä¸€è‡´ï¼‰
    q_machine = st.text_input("æ©Ÿç¨®åã§æ¤œç´¢", placeholder="ä¾‹: ãƒã‚¤ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼")
    if q_machine:
        filtered_machines = [m for m in machines if m.startswith(q_machine)] or \
                            [m for m in machines if q_machine in m]
    else:
        filtered_machines = machines

    default_machine = qparams.get("machine") if "machine" in qparams else None
    if default_machine in filtered_machines:
        machine_sel = st.selectbox("æ©Ÿç¨®é¸æŠ", filtered_machines, index=filtered_machines.index(default_machine))
    else:
        machine_sel = st.selectbox("æ©Ÿç¨®é¸æŠ", filtered_machines)

    @st.cache_data
    def get_data(table_name: str, machine: str, start: dt.date, end: dt.date, _cols_key: tuple):
        t = sa.Table(table_name, sa.MetaData(), autoload_with=eng)
        q = sa.select(t).where(
            t.c.æ©Ÿç¨® == machine,
            t.c.date.between(start, end)
        ).order_by(t.c.date, t.c.å°ç•ªå·)
        return pd.read_sql(q, eng)

    df = get_data(table_name, machine_sel, vis_start, vis_end, needed_cols)
    if df.empty:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"); st.stop()

    # URLã¸ç¾åœ¨ã®çŠ¶æ…‹ã‚’åæ˜ 
    st.query_params.update({"table": table_name, "machine": machine_sel})

    # KPIã‚«ãƒ¼ãƒ‰ï¼ˆæœŸé–“å…¨ä½“ã¾ãŸã¯å½“æ—¥ï¼‰
    dfr = df[df["date"] == vis_end] if vis_start == vis_end else df
    c1, c2, c3 = st.columns(3)
    c1.metric("å¹³å‡ åˆæˆç¢ºç‡(å®Ÿæ•°)", f'{dfr["åˆæˆç¢ºç‡"].mean():.4f}' if not dfr.empty else "-")
    c2.metric("å¯¾è±¡å°æ•°", dfr["å°ç•ªå·"].nunique() if "å°ç•ªå·" in dfr.columns else 0)
    if not dfr.empty:
        best_row = dfr.loc[dfr["åˆæˆç¢ºç‡"].idxmax()]
        c3.metric("ãƒ™ã‚¹ãƒˆå° (åˆæˆ)", f'å°{int(best_row["å°ç•ªå·"])}')
    else:
        c3.metric("ãƒ™ã‚¹ãƒˆå° (åˆæˆ)", "-")

    # è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
    fmt_as_fraction = st.toggle("Yè»¸ã‚’ 1/â—¯ è¡¨ç¤ºã«ã™ã‚‹", value=True)
    use_hot_bg = st.toggle("â€œç†±ã„æ—¥â€èƒŒæ™¯ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆ5ãƒ»7ãƒ»åœŸæ—¥ï¼‰", value=False)
    use_downsample = st.toggle("é•·æœŸé–“ã¯é€±å¹³å‡ã§è¡¨ç¤ºï¼ˆè»½é‡åŒ–ï¼‰", value=False)
    show_multi = st.checkbox("è¤‡æ•°å°ã‚’æ¯”è¼ƒã™ã‚‹", value=False)

    # è¨­å®šãƒ©ã‚¤ãƒ³
    thresholds = setting_map.get(machine_sel, {})
    df_rules = pd.DataFrame([{"setting": k, "value": v} for k, v in thresholds.items()]) \
        if thresholds else pd.DataFrame(columns=["setting","value"])

    # èƒŒæ™¯
    def build_hot_background(start_d, end_d):
        df_bg = pd.DataFrame({"date": pd.date_range(start_d, end_d, freq="D")})
        df_bg["is_hot"] = df_bg["date"].apply(lambda d: (d.day in (5, 7)) or (d.weekday() >= 5))
        return alt.Chart(df_bg).mark_rect(opacity=0.08).encode(
            x="date:T",
            color=alt.condition("datum.is_hot", alt.value("red"), alt.value("transparent"), legend=None)
        )

    # Yè»¸
    if fmt_as_fraction:
        y_axis = alt.Axis(
            title="åˆæˆç¢ºç‡",
            labelExpr="isValid(datum.value) ? (datum.value==0 ? '0' : '1/'+format(round(1/datum.value),'d')) : ''"
        )
        tip_fmt = ".4f"
    else:
        y_axis = alt.Axis(title="åˆæˆç¢ºç‡(å®Ÿæ•°)", format=".4f")
        tip_fmt = ".4f"

    # ã‚¿ãƒ–æ§‹æˆï¼ˆæ¯”è¼ƒã¯æ—¢å®šOFFï¼‰
    if show_multi:
        tab_avg, tab_single, tab_multi = st.tabs(["å…¨å°å¹³å‡", "å˜å°", "è¤‡æ•°å°æ¯”è¼ƒ"])
    else:
        tab_avg, tab_single = st.tabs(["å…¨å°å¹³å‡", "å˜å°"])
        tab_multi = None

    # ---------- å…¨å°å¹³å‡ ----------
    with tab_avg:
        df_avg = (
            df.groupby("date", as_index=False)["åˆæˆç¢ºç‡"]
              .mean()
              .rename(columns={"åˆæˆç¢ºç‡": "plot_val"})
        )
        if use_downsample:
            df_avg = df_avg.set_index("date").resample("W")["plot_val"].mean().reset_index()

        base = alt.Chart(df_avg).mark_line().encode(
            x="date:T",
            y=alt.Y("plot_val:Q", axis=y_axis),
            tooltip=[alt.Tooltip("date:T", title="æ—¥ä»˜"),
                     alt.Tooltip("plot_val:Q", title="å€¤ (0=æ¬ æå«ã‚€)", format=tip_fmt)]
        ).properties(height=420)

        chart = base
        if not df_rules.empty:
            rules = alt.Chart(df_rules).mark_rule(strokeDash=[4, 2]).encode(
                y="value:Q",
                color=alt.Color("setting:N", legend=alt.Legend(title="è¨­å®šãƒ©ã‚¤ãƒ³"))
            )
            chart = chart + rules
        if use_hot_bg:
            chart = build_hot_background(vis_start, vis_end) + chart

        st.subheader(f"ğŸ“ˆ å…¨å°å¹³å‡ åˆæˆç¢ºç‡ | {machine_sel}")
        st.altair_chart(chart, use_container_width=True)

    # ---------- å˜å° ----------
    @st.cache_data
    def get_slots(table_name: str, machine: str, start: dt.date, end: dt.date, _cols_key: tuple):
        t = sa.Table(table_name, sa.MetaData(), autoload_with=eng)
        q = sa.select(t.c.å°ç•ªå·).where(
            t.c.æ©Ÿç¨® == machine, t.c.date.between(start, end)
        ).distinct().order_by(t.c.å°ç•ªå·)
        with eng.connect() as conn:
            vals = [r[0] for r in conn.execute(q) if r[0] is not None]
        return [int(v) for v in vals]

    slots_all = get_slots(table_name, machine_sel, vis_start, vis_end, needed_cols)

    with tab_single:
        if not slots_all:
            st.info("å°ç•ªå·ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            slot_sel = st.selectbox("å°ç•ªå·ã‚’é¸æŠ", slots_all)
            df_single = df[df["å°ç•ªå·"] == slot_sel].rename(columns={"åˆæˆç¢ºç‡":"plot_val"})
            if use_downsample:
                df_single = (df_single.set_index("date")
                             .resample("W")["plot_val"].mean().reset_index())
            base = alt.Chart(df_single).mark_line().encode(
                x="date:T",
                y=alt.Y("plot_val:Q", axis=y_axis),
                tooltip=[alt.Tooltip("date:T", title="æ—¥ä»˜"),
                         alt.Tooltip("plot_val:Q", title="å€¤ (0=æ¬ æå«ã‚€)", format=tip_fmt)]
            ).properties(height=420)

            chart = base
            if not df_rules.empty:
                rules = alt.Chart(df_rules).mark_rule(strokeDash=[4, 2]).encode(
                    y="value:Q",
                    color=alt.Color("setting:N", legend=alt.Legend(title="è¨­å®šãƒ©ã‚¤ãƒ³"))
                )
                chart = chart + rules
            if use_hot_bg:
                chart = build_hot_background(vis_start, vis_end) + chart

            st.subheader(f"ğŸ“ˆ åˆæˆç¢ºç‡ | {machine_sel} | å° {slot_sel}")
            st.altair_chart(chart, use_container_width=True)

    # ---------- è¤‡æ•°å°æ¯”è¼ƒï¼ˆä»»æ„ï¼‰ ----------
    if tab_multi is not None:
        with tab_multi:
            if not slots_all:
                st.info("å°ç•ªå·ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                default_slots = slots_all[:min(3, len(slots_all))]
                compare_slots = st.multiselect("æ¯”è¼ƒã™ã‚‹å°ç•ªå·ï¼ˆæœ€å¤§6å°ï¼‰",
                                               options=slots_all, default=default_slots, max_selections=6)
                if compare_slots:
                    df_multi = df[df["å°ç•ªå·"].isin(compare_slots)].rename(columns={"åˆæˆç¢ºç‡":"plot_val"})
                    if use_downsample:
                        df_multi = (df_multi.set_index("date")
                                    .groupby("å°ç•ªå·")["plot_val"].resample("W").mean()
                                    .reset_index())
                    base = alt.Chart(df_multi).mark_line().encode(
                        x="date:T",
                        y=alt.Y("plot_val:Q", axis=y_axis),
                        color=alt.Color("å°ç•ªå·:N", legend=alt.Legend(title="å°ç•ªå·")),
                        tooltip=[alt.Tooltip("date:T", title="æ—¥ä»˜"),
                                 alt.Tooltip("å°ç•ªå·:N", title="å°"),
                                 alt.Tooltip("plot_val:Q", title="å€¤ (0=æ¬ æå«ã‚€)", format=tip_fmt)]
                    ).properties(height=420)

                    chart = base
                    if not df_rules.empty:
                        rules = alt.Chart(df_rules).mark_rule(strokeDash=[4, 2]).encode(
                            y="value:Q",
                            color=alt.Color("setting:N", legend=alt.Legend(title="è¨­å®šãƒ©ã‚¤ãƒ³"))
                        )
                        chart = chart + rules
                    if use_hot_bg:
                        chart = build_hot_background(vis_start, vis_end) + chart

                    st.subheader(f"ğŸ“ˆ åˆæˆç¢ºç‡ æ¯”è¼ƒ | {machine_sel} | å° {', '.join(map(str, compare_slots))}")
                    st.altair_chart(chart, use_container_width=True)
                else:
                    st.info("å°ç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

    # ========== æŠ½å‡ºçµæœã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ==========
    cols_basic = ["date", "æ©Ÿç¨®", "å°ç•ªå·", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "BBå›æ•°", "RBå›æ•°", "åˆæˆç¢ºç‡"]
    dl_cols_mode = st.radio("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰åˆ—", ["åŸºæœ¬ã‚»ãƒƒãƒˆ", "ã™ã¹ã¦"], horizontal=True)
    out_df = df[cols_basic] if (dl_cols_mode == "åŸºæœ¬ã‚»ãƒƒãƒˆ" and all(c in df.columns for c in cols_basic)) else df
    csv_bytes = out_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ã“ã®æœŸé–“ãƒ»æ©Ÿç¨®ã®ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_bytes,
        file_name=f"{table_name}_{machine_sel}_{vis_start}_{vis_end}.csv",
        mime="text/csv"
    )
